
++++++++++++++++++++++++++++++++++++ DEVOPS
SDLC ???
Requirement Analysis ——> Prod delivery
Requirement analysis
Design
Development / Code
Test
Deployment

Waterfall model : follow linear approach
- monolith application
- can’t be changed if new changes comes from the requirement
- it has distinct goals for each phase of development. imagine a waterfall on the cliff of a steep mountain. once the water has flowed over the edge of the cliff, it cannot turn back.
- traditional waterfall model
	+ requirement and gathering analysis
	+ design
	+ implement
	+ test
	+ deploy
	+ maintenance

limitation of waterfall model
- high amounts of risk and uncertainty
- no working software is produced until late during the life cycle
- not a good model for complex and object-oriented projects
- once an application is in the testing stage, it is very difficult to go back and change something that was not well-thought out in the concept stage
- not suitable for the projects where requirements are at a moderate to high risk of changing

traditional development model challenges
- consider developing software in a traditional way using a waterfall model.
- waterfall model challenge : new phase in the development process begins only if the previous phase is complete.
	+ developers :
	+ huge waiting time for code deployment
	+ pressure of work on old and pending code
	+ operations :
	+ difficult to maintain uptime of the production env.
	+ tools to automate infrastructure manage. are not effective
	+ difficult to diagnose and provide feedback on the product
	+ no. of servers to be monitored increases

- what needs to be done?
	developers :
	+ use system with little or no waiting time
	+ use system with updated code
	operations :
	+ systems should be up and running most of the time
	+ system required for easy administration
	+ effective monitoring and feedbacks system should be established

Agile mythology : entire application is broken down into small modules
- iteration1,2,3,4,.....
- each modules will be created and tested
- all modules will be integrated together for deployment
- allow changes

What is agile methodology?
- in the agile methodology each project is broken up into several ‘Iterations’
- all iterations should be of the same time duration (between 2 to 8 weeks)
- at the end of each iteration, a working product should be delivered

Waterfall vs agile

Waterfall :- analyze => plan => design => build => test => deploy
agile :- analyze => plan => [ design <=> build <=> test ] => deploy

Teams involved in SDLC
- Infra-structure team : plays a crucial role in managing and maintaining the underlying infrastructure that supports the software development and delivery processes. The infrastructure team is responsible for setting up, configuring, and managing the hardware, servers, networks, and other infrastructure components required for running the software applications.
	+ some key responsibilities and activities typically performed by the infrastructure team in DevOps
	+ Infrastructure Provisioning
		+ Configuration Management e.g., Ansible, Puppet, Chef
		+ Infrastructure as Code (IaC) e.g., using tools like Terraform		+ Deployment and Release Management		
		+ Monitoring and Alerting
		+ Scalability and Capacity Planning		
		+ Security and Compliance
		+ Troubleshooting and Incident Management: 
- development team : coding and unit testing
- testing : functional testing and regression testing
- release management team
- security team
- monitoring team
- support team

- Regression testing is a type of software testing that focuses on verifying that previously developed and tested software functionalities continue to work correctly after modifications, bug fixes, or enhancements have been made to the software. The purpose of regression testing is to ensure that the changes introduced in the software do not unintentionally affect the existing features and that the overall system remains stable.
 	+ Regression testing can be performed at different stages of the software development life cycle, such as during unit testing, integration testing, or system testing.
	+ Regression testing typically involves the following steps:
		+ Test Case Selection
		+ Test Execution
		+ Comparison and Analysis
		+ Defect Reporting: 

- Functional testing is a type of software testing that focuses on verifying whether the system or software application functions correctly and meets the specified functional requirements. It is performed to ensure that the software performs its intended tasks and produces the expected results.
	+ Functional testing can be performed at different levels of the software development life cycle, including unit testing, integration testing, system testing, and acceptance testing.
	+ Functional testing typically includes the following activities:
	+ Test Case Design
		+ Test Data Preparation
		+ Test Execution
		+ Test Result Comparison
		+ Defect Reporting


What is DevOps?
- devops : what devops is not!!
	+ devops is not a role, person or organization
	+ devops is not a separate team
	+ devops is not a product or a tool
	+ devops is not about just writing scripts or implementing tools
- DevOps is a practice that allows a single team to manage the entire application development life cycle, that is, development, testing, deployment and monitoring.
- it’s a software development strategy which promotes collaboration between development and operations teams to achieve continuous integration & continuous delivery/deployment in more automated fashion
- strategy/process to achieve continuous improvement
- it consists of
	+ continuous development
	+ continuous integration
	+ continuous testing
	+ continuous delivery
	+ continuous deployment
	+ continuous monitoring

Role of DevOps
- provisioning of infrastructure
	+ e.g Infra team want to create 100 VMs
	+ this can be done using Terraform to write automated script
	+ this is called Infrastructure as a Code
	+ can also use Ansible
- roles of developer
	+ code
	+ build 
	+ artifact
	+ deploy
	+ scripts

Devops challenges
- people : adapting to the practice
- process
- tools

DevOps Lifecycle
- plan : where you plan, track, visualize and summarize your project before working/starting it. or application lifecycle management tool
	+ jira, trello, tricentis 
- code : where the developers write their code
	+ eclipse, git
- build : build is a pre-release version and is identified by a build number
	+ apache ant, jenkins, maven, gradle
- test : process of executing automated tests as part of software delivery pipeline to obtain feedback on the business risks associated with a software release as rapidly as possible
	+ jmeter, Junit, sellenium
- release : helps to integrate code into a shared repository, in order to detect and locate errors quickly and easily.
	+ travis cl, gitlab, bamboo, jenkins
- deploy : manage and maintain development and deployment of software systems and servers in any computational environment.
	+ aws, chef, ansible
- operate : keep the system upgraded with the latest update
	+ aws, chef, ansible
- monitor : ensures that the application is performing as desired and the environment is stable. it quickly determines when a service is unavailable and understand the underlying causes
	+ nagios, sensu, splunk

What does Devops do?
- integrates developers and operations teams?
- improves collaboration and productivity by:
	+ automating infrastructure
	+ automating workflows
	+ continuously measuring application performance

Benefits of Devops
i. Cost avoidance
ii. accelerated Innovation
iii. flexibility
iv. agility & speed
v. IT efficiency

Devops adoption
i. Cultural transformation : the workplace culture undergoes a major transformation while implementing cultural changes with devops.
	+ it is a long-term process that requires patience and endurance to maintain a positive and transparent atmosphere in the workplace with the changes being implemented
ii. Process transformation : adopting devops becomes challenging for companies which follow specific guidelines for software development, as devops doesn’t have any fixed framework stating procedures that employees can fellow to reach their desired goals.
iii. Technology transformation : organizations require updated hardware and software systems to avoid performance issues such as instability, slow load and processing times and security lapses
	+ they should keep up with the latest technology trends on a regular basis, so that new systems can co-exist with the existing systems.
iv. Automate everything : most of the legacy tools and systems used by organizations may not be conducive to automation and collaboration
	+ automation is essential since continuous testing and development are necessary for smooth deployment
v. Adoption of tools : the dev and ops teams have separate toolsets and metrics
	+ it is necessary to integrate all the tools properly to make testing, deployment and building all work together in a continuous manner.

DevOps stages
- version control / source code management : system that records changes to documents, computer programs and other collection of information over time.
	+ decentralized version control
	+ centralized version control
- continuous integration : requires developers to integrate code into a shared repository several times a day. by integrating regularly, you can detect errors quickly and locate them more easily.
	+ It’s the practice of automatically building and unit testing an entire application frequently, ideally on every source code check-in.
	+ it’s a practice not a tool
	+ > develop > test > deploy >
	+ a CI culture
		i. run tests locally before committing
		ii. don’t commit new code to broken builds
		iii. don’t leave the build broken
		iv. don’t remove tests that fail
	+ use notifications to update your build progress. at work notification is generated for CodeCommit, build start, build completion, deploy to stage and deploy to prod.
	+ it create artifacts. (binaries) *.war / *.jar
- continuous testing : process of executing automated tests as part of the software delivery pipeline to obtain immediate feedback.
	QA testing => User Acceptance testing => Pre-prod/Staging testing
- continuous deployment : here code changes are automatically built, tested, and prepared for a release to production.
	+ continuous delivery is the practice of deploying every build to a production-like environment and performing automated integration and acceptance testing. manual approval (to review the code) is needed for prod. deployment
	+ agile methodology is capable here because it requires manual intervention
	Dev => QA => UAT => prod-like => prod
	use case ::: Banking domain ( most banking sector use c.delivery approach)
	periodic sms notification
	create deployment windows == 4hrs == during non-business hours
	if they fixed the app within 4hrs => deployment, else reverse changes
		security audit will be done on the code
		lot of reviews and approval is required before moving to prod
	+ benefits of continuous delivery :-
	i. empowering teams
	ii. lowered cycle times
	iii. better security
	iv. rhythm of practice
	v. more time to be productive
	+ best for app that is tightly coupled with dependencies
	+ continuous deployment is the practice of automatically deploying every build to production after it passes its automated tests. no manual approval (to review the code) is required.
	Dev => QA => UAT => prod
	+ agile methodology is not capable here because everything is automated
	use case ::: facebook/netflix/google/amazon
	does not involve manual intervention
	no deployment window
	+ best for app that is loosely coupled with no dependency
- containerization : involves encapsulating an application in a container with its working environment
- configuration management : process of standardizing resource configurations and enforcing their state across IT infrastructure in an automated yet agile manner.
	+ puppet, ansible
	+ Configuration management using puppet : puppet is a config. management tool used to manage and maintain the development and deployment of software systems and servers in any computational environment.
	+ Configuration management using ansible : ansible is an open source IT configuration management, deployment & orchestration tool. it aims to provide large productivity gains to a wide variety of automation challenges.
- continuous monitoring
	+ management visibility : increase overall visibility of the management to organizational risk and performance
	+ metrics and reporting : increase overall visibility of the management to organizational risk and performance
	+ automated feedback mechanism : increase cost effectiveness through automated IT solutions for monitoring and reporting
	+ monitor operational effectiveness : continually monitor operations for adherence to intended levels of effectiveness and detect risk/issues.
	+ regulatory landscape : ensure compliance with policies, procedures and regulations
	+ risk-based decision making : provide key risk indicators based on the unique client environment, risk tolerance and key assets to make qualitative risk decisions.

Complete Devops CI/Cd/CD
- code => commited in a src code repository
- pick up the code
- perform build
- create artifacts
- delivery / deploy

DevOps delivery pipeline
	Version control > build > unit test > deploy > auto test > deploy to production > measure + validate
i. Source code : commited in a  source code repository
ii. Version control  
	+ GitHub (i & ii)
iii. Build system : watch over the source code for changes e.g jenkins, bamboo, teamcity, online services like circle ci or trans ci.
	+ its job is to build code and run unit test.
iv.  build tool : allow to build on the developer machine and as much code in the source control.
	+ Jenkins ( iii & iv)
v. unit test : maybe separate tool but incorporated in the build system.
	+ used to verify a piece of code is doing the job it was intended to do.
	+ it does not have dependencies on external system
vi. integration tests : 
	+ Testing ( v, vi, xii & xiii)
vii. artifacts : packaging what you built into artifacts like jar, war, ear files, rpm package  or docker image
viii. artifact repo : once packaged the code, it goes into artifact repo. storage device like amazon s3, docker registry, puppet forges etc
	+ Sonatype Nexus, maven repository, JFrog artifactory ( vii & viii)
ix. deployment server :
x. deployment tool : to bring working instance of your service
	+ to deploy both to your test and production environments
	+ chef ( ix & x)
xi. CI environment : sometimes you run your deployment tool on test environment ( or CI environment) to see issues with the code.
xii. integration tests : designed to exercise a real running service in a real environment and ensure it’s working correctly
xiii. e2e tests : end to end acceptance testing stage
	+ this may include manual testing component
	+ Testing ( v, vi, xii & xiii)
xiv. production environment : then use artifact that passed testing and same deployment tool to deploy to the production environment

CI flow diagram
Delivery team <=> Version control <=> build and unit tests <=> automated acceptance tests <=> user acceptance tests <=> release
Use Cases ::	amazon e-commerce site
	Sign-up 
	Sign-in
	search for req. items
	check the reviews
	adding the items to cart
	place my order
	payment
	order is confirmed
	track…

	Monolith
		the entire application is considered as one product to be developed and deployed together.
		when dev1 & dev2 are done, they can’t independently deploy their work
	Micro-services
		entire application is split into micro services
		
		when there is changes in the requirement, developers can build, test and independently deploy
		dev1 = sign-up service = code/unit testing(by developer)/integrate for further testing(by QA)/deliver/deploy
		dev2 = sign-in service = code/unit testing(by developer)/integrate for further testing(by QA)/deliver/deploy



++++++++++++++++++++++++++++++++++++ Version Control

Version Control with Git
- the main difference between operation staff and development staff is the use of version control

Why need version control?
- Collaboration
- Storing versions
- Figuring out what happened 
- backup
- solution for all these problems : all these problems can be solved with the help of a “Version control system”
- version control helps the teams to solve these kind of problems, by tracking every individual change by each contributor and helps prevent concurrent work from conflicting
- a version control software supports a developer’s preferred workflow without imposing one way of working

Issues without version control
- once saved, all the changes made in the files are permanent and cannot be reverted back
- no record of what was done and by whom
- downtime that can occur because of a faulty update could cost millions in losses

What is Version Control
- It’s system that documents changes made to a file or a set of files.
- it allows multiple users to manage multiple revisions of the same unit of information
- It’s a snapshot of your project over time.

Version Control types
	i. local version control (LVC) : the practice of having the version database in the local computer	
		+ local database keeps a record of the changes made to files in version database	
		+ issues : multiple people working in parallel on the same project
		+ solution : centralized version control
	ii. centralized version control (CVC) : a central repository is maintained where all the versioned files are kept.
		+ now users can checkout, and check-in files from their different computers at any time
		+ issues : in case of central server failure whole system goes down
		+ solution : distributed version control
	iii. distributed version control : version database is stored at every users’ local system and at the remote server.
		+ users  manipulate the local files and then upload the changes to the remote server.
		+ if any of the servers die, a client server can be used to restore

What is Git?
- Git is an open-source distribute version control system (DVCS) which records changes made to the files laying emphasis on speed, data integrity and distributed, non-linear workflows.

The Git file workflow
- use git w.f to manage your project effectively
- working with set of guidelines increases git’s consistency and productivity
- remote repository : is the server where all the collaborators upload changes made to the files.
- local repository : user’s copy of the version database
	+ user accesses all the files through local repository and then push the change made to the remote repository.
- workspace : is user’s active directory
	+ user modifies existing files and creates new files in this space.
- stage : place where all the modified files marked to be committed are placed.
- clone command : creates a copy of an existing remote repository inside the local repository
	$ git clone git@github.com:wickett/word-cloud-generator.git 
	
		username - wickett
		repository - word-cloud-generator
	+ then change directory into the repo
	$ cd word-cloud-generator/
	+ to see all the commit in the repo
	$ git log
	+ to see what was committed at a particular commit 
	$ git show commitId
	$ git status     // to see details of changes whether staged or committed
	$ git diff       // to see exactly what changed
	+ set up git pre commit hook, to take a certain action before proceeding with commit. any issues that are found will stop git from committing.
	+ look at hooks by 
	$ cat ./hooks/git-pre-commit.hook
- commit command : commits all the files in the staging area to the local repository
- push command : pushes all the changes made in the local repository to the remote repository.
- fetch command : collects the changes made in the remote repository and copies them to the local repository.
	+ it doesn’t affect our workspace
- pull command : gets all the changes from the remote repository and copies them to the local repository.
	+ it merges those changes to the current working directory
- Webhooks :  allow external services to be notified when certain events happen. When the specified events happen, we’ll send a POST request to each of the URLs you provide. Learn more in our Webhooks Guide.
	+ every commit, branch and pull request gets tested by CI and the status gets integrated into GitHub 

Git common commands
- to add a file to the staging area
	git add <filename>
- to check the working tree status
	git status
- to commit the staged files to your local repository
	git commit
- to display all the changes made to the tracked files
	git diff
- to stage and commit multiple files at once we use -a flag with the commit command
	git commit -a -m ‘message’
- to deletes the file from git repository as well as users system
	git rm <filename>
- to remove the file from git repository but not from the system, we use —cached option
	git rm --cached <filename>
- you can force remove a staged file by using -f flag
	git rm -f <filename>
- what can you do if you don’t want Git to track select files in the repository?
	+ you can create a .gitignore file in your repository and mention all the files you want Git to ignore
	$ gedit .gitignore
	#all .class files
	*.class
 
- to show all the commits so far on the current branch
	git log
- you can change the last commit
	git commit --amend
- to access a particular commit
	+ commit tags provide an alias for commitID
		git tag --a <annotation> --m <message>
	+ to view all the tags you have created
		git tag
	+ adding a tag to one of the previous commits
		git tag --a <annotation> < commitID> --m <message>
	+ commit id can be obtained from git logs
	+ all these tags can be viewed in git
		git show <tag-name>

What is a remote repository?
- mostly the users work on a local repository. but in order to collaborate with other people, we use a remote repository
- it’s a place where the users upload and share their commits with other collaborators.
- to push local repository to remote 
	git push origin master
- tags can be pushed, viewed and shared on remote
	git push origin --tags
- to list all the remotes attached to your local repository
	git remote -v
- to copy changes from remote to local repository
	git fetch origin
	+ it does not affect the present working directory
- to copy all the changes from remote to local repository
	+ then merges the changes with the present working directory
	git pull origin

Branching
- a project in its development could take multiple different paths to achieve its goal. branching helps us take these different directions, test them out and in the end achieve the required goal.

Branching in Git
- branching is an integral part of any Version Control (VC) system
- unlike other version control system, Git does not create a copy of existing files for a new branch.
- it points to a snapshot of the changes you have made in the system

Merging in Git
- merging integrates the changes made in different branches into one single branch
- all the changes made in Branch1 after merging are available in the Merged branch(Master)
- Branch1 becomes redundant after merging, hence it can be deleted

Merge conflict
- arise when two files having same content modified are merged
- can occur on merging branches or when merging forked history together

Resolving merge conflict : m.c are resolved manually by users
- git provides different merge-tools to compare and choose the required changes
- user can also use third Merge-tools with git

Stashing, rebasing, reverting and resetting  [ re-vbs]
- git stashing : is a way of creating a checkpoint for non-committed changes
	+ it saves all the changes to a temporary location so the user can perform other tasks such as switching branches, reverting etc 
	+ these changes can then be reapplied anywhere
- branch rebasing : is used when changes made in one branch needs to be reflected in another branch
	git rebase <rebase branch>
	git rebase master
- revert : revert or undo the changes made in the previous commit
	+ new commit is created without the changes made in the other commit
	git revert <commit id>
	+ old commit still resides in the history
	git revert HEAD   // HEAD refers to the latest commit
- reset : used to undo changes at different levels
	+ modifiers like —hard, —soft and —mixed can be used to decide to which to reset
	git reset <modifier> <commit id>
	git reset v1.5

Git workflows
- depending upon the collaborators or the organization using git different w.f could be adopted to maximize productivity and consistency
i. Centralized workflow
- a centralized repository is created on a remote server
- all the developers clone this central repository
- all the changes are made locally on every users computers
- isolated env. enables devs to work on their own and then push changes to remote repository
- suitable for smaller enterprises where teams are small

ii. Feature branching 
- all the development is done as features on separate branches 
- the master branch remains untouched unless all developers agree
- back and forth of pull requests enable developers to collaborate easy
- pull requests enable feedback as well as acknowledgement from other devs
- master branch never gets incomplete or broken code

iii. Gitflow workflow
- the gitflow w.f works as an extension to the feature branching model
- in addition to feature branches, separate branches for bugfixes, maintenance, development etc are maintained
- all other features branching workflow are also present
- this w.f is suitable for large organizations with emphasis on rapid software releases

iv. Forking workflow
- a server side public repository is maintained by a project lead
- every collaborator gets his own private repository (clone of public)
- developers work on their own private repositories
- the project maintainer chooses and accepts code from other collaborators
- only the maintainer can upload changes to the public repository

Using sshagent to access VMS / Instance
	+ install ssh client on your machine
	i. putty
	+ PuTTY is an SSH and telnet client, developed originally by Simon Tatham for the Windows platform ( or Mac)
	+ download from www.putty.org
	+ to use putty with AWS, download .ppk file (from EC2 configuration/create key pair). required this private key file to login into putty.
	+ configure accessing AWS instance / virtual machine using putty. 
		 
		copy the public IP address of that machine (AWS instance) and paste it in remote host (SSH)
		enter the username (of AWS instance) in username (SSH)
		Connection => SSH => Auth => private key for authentication => browse to private key (downloaded)
		from AWS instance => connect => SSH client => copy the ssh cmd to connect
		connect from the command prompt using the copied ssh cmd
	ii. MobaXterm
	+ MobaXterm provides all the important remote network tools (SSH, X11, RDP, VNC, FTP, MOSH, ...) and Unix commands (bash, ls, cat, sed, grep, awk, rsync, ...) to Windows desktop, in a single portable exe file which works out of the box.
	+ to use MobaXterm with AWS, just leave it at .pem (from EC2 configuration/create key pair) to access private key file.
	+ configure accessing AWS instance / virtual machine using MobaXterm. 
		 
		copy the public IP address of that machine (AWS instance) and paste it in remote host (SSH)
		enter the username (of AWS instance) in username (SSH)
		SSH => advanced SSH settings => click use private key => browse to private key (downloaded)
		from AWS instance => connect => SSH client => copy the ssh cmd to connect
		connect from the command prompt using the copied ssh cmd
	iii. SSH client: OpenSSH (pre-installed in Ubuntu). use .pem file as private key.
change to root user using : $ sudo -i
					   # yum update -y [ to update default packages]
to install git on AWS instance
# yum install git
	
++++++++++++++++++++++++++++++++++++ Introduction to maven

What is maven?
- it’s a tool that is used to compile, validate codes and analyze the test-cases in the code
- manages the building, reporting and documentation from source control management (scm)
- maven projects are configured through project object model (pom)
- pom.xml file contains documentation for all the objects, properties, methods, and events

What maven is capable of?
- information of project is centralized through maven
- our software project is modeled by it
- build process is managed
- data about the software project is gathered and then build itself
- document the software, and our project
- deployable artifacts can be generated from source code
- your source code is compiled, packed, test and distributed
- reports is created, website is generated for the project

Maven build lifecycle
- in maven, the build is run using a predefined and ordered set of steps to call the build life-cycle
- the build tasks that will be performed during each phase are determined by the configuration in the project file and in particular the selected packaging
- maven relies on build lifecycles to define the process
 of building and distributing artifacts (e.g jar files, war files)
- there are 3 built-in build lifecycles
	i. default : handles project building and deployment
	ii. clean : handles project cleaning 
	iii. site : handles project’s site generation

Maven artifacts
- an artifact is a file resulting from packaging a project
- can be a jar, war, ear, .xml
- artifacts are deployed in repositories, so they can be used, as dependencies, by other projects
- artifacts are identified by 3 components : 
	+ group id - a unique identifier for a group of related artifacts. usually named like java packages ( e.g pt.jpereira.mobile)
	+ artifact id - a unique identifier, within the context of grouped, that identifies the artifact(project) ( e.g puzzle)
	+ version - also called artifact coordinates

maven repository
- maven repository stores all the versions of all the dependencies
- there are 3 types of maven repository
	i. local : located in the developers machine
	ii. central repository : whenever build job is run, maven try to find it from local repo but when it’s not there, they download the dependencies from central repo.
repo.maven.apache.org
	iii. remote : are created for objects and modules which are specific to organization

Project object module (POM)
- xml file located at the root of the project (pom.xml)
- it includes configuration for your project
	+ information about the project
	+ configuration details to build the project
	+ contains default values for most of the projects. e.g source dir, 
	+ dependencies of the project
	+ configuration about plugins and goals
	+ used repositories

declaring dependencies
- while declaring the dependencies, coordinates of the artifacts must be provided
	+ groupId : a unique identifier for the group of related artifact
	+ artifactId : an artifact is a file which is made after packaging a project and artifactId is a unique identifier within the context of groupId and it identifies the artifacts
	+ version : it is an identifier for the release or build number of project

dependency management
- it has mechanism to centralize dependency information
- maven has all information about dependencies in the parent POM (version, exclusions, scope, type)
- child POMs only have to have a simple reference to the dependencies, excluding version, exclusions, scope type

Maven through terminal
- install the maven tar file after verifying the signature
- after installing the maven, check its version from ubuntu terminal by using the below command
	$mvn -version
- below command will create a project using maven
	$mvn archetype:generate -DgroupId=com.mycompany.app -DartifactId=my-app-DarchetypeArtifactId=maven-archetype-quickstart - DinteractiveMode=false
- the generate goal will create a directory having the same name given as the artifactId

Build the package in maven through terminal
- below command will build the maven project
	$mvn package
- the newly compile and build project can be tested by using the below command
	$java -cp target/my-app-1.0 proj_name.jar com.mycompany.app.App
- the above command will print the output of the project

maven lifecycle phases
- validate : check if the project is correct, having all the necessary info
- compile : compile the source code of the project
- test : test the compiled source code using a suitable unit testing framework
- package : take the compiled code and package it in its distributable format such as jar
- integration-test: process and deploy the package into an environment where integration tests can be run
- verify : run any checks to verify the package is valid
- install : install the package into the local repository
- deploy : done in an integration or release environment, copies the final package to the remote repository for sharing with other developers and projects.

Continuous integration & its importance

Traditional integration 
- greater time gap in case of traditional integration
- relatively greater risk or change in conflicts

Problems with traditional integration
- difficult to implement all the details that have changed
- difficult to get the latest version of the code early
- difficult to manage all the changes with the changing API versions
- difficult to manage the entire subsystem of the application that behave differently
- difficult to rewrite the whole code

How to solve the problem?
step 01 - split the entire chunk of codes into segments
step 02 - keep small segments of manageable code
step 03 - integrate the segmented code, multiples times a day
step 04 - adopt a continuous integration methodology to coordinate with your team



++++++++++++++++++++++++++++++++++++ JENKINS

What is continuous integration?
- it is the process of automating the building and testing of code, each time one of the team member commits changes to version control

Importance of Continuous integration
i. Improves quality : improves the quality by running multiple unit tests and analyzing various static code
ii. Increases productivity : automating build of code saves a lot of time, thereby increasing productivity
iii. Reduces risk : eliminate the risk of potential human errors by automating test


Jenkins Installation guide
Step 1 : Have java jdk installed
Step 2 :  update local package index and then install Jenkins
		sudo apt-get update 
		sudo apt-get install jenkins
Step 3 : start Jenkins service and check the status of Jenkins if it’s running.
		service jenkins start
		systemctl status jenkins
Step 4 : On the browser, go to Ipaddress:8080
Step 5 : Use below common to get the password, copy it and paste it within the Administrator password box
		cat /var/lib/jenkins/secrets/initialAdminPassword

Continuous Integration
- Continuous integration is a development practice that requires developers to integrate  code into a shared repository at regular basis.
- It can also be define as the process of automating the building and testing of code, each time one of the member commits changes to version control.

How CI works?
- developers commit code to a repository on a regular basis
- VSC is being monitored . when a commit is detected, a build will be triggered automatically
- if the build is not successful, developers will be notified immediately
- if the build is successful then the stakeholders, testers, team leads,  etc will be notified

CI includes:
- automated builds
- automated tests
- automated code quality metrics

Different CI tools :
- Jenkins, Bamboo, Travis CI, Codeship, Circle CI, Team City

CI - Important points
- fixing broken build highest priority
- automated deployment process
- high quality test

What is Jenkins?
A continuous integration server which manages and control processes such as plan, code, build, test, deploy, operate and monitor in DevOps environment.

How Jenkins works?
- get source code from repository 
- perform the code compilation and build the software with ANT, Maven or Gradle
- run an internal shell script
- automatically build and test
- monitoring the execution of the above tasks
- notifying the user about the success or failure of each build via email, jabber, twitter

Getting started with Jenkins
- to install jenkins on mac 
	brew install jenkins

post-installation setup
- unlocking Jenkins : browse to http://localhost:8080
- type the following command on the terminal
	cat /var/Jenkins_home/secrets/initialAdminPassword
- after unlocking jenkins, the Customize Jenkins page appears. Here you can install any plugins as part of your initial setup. 2 options : Install suggested plugins / select plugins to install.

Jenkins architecture :
													        - Test environment
	SCM => Build+Unit test => function test => deploy => <Is branch> => - Release env.
													        - Production env.
- source control management
- jenkins operation
- build, function test and deploy
- condition check
- deployed for testing
- deployed for release
- deployed for production

Terminologies used in Jenkins
- job / project : it is executable tasks that is controlled / monitored by jenkins.
- build : result of one run of a project
- node / slave : are machines/ computers that are set up to build projects for a master. Jenkins runs a separate program called “slave agent” on slaves
- master : it is central, coordinating process which stores configuration, loads plugins and renders various user interfaces for Jenkins.
- plugins : extension to jenkins functionality provided separately from jenkins core
- label : used for grouping agents
- upstream project :  a project can have one or more upstream projects. meaning that a build for the current project may be scheduled when an upstream build is finished.
- downstream : the current project is then known as an upstream project of the downstream project.
- artifact : an immutable file generated during a build or pipeline run. file is archived onto the Jenkins master for later retrieval by users.
- workspace : disposable directory on node used as a working directory for building.
- publisher : part of a build after the completion of all configured steps. it publishes reports, send notifications etc


jenkins UI overview
- new item : help to create a new job
- people : manages users within jenkins
- build history : shows history of builds with this jenkins instance
- manage jenkins : it provides options to configure system, manage plugins etc
- my views : provides a private view to filter jobs
- credentials : to manage all the global credentials
- build queue : shows all the builds that are triggered
- build executor status : current build executors and its jenkins instances
- freestyle project : freestyle build jobs are general-purpose build, which provides maximum flexibility
- maven project :  is a build job specially adapted to maven projects
- multiconfiguration job : allows you to run the same job on different environments.

Jenkins projects
- free style project : this is the central feature of Jenkins.
	+ Jenkins will build your project by combining any SCM with any build system.
- multi-configuration project : suitable for projects that need large number of different configurations such as testing on multiple env., platform specific builds etc
- GitHub organization : scans GitHub organization for all matching repositories
- pipeline : suitable for building pipelines or organizing complex activities that do not easily fit in free style
- folder : create a container and stores nested items in it.
	+ useful for grouping things together
- multi branch pipeline : creates a set of pipeline projects according to detected branches in one SCM repositories

jenkins source code management
- corresponding plugins for source code management tools has to be added to checkout the code from the repositories.

create your first jenkins job
- create a new job : select new item
- selection of project : give a name to the job and choose the relevant project type.
- job configuration : specify job configuration including option to “add build step” to run shel scripts. in the text box enter ‘echo “Hello Jenkins!!! This my first project”. 
- final dashboard : project information and overview should be visible
build first jenkins job
- build your job : click on your project from the jenkins home menu and click on “Build Now” to trigger the job
- get build details : click on the build number in the build history. this shows an overview of the build information.
- console output : contains all the logs captured during the build, which will be helpful for analyzing the builds.

Jenkins Management: Securing Jenkins
- Configure global security : one can manage security level in Jenkins environment and projects through Jenkins security.
	+ anyone can do anything : this is the least secure setup allowing anyone to perform any operation on jenkins server.
	+ legacy mode : this authorization uses existing projects
	+ logged-in users can do anything : users who have logged in can do anything within this authorization option.
	+ matrix based security : is the best practice among different modes of authorization.
	+ project based matrix authorization strategy : helps in managing the users access for various task with respect to projects.

Jenkins Management : Notification
- Jenkins has the Email Extension Plugin which enables Jenkins to send the email to the developer to inform about the job.
- email notification setup :
	i. Smtp server needs to be mentioned
	ii. User has to enter the email address & password of that email
	iii. Smtp port number for gmail server is 465
	iv. jenkins send the mail in plain text
	v. subject in mail is set default in jenkins
	vi. format in which jenkins will send mail
- Post build actions
	+ mentioned email-id receives mails from jenkins for different build information
	+ clicking on this checkbox enables jenkins to send separate mail to individuals
	+ clicking on this checkbox enables jenkins to send mail for unstable builds

Adding slave node to Jenkins

Jenkins Master : 	
- jenkins master performs basic installation and handles task related to builds and configuration
- they schedule builds
- they monitor slaves
- records and presents the build result

Jenkins slave :
- slaves are basically setup to offload builds from the master and distribute the workload
- they listen to the master’s request
- slaves can run on a variety of operating systems
- they mainly execute build jobs which are dispatched by jenkins master instance.
	
Master slave architecture in jenkins :
- Jenkins master will distribute its workload to the slaves
- Jenkins slaves are generally required to provide the desired environment. It works on the basis of requests received from Jenkins master.

how to setup slaves on jenkins :
- using username and password
- using ssh keys
- preparing slaves using ssh key follow the same steps as setting up slave setup using username and password does. the only difference is that instead of username and password for slave, ssh key is used.

Build delivery pipeline using Jenkins
build a job :
i. GitHub repository link : specify the GitHub repo link to pull the code to jenkins
ii. build trigger : specify the previous job that  triggers the current job
iii. job goal : specify the testing action/goal for each job
iv. post build action : specify the next job/ action to be triggered 

Continuous delivery pipeline
- it enables a constant flow of changes into production via an automated software production line
- a typical CD pipeline includes the following stages:
	+ build automation
	+ continuous integration
	+ test automation
	+ deployment automation

Build pipeline plugin installation
	+ step 01 - manage jenkins
	+ step 02 - manage plugins
	+ step 03 - select available option
	+ step 04 - install build pipeline plugin

Create pipeline build view
- configuring pipeline view : after setting the initial job and configuring it accordingly, pipeline is ready to run. 
- complete build pipeline :
	+ yellow blocks represent the currently executing  job
	+ blue blocks represent the jobs waiting for execution
	+ green blocks represent the succcessful build of job
	+ red blocks represent failure of building of job
	+ after clicking on the view that is created, you can run your pipeline build

Pipeline as code
- pipeline as code : pipeline as code provides functionalities like
	+ defining the pipeline flow into source control
	+ suspend or resume executing jobs
	+ pipeline plugin enables users to implement the whole build, test and deploy pipeline in Jenkins and store their, treating the pipeline as another piece of code in their source control repository.
- jenkins pipeline stage view : helps to visualize the progress of various stages of the pipeline in real time.

Basic structure of a declarative pipeline :
i. Nodes
- a node block is the jenkins agent wherein stage blocks, directives and steps should run
- the node block structure is as below:
	node(‘<parameter>’){<constituents>}
ii. Stage
- a stage block is a bundle of closely related steps and directives that have a common objective
- the stage block structure is as below :
	stage(‘<parameter>’){<constituents>}
iii. Directives
- the purpose of directives are to suggest the node block, stage block, and steps by providing them with any of the following :
	+ options
	+ parameters
	+ environments
	+ triggers, tools
iv. Steps
- fundamental block of a pipeline
- a step could be 
	+ a batch script or a shell script, or
	+ any other command that is executable
- steps have various purposes, such as cloning a repository, building code, running tests, uploading artifacts to the repository server, performing static code analysis, and so on.

Pipeline syntax overview

- declarative pipeline fundamentals : in declarative pipeline syntax, there is concept of pipeline block which defines all the work done throughout the entire pipeline.
jenkinsfile(Declarative pipeline)
pipeline{
	agent any // i.
	stages{
		stage(‘Build’){ // ii.
			
			steps{
				// iii.
			}
		}
		stage(‘Test’){ // iv.
			
			steps{
				// v.
			}
		}
		stage(‘Deploy’){ // vi.
			
			steps{
				// vii.
			}
		}		
	}
}

i. execute the pipeline or any of its stages, on any available agent
ii. define the “Build”
iii. perform some steps related to the “Build” stage
iv. defines the “Test” stage
v. perform some steps related to the “Test” stage
vi. define the “Deploy” stage
vii. perform some steps related to the “Deploy” stage

- scripted pipeline fundamentals - in a scripted pipeline, the node blocks does core work for the entire pipeline. adding the pipeline’s work inside a node block does 2 things :
	+ schedules the steps contained within the block to run when an executor is free on a node
	+ creates a workspace where work can be done on files checked out from source control

jenkinsfile(Scripted pipeline)
node{ 	// i
		stage(‘Build’){ // ii.
				
				// iii.
		}
		stage(‘Test’){ // iv.
				// v.
		}
		stage(‘Deploy’){ // vi.
			
				// vii.
		}		
	}
}


Plugins and its uses

Introduction to jenkins plugin
- the plugins increase jenkins capabilities and increases productivity. 

Jenkins plugin installation 
- using the web UI : manage jenkins  > manage plugins view, available to administrators in a jenkins environment
- using the jenkins CLI : d cli provides a command line to download a plugin and its dependencies and it is available only to jenkins administrators.
	java -jar jenkins-cli.jar -s http://localhost:8080/ install-plugin SOURCE .. [-deploy] [-name VAL] [-restart]

plugin management in jenkins [tabs]
- update : shows updates to plugins already installed
- available : shows plugins that are available for installation
- installed : displays plugins installed that have no updates
- advanced : lists configuration of HTTP proxy, allows manual upload of plugin and URL of plugin site

removing a plugin
- it’s good to remove the plugins from jenkins master, when it’s no longer used to reduce memory overhead at boot or runtime.
- to uninstall a plugin navigate to installed tab on the manage plugins page.
- an alternative way to uninstall the plugin is by removing the corresponding .hpi file from the JENKINS_HOME/plugins directory on the master.
disabling a plugin
- disabling a plugin is the best way to retire a plugin. jenkins will continue to recognize that the plugin is installed, but it will not start the plugin.
- to disable a plugin uncheck the box on the “installed tab” of the “manage plugins page”.

commonly used plugins
- GitHub plugin : GitHub plugin integrates jenkins with GitHub projects. It is used to automatically trigger each build on jenkins server after each commit in the repository.
- HTML publisher plugin : publishes the html report generated by the jenkins build. the reports generated can be viewed using the links in the dashboard.
- copy artifact plugin : helps to copy artifacts from another project
- extended choice parameter plugin : adds more functionality to the choice parameters.
build with parameter plugin : lets the user to provide the parameters to the build, prompts for the confirmation before triggering the build.

Configuring Jenkins to work with plugins
- configure GitHub plugin
	+ “Manage Jenkins” > configure system > Add GitHub server > add the credentials and save the configuration
	+ navigate to jenkins project, set project URL = Github repository,
	+ under source code mgt, check Git, set repository URL = Github repository
	+ under build triggers, select “build when a change is pushed to GitHub” checkbox
	+ from your GitHub repository, go to settings > Integration & services > add service > add jenkins ( GitHub plugin)
	+ set the jenkins hook url for your jenkins machine, and add /github-webhook/
	http://localhost:8080/github-webhook/
- configure HTML publisher plugin
	+ select the “configure” option for your project 
	+ under the “post build action” > publish HTML reports > fill in the directory and other details
- configure copy artifact plugin
	+ select the “configure” option for your project 
	+ under the “build option” > copy artifacts from another project
	+ fill in the “Project name” from where the artifacts has to be copied
- configuring jenkins to work with java, git, maven
	+ go to “manage jenkins” > global tool configuration
	+ we need to add JDK, Git and maven installation. so that jenkins knows where to look for the path.
 
Poll Source Code changes periodically
- you can configure jenkins to poll the source code repository changes, either periodically or based on a schedule
- how to configure jenkins to poll SCM
	+ go to configure page of a project > build triggers > poll SCM > enter the schedule that you want in cron-style.
	+ the jenkins home page will have an extra option “Polling Log”, which contains all the polling logs
	+ to verify that polling of source code repository works perfectly, make some changes in your repository and then the build gets triggered automatically
- Cron syntax
	+ in cron, each line consists of 5 fields separated by TAB or whitespace :
	minute(0-59)
	hour(0-23)
	day of month(1-31)
	month(1-12)
	day of week

DEMO - create a maven job
- create a new job : select new item from the jenkins menu.
- selection of project : give a name to the job and choose the relevant project type (Freestyle project)
- job configuration : specify the job configuration and clone the source code from the git repo containing the maven project. you also have an option “invoke top-level maven targets” and give the name of the Maven Version set and the goals.
	+ the goals available for maven build are as follows: validate, compile, test, package, verify, install, deploy
- final dashboard : build and execute the project.


Setting up your build job and security 

Build triggers
- we need to tell Jenkins when to kick off a build
- this will be configured under Build Triggers section
- Triggering a build job once another build job has finished :
	
	+ this set up a build pipeline, that will be run whenever another build has finished
	+ enter the name of the preceding build job in the field Projects to watch
- Scheduled build jobs :
	+ continuous integration server should provide feedback much more quickly than once a day
	+ trigger the build jobs at regular intervals
- Polling the SCM : 
	+ polling involves checking the version control server at regular intervals if any changes have been committed.
	+ if any changes have been made to the source code in the project, Jenkins starts off a build
	+ in jenkins, SCM polling is easy to configure, and uses the same cron syntax
- Triggering builds remotely :
	+ this build trigger allows SCM system to triggers a jenkins build whenever a change is committed
- Manual build jobs : 
	+ these build should be started manually
	+ for these kind of jobs, the build triggers section is left empty

Maven build steps
- maven build steps are easy to configure and are flexible
- choose invoke top level maven targets from build step lists
- pick a version of maven to run and enter the maven goals you want to run.

Ant build steps
- freestyle build jobs work equally well with Ant
- Apache Ant is widely-used and very well-known java build scripting tool
- there are Ant libraries available for many development tools and low-level tasks, such as using SSH, or working with proprietary application servers.

Post build actions
- reporting on test results :
	+ it reports the test failure, tests that were excluded, time taken to execute and so on
	+ jenkins provides multiple plugins to support reporting e.g Publish JUnit test result report
	+ in a freestyle build job, you need to select “Publish Unit test result report”
- archiving build results :
	+ an artifact might be a binary executable, or some other related deliverable, such as documentation or source code
	+ to configure jenkins to store artifacts, choose Archive the artifacts in Post-Build Actions and specify the artifacts to be stored
	+ in the Files to archive field, can provide the full paths of the files you want to archive.
- Notifications :
	+ the main intention of a CI server is to notify the users regarding the build status
	+ jenkins provides the support for email notification
	+ this can be enabled by choosing E-mail Notification checkbox in Post-build actions
	+ when the build breaks, jenkins sends an email message to the list of users



Edureka practice

CI/CD workflow
	+ jenkins is build orchestration tool 
	+ Developers :::: java application => index.jsp => commit the code to a GitHub repo.
	+ build : we use jenkins to automate build
		process of compile the code and create artifacts (binaries like *.war / *.jar etc)
		build tools include maven, gradle, ant
	+ deployment (QA/UAT/PROD) : : we use jenkins to automate deployment 

what to do?
- Install and configure jenkins server
- configure the required tools/plugins
- configure the build server
	jenkins use Master / slave architecture
	+ purpose of master :
	+ purpose of slave :
	VMs ==== Jenkins (master): machine where you install jenkins. Master is responsible to manage and schedule the jobs to the slave machines
		
		install maven build and perform the build (java, .net, python, node.js, angular). have multiple build application is not encourage on the master
		the actual build will only happen on slave machine
	VMs ==== Jenkins slave1 : java appln. —- maven, unit
	VMs ==== Jenkins slave2	: python
	VMs ==== Jenkins slave3
	VMs ==== Jenkins slave4



- install jenkins on VM / AWS instance for masters
	+ launch new AWS linux instances
		#Jenkins Master, Node1	
		www.jenkins.io/download/
		sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
		sudo rpm —import https://pkg.jenkins.io/redhat-stable/jenkins.io.key
		#Jenkins Master, Node1
		#launch AWS linux instance with port 8080 & Name tag
		security => security group => edit inbound rules => add rule => TCP / 8080 / anywhere / 0.0.0.0/0
		#update the instance
[ec2-user@ip-172-31-10-252 ~]$ sudo -i
[root@ip-172-31-10-252 ~]# yum update -y
[root@ip-172-31-10-252 ~]# sudo wget -O /etc/yum.repos.d/jenkins.repo https://pkg.jenkins.io/redhat-stable/jenkins.repo
[root@ip-172-31-10-252 ~]# sudo rpm --import https://pkg.jenkins.io/redhat-stable/jenkins.io.key

	+ install epel package
[root@ip-172-31-10-252 ~]# amazon-linux-extras install epel

	+ install java
[root@ip-172-31-10-252 ~]# amazon-linux-extras install java-openjdk11

	+ install jenkins
[root@ip-172-31-10-252 ~]# yum install Jenkins
		
	+ start jenkins
[root@ip-172-31-10-252 ~]# service jenkins start

	+ setup jenkins to start at boot
		#chkconfig jenkins on
[root@ip-172-31-10-252 ~]# systemctl status jenkins
[root@ip-172-31-10-252 ~]# systemctl enable jenkins	// jenkins will be restarted automatically when machine is restarted
[root@ip-172-31-10-252 ~]# systemctl start jenkins

	+ open new browser using <public-ip-addr>:8080, this is how you open jenkins  web page 
		http://13.234.112.31:8080/
		this is login into jenkins through browser
		
	+ copy administrator password, after opening Jenkins web page 
		cat /var/lib/jenkins/secrets/initialAdminPassword

	+ config.xml file is use manage entire jenkins
		cat /var/lib/jenkins/config.xml
		here the password based login is enabled

	+ install jenkins plugins
		/var/lib/jenkins/plugins	

	+ setup username, password, full name and e-mail for admin user.

	+ instance configuration
		http://52.66.172.208:8080/

- how to use jenkins master to build job : you should be able to build job on the master before build on the slave machine.

	i. create freestyle job
	new item => job name [aaa-oct29-freestyle] => template :: freestyle project => manual configurations 
	General >> descriptions
		+ here you can add parameters like variables e.g env
	source code mgt >> git
	build triggers >>
	build environment >>
	build >> build steps / execute shell [e.g echo “Hello Team” ]

		+ create a build job : see the list of available environment variables like build number, build url, job url, job name, which  can be use in your job. like echo “jenkins-${JOB_NAME}-${BUILD_NUMBER} - ${BUILD_URL}”

		+ if notification is sent to the developer with this env.v, he access the job easily	

		+ create a deployment job : to deploy artifacts. the function is the same. copy the artifact from one server to another server
		DEV == QA == UAT == PROD

		+ to use user defined variable in add parameter : echo “Runtime value - ${env}”. here you create one job of copying the artifact but change the env [QA, UAT, PROD]

		+ how to deploy a job? which command to copy artifact from system to another? = groovy script
		
	post-build actions >>
	=> save job => build now [ see job status in build history]

	+ copy IP address from AWS => log into MobaXterm => session settings =>  SSH [Basic SSH settings] => paste IP address in Remote host => specify username : ec2-user => port : 22 => advanced SSH settings => use private key => shell [ in the tab]

[ec2-user@ip-172-31-10-252 ~]$ sudo -i
[root@ip-172-31-10-252 ~]# cd /var/lib/jenkins/
[root@ip-172-31-10-252 jenkins]# ls 	//here you’ll see workspace directory
[root@ip-172-31-10-252 jenkins]# cd workspace/
[root@ip-172-31-10-252 workspace]# ls  //here you’ll see the created job artifact [aaa-oct29-freestyle]
[root@ip-172-31-10-252 workspace]# cd aaa-oct29-freestyle
[root@ip-172-31-10-252 aaa-oct29-freestyle]# ls	// here you’ll see created artifacts and other job related files. from here the artifacts can be transfer to artifact machines.
[root@ip-172-31-10-252 workspace]# cd ..
[root@ip-172-31-10-252 jenkins]# cd jobs    
[root@ip-172-31-10-252 jobs]# ls        // this will list created jobs
[root@ip-172-31-10-252 jobs]# cd aaa-oct29-freestyle
[root@ip-172-31-10-252 aaa-oct29-freestyle]# ls
builds config.xml nextBuildNumber
[root@ip-172-31-10-252 aaa-oct29-freestyle]# pwd
/var/lib/jenkins/jobs/aaa-oct29-freestyle   // take backup of this folder only
	+ make change to job : aaa-oct29-freestyle [from dashboard] => configure => manual configurations [ as above]

	ii. create pipeline job
	new item => job name [test-pipeline-project] => template :: pipeline => manual configurations 
	General >> descriptions
		+ here you can add parameters like variables e.g env
	build triggers >>
	advanced project options >> git
	pipeline >> write script here
		+ other steps in freestyle template will be control through script
		+ create pipeline script or use script from SCM
		+ use pipeline template to create your script
 
	=> save job => build now [ see job status in build history]

- Jenkins pipeline scripts
	i. scripted pipelines : follows scripting methodologies use in the scripting language
		+ start with : node
	ii. declarative pipelines : do not follow scripting methodologies. make the coding simple. parameters includes :
		+ start with : pipeline 	
		+ agent : define machine where you want to run your job like master or slave machine
		+ stages : within this we have multiple stage
		+ steps : perform some tasks including commands you want to run. it contains the name of the stage and the command to run.

	+ use template to make job creation very easy
	+ install maven plugins : manage jenkins >> plugins >> available :: Maven integration, Pipeline Maven integration >> install without restart

- java app team
	+ build tools and unit test & create artifacts
- Role on Jenkins 
	+ installation and config of all the jenkins slave and tools plugins
	+ create CI/CD pipeline to perform build and deployment for each microservice or application. this pipeline can be reusable
	+ upgrade required tools and plugins
	+ ensure high availability of jenkins master and slave machines. this can be done by taking periodic snapshot of the volume of running instance (on aws)
	
	Non-prod environment 		prod environment
	DEV/QA/UAT				PROD

- you can have unique template for each job like DEV-job, QA-job, UAT-job, PROD-job

- Configure jenkins master and slave machines
	+ to create artifacts
	*war ===> app_server e.g tomcat / nginx
	+ servers 

Dev server
Build server - where artifacts are created and posted to target environment
Hosted server - to host app (artifact) , you need app server like tomcat / nginx
 
- install jenkins on VM / AWS instance for slaves
	+ open slave machine on mobxterm 
		copy the IP address with default user - ec2-user
		connect using the ssh key

	+ prepare your node (instance) i.e depending on the functionality of the slave machine you need to install the dependencies like jdk, building tools etc

[ec2-user@ip-172-31-10-255 ~]$ sudo yum update -y
###Execute in slave nodes:
[ec2-user@ip-172-31-10-255 ~]$ sudo -i

#Install epel Package: EPEL stands for Extra Packages for Enterprise Linux. 
[root@ip-172-31-10-255 ~]# amazon-linux-extras install epel

#Install java:
[root@ip-172-31-10-255 ~]# amazon-linux-extras install java-openjdk11

#Install GIT:
[root@ip-172-31-10-255 ~]# yum install git

#Install Maven:             
[root@ip-172-31-10-255 ~]# cd /tmp   // download maven archive file here
[root@ip-172-31-10-255 ~]# sudo tar xf /tmp/apache-maven-3.8.6-bin.tar.gz -C /opt

#Create a symlink for Apache maven directory to update the maven versions
[root@ip-172-31-10-255 ~]# sudo ln -s /opt/apache-maven-3.8.6 /opt/maven

#set java path / environment variables:
open .bashrc & add the following lines
export JAVA_HOME=“/usr/lib/jvm/java-11-openjdk-11.0.16.0.8-1.amzn2.0.1.x86_64”
export MAVEN_HOME=/opt/apache-maven-3.8.6
export M2=/opt/apache-maven-3.8.6/bin
PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$MAVEN_HOME:$M2

[root@ip-172-31-10-255 ~]# source ~/.bash_profile
[root@ip-172-31-10-255 ~]# mvn —-version

	+ you need to connect the slave to the master machine. you can use ec2-user as your default user, but you need to create new user.

#add user
[root@ip-172-31-10-255 ~]# useradd -m -d /home/jenkins jenkin  // workspace will be created in this user path
[root@ip-172-31-10-255 ~]# su - jenkin

[jenkin@ip-172-31-10-255 ~]$ ssh-keygen    // to create ssh key to connect. to create both private and public keys
[jenkin@ip-172-31-10-255 ~]$ ls ~/.ssh

#you should see following two files:
#id_rsa - private key
#id_rsa.pub - public

//cat id_rsa & copy the private key and paste it into jenkins node config. enter private key directly field. you need this private key to establish connection between the slave and master machine.

[jenkin@ip-172-31-10-255 ~]$ cat id_rsa.pub > authorized_keys [ .ssh directory ]

// change the owner and restrict access to the keys
[jenkin@ip-172-31-10-255 ~]$ chown -R jenkin /home/jenkins/.ssh
[jenkin@ip-172-31-10-255 ~]$ chmod 600 /home/jenkins/.ssh/authorized_keys
[jenkin@ip-172-31-10-255 ~]$ chmod 700 /home/jenkins/.ssh

// to exit this user to root
[jenkin@ip-172-31-10-255 ~]$ exit

	+ once the slave machine is ready you can go to the master machine

- configuration on master machine after slave machine is ready
	+ connect master to slave using jenkin user
	on master : manage jenkins > manage nodes and clouds > new node [ to create new node] > node name [ better to use slave  machine/instance name] > type [ permanent agent] > create
		> description : this node is used to build java application using maven
		> number of executors : 2
		> remote root directory : /home/jenkins  [ from slave machine]
		> label : javanode1 [ to identify the slave machine]
		> launch method : launch agents via SSH
			Host : slave machine/instance private IP address [ bcos public key might change anytime]
			add credentials 
				kind : SSH username with private key
				scope : global
				ID : javabuildnode [ meaningful name for reference]
				description : javabuildnode [ meaningful name for reference]
				username : jenkin [ user name created on slave.m]
				private key / enter directly > add : paste private key from slave.m
// cat id_rsa [ to copy ssh private key from slave.m]
				host key verification strategy : manually trusted key verification strategy / enabled
			availability : keep this agent online as much as possible
			node properties : 
		> save
		> open node > trust SSH host key > yes
		> launch agent

	+ go to the project/node created for pipeline on the master.m and edit script to include the slave label as javanode1
//sample pipeline script

pipeline {
	agent javanode1 // to schedule the slave.m for the job

	stages [

		stage(’SCM Checkout’) {
			steps {
				echo ‘Hello World’
			}
		}

		stage(’Build’) {
			steps {
				echo ‘Hello World’
			}
		}

		stage(’Unit Test’) {
			steps {
				echo ‘Hello World’
			}
		}

		stage(’Deploy to QA’) {
			steps {
				echo ‘Hello World’
			}
		}
	}
}
		+ check the output 
			project/node > console output
		+ to define the location for workspace in slave.m. here you schedule the project to run in the slave.m and the workspace will be created remotely on the slave.m
			project/node > configure 
			> general 
			> restrict where this project can be run
				label expression : javanode1 [ label created for slave.m above]

- how to run job in specific slave.m
- use case : to build and create artifact for my java web appln on my slave.m
	+ source code 
		repository : GitHub.com/LoksaiETA/Java-mvn-app2.git
			artifact name is defined by <artifactId> in pom.xml file
			packaging type is defined by <packaging>
		test : main/test/codes will be use for unit test

	+ project > configuation > pipeline 
		definition : pipeline script
		script : try sample pipeline [ Github + Maven] // use pipeline syntax to generate pipeline scripts

pipeline {
	agent {
		label ‘javanode1’
	}
	tools {
		// install the maven version configured as “M3” and add it to the path
		maven “Maven” // name from configuration below
// maven need to be configured on the master.m 
// manage jenkins > global tool configuration > add maven > [can’t be install automatically since it’s installed on slave.m] > Name : Maven >
MAVEN_HOME : /opt/apache-maven-3.8.6
	}

	stages {

		stage(’SCM Checkout’) {
			steps {
				echo ‘Java SRC’
				git ‘https://gitHub.com/LoksaiETA/Java-mvn-app2.git'  [ copy url from source code repository]
			}

		stage(‘Build’) {
			steps {
				sh “mvn -Dmaven.test.failure.ignore=true clean package”
			}
		}
	}
}

	+ > apply / save > build now

	+ go to slave.m
[jenkin@ip-172-31-46-130 ~]$ cd workspace/
[jenkin@ip-172-31-46-130 workspace]$ ls
[jenkin@ip-172-31-46-130 workspace]$ cd test-pipeline-project1
[jenkin@ip-172-31-46-130 test-pipeline-project1]$	ls
	+ artifact is created here
/home/jenkins/workspace/test-pipeline-project1/target/mvn-hello-world.war
		
			
// link to documents	
https://github.com/LoksaiETA/DCP-13th-March 
https://github.com/olahim/DCP-10th-Apr.
https://github.com/olahim/DCP-13th-March
	
		
- Backup Strategies
	+ Infra-structure team === take a periodic snapshot of your volume (like EBS volume)
	+ jenkins-cli / rest-api	
Manage Jenkins
- manage plugins
	+ manage jenkins => manage plugins => updates/available/installed/advanced
- global tool configuration
	+ to configure installed plugins like home directory
	+ this configuration applies to all the slaves
- configure system
	+ has jenkins home directory
	+ no of executor to execute jobs simultaneously
- manage nodes and clouds
	+ master-slave configuration is done here
- manage users
	+ users
	+ to add multiple users
- 
	



Edureka practice
Today’s tasks :: build and deployment
required :
- jenkins master == to schedule the builds
- jenkins slave == build server & create artifacts [ .war in target/]
- QA environment == application server (tomcat) for running my war files [ to test]
- UAT
- PROD
create a CI/CD pipeline to perform these build, create artifact and deployment
 
i. QA environment == running my war files [ to test]
	+ create virtual machine and install app server (tomcat) 
#Install & configure Tomcat server : QA-server
#Launch AWS EC2 Linux Instance:  8080 
copy public ip address and user name [ec2-user] to mobxterm
[ec2-user@ip-172-31-45-0 ~] sudo -i 
[ec2-user@ip-172-31-45-0 ~] yum update -y   // to update the packages
#Install epel Package:
amazon-linux-extras install epel    // install package reserved for amazon linux
#Install Java: 
amazon-linux-extras install java-openjdk11
#Set Java Path / Environment Variables:
#open .bashrc & add the following lines:
export JAVA_HOME="/usr/lib/jvm/java-11-openjdk-11.0.18.0.10-1.amzn2.0.1.x86_64"
PATH=$PATH:$HOME/bin:$JAVA_HOME/bin
#Save the file
#open .bash_profile & add the following lines:
export JAVA_HOME="/usr/lib/jvm/java-11-openjdk-11.0.18.0.10-1.amzn2.0.1.x86_64"
PATH=$PATH:$HOME/bin:$JAVA_HOME/bin
#Save the file
source ~/.bash_profile
************************
#Install tomcat in Amazon Linux Instance:
[root@ip-172-31-45-0 ~]# cd /opt
[root@ip-172-31-45-0 opt]# wget https://dlcdn.apache.org/tomcat/tomcat-8/v8.5.87/bin/apache-tomcat-8.5.87.tar.gz		// to download tomcat
[root@ip-172-31-45-0 opt]# tar -xvzf /opt/apache-tomcat-8.5.87.tar.gz // to extract tomcat
[root@ip-172-31-45-0 opt]# mv apache-tomcat-8.5.87 tomcat // to rename the directory
#Start Tomcat Server:
#Goto:
[root@ip-172-31-45-0 opt]# cd /opt/tomcat/bin
[root@ip-172-31-45-0 bin]# ./startup.sh		// to run tomcat
172-31-45-0:8080 // to open tomcat [ home page] running at port:8080 in browser
- to access artifact/application
172-31-45-0:8080/artifact-name
************************
#Add-USer for Tomcat : to access tomcat server from jenkins
[root@ip-172-31-45-0 ~]# useradd -m -d /home/jenkins jenkin // create jenkin user with the directory
[root@ip-172-31-45-0 ~]# su - jenkin
[jenkin@ip-172-31-45-0 ~]$ ssh-keygen   // creating ssh key for jenkin user
[jenkin@ip-172-31-45-0 ~]$ ls ~/.ssh 
#You should see following two files:
#id_rsa - private key
#id_rsa.pub - public
[jenkin@ip-172-31-45-0 ~]$ cd /home/jenkins/.ssh
[jenkin@ip-172-31-45-0 .ssh]$ cat id_rsa.pub > authorized_keys
[jenkin@ip-172-31-45-0 .ssh]$ chown -R jenkin /home/jenkins/.ssh
[jenkin@ip-172-31-45-0 .ssh]$ chmod 600 /home/jenkins/.ssh/authorized_keys
[jenkin@ip-172-31-45-0 .ssh]$ chmod 700 /home/jenkins/.ssh
#make jenkin user as a owner to tomcat dir :
jenkin@ip-172-31-45-0 tomcat]$ cd /opt/tomcat
[jenkin@ip-172-31-45-0 tomcat]$ exit
 
[root@ip-172-31-45-0 ~]# chown -R jenkin /opt/tomcat // to be able to launch the tomcat server
[root@ip-172-31-45-0 ~]# su - jenkin
[jenkin@ip-172-31-45-0 tomcat]$ ls
[jenkin@ip-172-31-45-0 tomcat]$ cd /webapps
[jenkin@ip-172-31-45-0 webapps]$ ls
[jenkin@ip-172-31-45-0 webapps]$ pwd		// /opt/tomcat/webapps
172-31-45-0:8080 // to open tomcat [ home page] running at port:8080 in browser, after starting tomcat
- to access artifact/application
172-31-45-0:8080/artifact-name   //  for artifact-name.war


ii. jenkins master == to schedule the builds

# Configure Jenkins :
#Go to Jenkins Master thru Browser.
43.204.100.29:8080  // to open gui for jenkins master

# to establish communication between jenkins master and QA server. so that artifact created in the build slave.m, to be copied to the QA server.

#Install Publish over ssh plugin : to publish artifact to the target machine

#Goto : Manage Jenkins > Plugin Manager > Publish over ssh Plugin > Install without restart [ to install plugin]

#Goto : Manage Jenkins > Configure System > Configure Publish over SSH > 
	SSH Server > add
		Name : QA-Server 
		Hostname : 172.31.45.0 [ private IP for QA-Server] you can also use public IP address
		Username : jenkin
		Remote directory : /opt/tomcat/webapps		// target directory for the artifact
	> advance settings > use password authentication or use different key > 
		key : copied private ssh key  [ created for user jenkin on QA-Server]
	> test configuration


#Create Jenkins Job to upload artifact
#Create Jenkins freestyle Job to Test the Deployment as discussed : test-pipeline-project1

test-pipeline-project1 [ created previously] > Pipeline > Definition > Pipeline script >
pipeline {

	agent {
		label ‘javanode1’
	}

	tools {
		// install the maven version configured as “M3” and add it to the path
		maven “Maven” // name from configuration below
// maven need to be configured on the master.m 
// manage jenkins > global tool configuration > add maven > [can’t be install automatically since it’s installed on slave.m] > Name : Maven 
		    MAVEN_HOME : /opt/apache-maven-3.8.6
	}

	stages {

		stage(’SCM Checkout’) {
			steps {
				echo ‘Java SRC’
				git ‘https://gitHub.com/LoksaiETA/Java-mvn-app2.git'  [ copy url from source code repository]
			}

		stage(‘Build’) {
			steps {
				sh “mvn -Dmaven.test.failure.ignore=true clean package”
			}
		}

		stage(‘Deploy to QA Environment’) {
			steps {
// how to create deployment script
// pipeline syntax > 
	sample step : sshPublisher:send build artifacts over SSH
		name : QA-Server
		source files : (/home/jenkins/workspace/test-pipeline-project1/)target/mvn-hello-world.war [ from build slave.m]
		remove prefix : target/
		remote directory : .
// note: here multiple targets can be configured
// by clicking add server	
				script {
					. . . . .
				}
			}
		}
	}
}
		+ > build now
		+ confirm the deployment to the QA-server, by checking the file in /opt/tomcat/webapps
/opt/tomcat/webapps/target/mvn-hello-world.war

publish over ssh plugin
pipeline1  === QA
pipeline2  === UAT deploy in cat

config email & create pipeline script for email
- how to notify the user!!! : email notification
	+ manage jenkins > configure system 
					E-mail notification >
						SMTP server : smtp.gmail.com
						Use SMTP authentication : Enabled
							user name : loksaivlog@gmail.com
							password : 
						use SSL : Enabled
						SMTP Port : 465
						Reply-To Address : anotherloksaivlog@gmail.com
	+ or generate email notification as part of pipeline
		email-validation > pipeline syntax > 
			Sample step : mail:Mail
				To : loksaivlog@gmail.com
				CC : loksaivlog@gmail.com
				BCC :
				Subject : Jenkins pipeline - Test email “Job ‘${JOB_NUMBER}’ (${BUILD_NUMBER}) is waiting for input”
				Body : Jenkins pipeline - Test email
					  “Please go to ${BUILD_URL} and verify the build”
			> generate pipeline script

stages{
	stage(){
	}
}
post{ // notification should not be created as part of a stage
	 // notification is a result of a particular stage e.g like post-build action
	always // or success or failure
	{
		// paste the generated email-notification script here
	}
}


Triggers
	test-pipeline-project1 > configuration > general > 
		Build triggers > Build periodically : Enabled [ schedule using crontab]

- use crontab.guru to schedule job
- you can create a script to carry out memory clean-up for jenkins-slave machine every week
- periodic build
- periodic deployment

building trigger based on source code change : when source code is modified
	+ build periodic : trigger build job based on your schedule even if there was no change in source code [ no commit]. crontab is used
	+ Poll SCM : trigger build job based on your schedule if there was any change in source code [ a commit] e.g i want to trigger a build for every one hour only if there is any commit happened in the src repo. crontab is used
	+ Github Webhook : is use for each of the commit, to execute build job. to use this, you must have git installed on your master.
		
	yum install git -y
	git - -version
	on GitHub.com > repository > settings > add webhook > add payload url [ jenkin url like 43.204.100.29:8080/] : 43.204.100.29:8080/github-webhook/
> content type : application/json > select individual event : Pushes > active : enabled
#Github webhook to automatically trigger build pipeline!
#Goto Github Repository setting, 
Select webhook, 
Click Add Webhook
Enter Jenkins Master URL. eg.:
http://<public-IP>:8080/github-webhook/
http://35.154.44.76:8080/github-webhook/
Choose the push event and save the webhook configuration


***********************
Build Periodic:
want to trigger a build for every one hour only
Intall GIT on your Jenkins Master!
	Poll SCM:
	want to trigger a build for every one hour only if there is any commit happened in the src repo 
	Github Webhook:
	want to trigger a build for every commit happened in the src repo 
DEV envi.
	Dev1 == commit to his src code repo --->>
	to perform some unit testing 
	
	His test cycle runs every 6 hrs.
	
		- Code promotion to QA 
		
	poll SCM !
	
		Done with QA Testing src code has been updated the the required commit 
		
	GITHUB Webhook !
	
	
Email Notification Plugins :::
SMTP Server :
smtp.gmail.com
SMTP Authentication
SMTP Port :: 465
Login to Gmail :::
Click Account Settings
Security 
Create App Token --- 16 



Summary :::
- complete jenkins module
- practice as much as possible to cover all the concepts
- master, slave, plugins, config system
- ci/cd pipeline
- publish over ssh publisher
- ssh connection
- config QA server with tomcat





++++++++++++++++++++++++++++++++++++ Configuration management using Ansible

Issues without a configuration management tool
- configuring : a large number of servers together becomes a daunting task
- scaling : new servers without a configuration managt. tool is difficult
- development and deployment : environment mismatch halts work

Configuration Management
- is all about bringing in consistency in the infrastructure
- this is done by ensuring that the current design system, state and environment is known, trusted and agreed upon by everyone.
- CM tools provides an easier approach to manage and configure servers
- writing individual scripts for server’s time and again just to get a few things done has become obsolete
- CM enables users to manage and configure the entire infrastructure and its environment

Infrastructure as Code (IaC)
- enables automation of IT Operations(build, deploy, manage) by provisioning of code, rather than manually handling each phase for different environments.
- it bridges the environment differences system admins encountered every time they had to either deploy new code or setup new servers.
- provisioning servers using IaC is easier than writing shell scripts
- shell scripts require workflow definitions whereas CM tool scripts have pre-defined workflows

About Schwarz group
- the Schwarz group is the largest european retailer
- it employs more than 458,000 employees in its 12,500 stores across 33 countries
- it comprises of two retail divisions: Lidi and Kaufland

the infrastructure issue
- managing the ever-growing number of stores was becoming difficult
- delivery times needed to be shorter to stay competitive in the market
- flexibility required to adapt to the changing local markets

the solution : ansible
- ansible enabled schwarz group to manage their stores more easily and reliably
- with the help of ansible, more stable digital features are pushed to the stores at a higher frequency
- enhanced risk management using the role-based system access

What is Ansible?
- Ansible is a deployment automation tool which traditionally uses push approach to achieve its objectives, by managing all the servers through one single machine running the ansible config. manage. tool.
- it uses the Push approach

Ansible Architecture
- Ansible connects to the nodes and pushes out small programs called ‘Modules’.
- these modules bring the systems to the desired state
- Ansible uses SSH to execute these modules and then removes them when finished
i. playbooks : Ansible uses playbooks to implement the changes desired by the users
- it contain plays, plays have tasks and tasks call modules
ii. inventory : it uses the inventory file to represent all the machines it is managing
iii. API : are there to support services like cloud and CLI
iv. plugins :  plugins act as extension to Ansible
v. cloud : cloud applications can seamlessly be integrated with Ansible
- Ansible makes provisioning cloud infrastructures easy
vi. hosts and network : 
- hosts are connected to the Ansible system via secure SSH connection
- different networks can be managed together giving each network separate access rights

Automation using ansible

Centralizing configuration management
- configuration manage. without an automation tool like ansible requires a lot of manual labour.
- with Ansible users can automate and bring the servers to the desired state with a single automation script

Automation using Ansible
- the desired state of the server is mentioned in the automation script
- the ansible automation engine handles the differences between the host servers
- it allows users to automate their environment using two different ways:

i. Ad-Hoc commands : are used to accomplish tasks quickly
- it uses the ‘ansible’ command line tool to execute tasks
- these commands can be executed on single or multiple hosts using any inventory file
- any ansible module can be used to run as an ad-hoc task

why use ad-hoc commands?
- used to execute one-off tasks
- quick and easy to execute
- non-reusable
- for example : tasks like rebooting servers, copying files, gathering facts, etc. can be easily done using a single ad-hoc command

- syntax:
ansible [pattern] -m [module] -a “[module options]”
	+ pattern : hosts/host group the command needs to be executed on
	+ module : module to be executed
	+ module options : module arguments
ansible frontend -m service -a “name=nginx state=restarted”

ii. Playbooks : are the access point to Ansible provisioning
- Ansible’s way of deploying and configuring different remote servers and environments
- can be used to 
	+ handle multi-tier rollouts
	+ load balancing tasks for the servers
- Playbooks : modules - are units of code which get the work done in ansible. they are used to control system resources like services, packages, files or execute system commands.
	+ modules abstract system tasks, like dealing with packages or handling files from ansible servers.
	+ users can choose to either use a module from Ansible’s in-built library or create their own custom modules
	+ almost all the modules support taking arguments in key=value format

	- name: Stop httpd service
	  service:  		    -—-> module
		name: httpd 	    ——-> module arguments
		state: stopped 

Playbook structure
- - -     			> signify the beginning of a playbook
- hosts: all 			> play in a pb starts with the host(s)
  become: yes 
  become_user: desmond  	> tell Ans to become user desmond
  become_method: su  	> run tasks with sudo privileges
  vars:    			> variables need to be defined b4 their use
    region: northeast
  tasks: 				> tasks to be executed on remote system
  [BLOCK] 			> to create groups of tasks
  - name: add docker repo
    apt_repository:
       repo: deb [arch=amd64] https://...
       state: present
    notify: 			> actions which are triggered at end of play
	  - update registry
  handlers: 			> notify triggers call upon tasks here
  - name: update registry
    apt:
       update_cache: yes

Variables in Playbooks
- variables are ansibles way to deal with differences between different systems and environments
- they can handle major/minor differences between systems with same configuration
- users can also collect facts from other remote systems and store them in variables
- variables can be defined and used anywhere in ansible:
	+ playbooks
	+ ad-hoc command
	+ inventory

Registering variables
- outputs from other tasks can be stored and used as variables for other variables

Handlers
- are tasks that execute only if the system configuration changes because of a task
- they only execute if a task notifies it for execution
- handlers require a globally unique name for identification
- controlling handlers : 
	+ handlers always run after a play has completed its tasks execution
	+ this ensures that handlers run only once even on multiple callings

Ansible roles
- overtime working with ansible a user may create hundreds of playbooks, variables, templates, defaults etc. roles allow users to group this logic into an organized manner making reusability and sharing of ansible structure easier.
- role uses directories to structure and group all the playbooks, variables, templates, tasks, handlers, files and dafaults
- this collected logic can be grouped in any way the user wants, for example you can group server specific roles together
- these roles can then be used inside playbooks and even as in-line commands

To install Ansible on a CentOS machine, follow these steps:
	1	- Update the system:
	+ Before installing Ansible, it's a good practice to update your CentOS system to ensure you have the latest security patches and updates.
	sudo yum update -y
	1	- Install the EPEL repository:
	+ Ansible is available in the Extra Packages for Enterprise Linux (EPEL) repository. You need to install the EPEL repository before installing Ansible.
	sudo yum install -y epel-release
	1	- Install Ansible:
	+ After enabling the EPEL repository, you can now install Ansible using the package manager.
	sudo yum install -y ansible
	1	- Verify the installation:
	+ Once Ansible is installed, you can verify the installation by checking the Ansible version.
	ansible --version



Edureka practice

Jenkins Master!!!

slave1 == java
slave2 == .Net
. . . 10 slave


in CI/CD
create n number of test servers
test servers : dynamically create test servers and config the test server with req. testing tool like jdk, selenium, junit

IAC == Infrastructure As Code tools e.g Terraform, Ansible, chef, puppet

your role
- install & consider ansible tool
- consider as client server architecture
	+ ansible controller machine. ansible is only installed here.
	+ ansible targets
		jenkins master/slave
		VM’s
- between controller and target we use SSH connectivity to communicate

Ansible inventory file : is a text file that contains information about the hosts and groups that Ansible can connect to and manage.

inventory file : appinventory

localhost

y.y.y.y
z.z.z.z

[mail]
a.a.a.a
b.b.b.b

[db]
c.c.c.c
d.d.d.d


playbook : is ansible configuration file. a single YAML file
-
	name: Play 1    // name of playbook
	hosts: localhost  
	tasks:
	  	- name: Execute command ‘date’
	    	  command: date
	  	- name: Execute script on server
	       script: test_script.sh

	name: Play 2
	hosts: localhost
	tasks:
		- name: Install web service
	   	  yum:
				name: httpd
				state: present

		  name: Start web server
		  service:
			   	name: httpd
				state: started

- play - defines a set of activities (tasks) to be run on hosts
- task - an action to be performed on the host
	+ execute a command
	+ run a script
	+ install a package
	+ shutdown/restart
- module is set of program you want run on target machine

- To summarize, the inventory file defines the target hosts/servers, while the playbook file defines the tasks to be performed on those hosts.


Ansible Installation & Configurations:

Launch 3 EC2 Instances... 
	- 1 for Ansible Controller : Ansible-Controller
	- 2 as Nodes : Ansible-Node1 & Ansible-Node2

SSH connections :::

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Login to Ansible Node1 & Node2. Perform below activities:

[ec2-user@ip-172-31-45-01 ~]$ sudo -i
[root@ip-172-31-45-01 ~]# yum update -y

- the minimum requirement for target is that it must have a user

#Add User in Ansible Nodes : 

[root@ip-172-31-45-01 ~]# useradd -m -d /home/ansibleadmin ansibleadmin // create ansible user with the directory
[root@ip-172-31-45-01 ~]# su - ansibleadmin

[ansibleadmin@ip-172-31-45-01 ~]$ ssh-keygen   // creating ssh key for ansible user
[ansibleadmin@ip-172-31-45-01 ~]$ ls ~/.ssh 
#You should see following two files:
#id_rsa - private key
#id_rsa.pub - public

[ansibleadmin@ip-172-31-45-01 ~]$ cd /home/ansibleadmin/.ssh
[ansibleadmin@ip-172-31-45-01 .ssh]$ cat id_rsa.pub > authorized_keys

[ansibleadmin@ip-172-31-45-01 .ssh]$ chown -R ansibleadmin /home/ansibleadmin/.ssh
[ansibleadmin@ip-172-31-45-01 .ssh]$ chmod 600 /home/ansibleadmin/.ssh/authorized_keys
[ansibleadmin@ip-172-31-45-01 .ssh]$ chmod 700 /home/ansibleadmin/.ssh

[ansibleadmin@ip-172-31-45-01 .ssh]$ cd ~
[ansibleadmin@ip-172-31-45-01 ~]$ exit

[root@ip-172-31-45-01 ~]# 

#As a root user edit below file:

[root@ip-172-31-45-01 ~]# visudo

#add the below mentioned line in the file and save it.
#as a root user, it will not ask for password
root       ALL=(ALL)     ALL
ansibleadmin ALL=(ALL) NOPASSWD: ALL
#devopsadmin ALL=(ALL) NOPASSWD: ALL

[root@ip-172-31-45-01 ~]# su - ansibleadmin

## create another node


~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#Login to Ansible Controller:

- ansible is only installed on the controller machine and that user must have direct access to ansible.

#Install epel Package :
#if you’re not using  amazon-linux then can’t install epel package.
#need to install python (which is included in epel), required for ansible.
#Install Ansible 

[ec2-user@ip-172-31-45-211 ~]$ sudo -i
[root@ip-172-31-45-211 ~]# yum update -y

[root@ip-172-31-45-211 ~]# amazon-linux-extras install epel -y
[root@ip-172-31-45-211 ~]# amazon-linux-extras install ansible2 -y // on centos : sudo yum install ansible
[root@ip-172-31-45-211 ~]# ansible - -version
## location path : /etc/ansible/ansible.cfg


#go to /etc/ansible/
[root@ip-172-31-45-211 ~]# cd /etc/ansible/
## by default config and inventory details are created in this file
#host - inventory file
#config
#roles 

#Add User in Ansible Controller : 

- in ansible machine we only need to create one user

root@ip-172-31-45-211 ~]# useradd -m -d /home/devopsadmin devopsadmin // create ansible user with the directory


# change the owner from root to devopsadmin

[root@ip-172-31-45-211 ~]# su - devopsadmin

[devopsadmin@ip-172-31-45-211 ~]$


##Create node keys dir under /etc/ansible to maintain the private key files of the nodes

[devopsadmin@ip-172-31-45-211 ~]$ mkdir ansiblenodekeys // directory to hold private keys

[devopsadmin@ip-172-31-45-211 ~]$ cd ansiblenodekeys

[devopsadmin@ip-172-31-45-211 ansiblenodekeys]$

[devopsadmin@ip-172-31-45-211 ansiblenodekeys]$ vi ansiblenode1.key 
### copy & paste the private key of ansible-node1

[devopsadmin@ip-172-31-45-211 ansiblenodekeys]$ vi ansiblenode2.key 
### copy & paste the private key of ansible-node2

##go back to ansible node to copy the private key to be copied into above file for each user
[ansibleadmin@ip-172-31-45-211 .ssh]$ cat id_rsa

## to make devopsadmin only user has access to these files 
[devopsadmin@ip-172-31-45-211 ansiblenodekeys]$ chmod 600 ansiblenode1.key
[devopsadmin@ip-172-31-45-211 ansiblenodekeys]$ chmod 600 ansiblenode2.key



#As a root user edit below file:
[root@ip-172-31-45-211 ~]# visudo

#add the below mentioned line in the file and save it.
devopsadmin ALL=(ALL) NOPASSWD: ALL

# now devopsadmin has the same privilege/access as root
[devopsadmin@ip-172-31-45-211 ~]$ cd /etc/ansible/
[devopsadmin@ip-172-31-45-211 ansible]$ li


###test ssh connection from ansible host/controller to remote hosts

[devopsadmin@ip-172-31-45-211 ansible]$ ssh -p22 -i /etc/ansible/ansiblenodekeys/ansiblenode1.key ansibleadmin@172.31.45.01    //  connect on port [=22] using ansiblenode1.key present in the controller.m, to ansibleadmin == user, on 172.31.45.01 == target machine

//command prompt will change to 
[ansibleadmin@ip-172-31-45-01 ~]$

//to get back to controller machine
[ansibleadmin@ip-172-31-45-01 ~]$ exit

[devopsadmin@ip-172-31-45-211 ansible]$

### test ssh connection from controller.m to another target.m
[devopsadmin@ip-172-31-45-211 ansible]$ ssh -p22 -i /etc/ansible/ansiblenodekeys/ansiblenode1.key ansibleadmin@172.31.45.02    //  connect on port [=22] using ansiblenode1.key present in the controller.m, to ansibleadmin == user, on 172.31.45.02 == target machine

# before you run any command ansible.c.m should be able to connect to your ansible.t.m



######## Ansible Adhoc-command
- using ansible adhoc command to quickly connect to machine
- write adhoc command with various modules
	
 ansible testnodes -m ping  // where testnodes is inventory file, where target hosts IP addresses are configured

- to locate inventory file on the controller machine

[devopsadmin@ip-172-31-45-211 ~]$ cd /etc/ansible/
[devopsadmin@ip-172-31-45-211 ansible]$ vi host  // that has target.m IP addresses

#hosts file is the default Inventory file for ansible 
###update vi /etc/ansible/host to contain group called testnodes.

[testnodes]
samplenode1 ansible_ssh_host=172.31.45.01 ansible_ssh_user=ansibleadmin ansible_ssh_private_key_file=/etc/ansible/ansiblenodekeys/ansiblenode1.key
samplenode2 ansible_ssh_host=172.31.45.02 ansible_ssh_user=ansibleadmin ansible_ssh_private_key_file=/etc/ansible/ansiblenodekeys/ansiblenode1.key

// then save the host file

## to test connection to the target.m 
[devopsadmin@ip-172-31-45-211 ansible]$ ansible testnodes -m ping // connection to both nodes
[devopsadmin@ip-172-31-45-211 ansible]$ ansible samplenode1 -m ping // connection to one node
[devopsadmin@ip-172-31-45-211 ansible]$ ansible all -m ping // connects to all available hosts from /etc/ansible/hosts file

#host machines can be idenfified using :
#all | group_name | individual_host_name


## can also use password authentication instead of SSH
samplenode1 ansible_ssh_host=172.31.45.01 ansible_ssh_user=ansibleadmin ansible_ssh_pass=P@###

[devopsadmin@ip-172-31-45-211 ansible]$ 

/etc/ansible/dev_inventory

[devnodes]
sampledevnode1 ansible_ssh_host=172.31.41.32 ansible_ssh_user=ansibleadmin
sampledevnode2 ansible_ssh_host=172.31.39.17 ansible_ssh_user=ansibleadmin

ansible devnodes -m ping -i /etc/ansible/dev_inventory


### Ansible modules : includes setup, copy, fetch, user, command, shell, ping, service etc

i. ping : to check connection between nodes

# to see the default value including inventory,
#inventory = /etc/ansible/hosts
[devopsadmin@ip-172-31-45-211 ansible]$ cat ansible.cfg

Manage Infrastructure

DEV - - dev-inventory

TEST - - test-inventory


#using user defined inventory file
#ansible ansible-node1 -m ping -i myinventoryfile.txt

ii. setup : collect information about the target machines. to get ansible facts or to pull system information.

[devopsadmin@ip-172-31-45-211 ansible]$ ansible samplenode1 -m setup
[devopsadmin@ip-172-31-45-211 ansible]$ ansible samplenode1 -m setup -a “filter=ansible_mounts”
// using filter option
 

iii. shell : to write any shell command on any target machine

[devopsadmin@ip-172-31-45-211 ansible]$ ansible testnodes -m shell -a “sleep 5 ; echo ‘hi’” 
// where testnodes is group name
// this will be published in the target machine
[ansibleadmin@ip-172-31-45-01 ~]$ cd .ansible/   // home directory only contains  authorized_keys file
[ansibleadmin@ip-172-31-45-01 .ansible]$ cd tmp/	// this directory was created by shell module
// it contains python file that will be deleted

[ansibleadmin@ip-172-31-45-01 .ansible]$ ANSIBLE_KEEP_REMOTE_FILES=1 ansible testnodes -m shell -a “uptime”  // the created python file will not be deleted

iv. copy : transfer a file from ansible controller to nodes using copy module

ansible samplenode1 -m copy -a “src=/etc/ansible/playbooks/samplefile.txt dest=/home/ansibleadmin”
ansible samplenode1 -m copy -a “src=/etc/ansible/testfile1.txt dest=/home/ansibleadmin backup=yes”

v. fetch : Transfer a file from Ansible Nodes to Ansible Controller using fetch Module

ansible samplenode1 -m fetch -a "src=/home/ansibleadmin/filefrom-AN1 dest=/home/devopsadmin"


#Adhoc Commands with various Modules
#How Ansible Works?
#Ansible Facts & Variables

#######Adhoc Commands with various Modules

###Test Connection:
ansible testnodes -m ping

### Test Uptime of Nodes:
ansible testnodes -m shell -a "uptime"

### Pass Multiple Groups:
ansible testnodes1:testnodes2 -m shell -a "uptime"

### Check RAM Utilization:
ansible testnodes -m shell -a "free -m"

###Pass Different Inventory File:
ansible -i inventory_file_name prodnodes -m shell -a "uptime"


### List all modules:
ansible-doc -l
### No. of modules:
ansible-doc -l | wc -l
### Search for specific modules:
ansible-doc -l | grep shell
### To know about any specific modules:
ansible-doc shell

239278249

######## Ansible playbook : playbook are use to run series of actions

#Ansible Variables !
#shell : echo $var1 
#yaml : "{{var1}}" ==> 

// here you want to pull system information (using setup) and display to playbook
// to get information about target.m
debugmod.yaml
- - -        			// beginning of playbook
    - hosts: samplenode1 // defined in the inventory file. can also use samplenode2 or all
	 tasks:
	 - debug:			// module to print output message like print/echo
	    msg: 
		- “The os distribution is: {{ansible_distribution}}” // var passed using {{}}
		- “The os name is: {{ansible_system}}”
		- “The os family is: {{ansible_os_family}}”
		- “The mount points are : {{ansible_mounts}}”

[devopsadmin@ip-172-31-45-211 ansible]$ mkdir playbooksdir
[devopsadmin@ip-172-31-45-211 playbooksdir]$ vi debugmod.yaml // save using wq
[devopsadmin@ip-172-31-45-211 playbooksdir]$ ansible-playbook debugmod.yaml // to run playbook

# How to just verify playbook syntax:
#ansible-playbook testfile.yaml --syntax-check

#echo "Hello"
#echo "${var1}"

ansible testnodes -m shell -a "uptime"

"{{ansible_distribution}}" - Jinja2 Templates 

echo "$var1"
"${var1}"



handling variables in ansible !!!


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#~~~~~~~~~~~~~~~~~~~~~~~~~ test_var-datatype.yaml
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#test_var-datatype.yaml
---
 - hosts: samplenode1
   vars:
    x: 23
    my_num: 45.67
    my_name: Loksai
    my_b: YES   
   tasks:
   - debug:
      msg:
       - "The value of x is: {{x}} and type: {{x|type_debug}}"
       - "THe value of my_num: {{my_num}} and type : {{my_num|type_debug}}"
       - "The value of my_name : {{my_name}} and type: {{my_name|type_debug}}"
       - "THe value of my_b is: {{my_b}} and type : {{my_b|type_debug}}"

[devopsadmin@ip-172-31-45-211 playbooksdir]$ vi test_var-datatype.yaml
[devopsadmin@ip-172-31-45-211 playbooksdir]$ ansible-playbook test_var-datatype.yaml


++++++++++++ register and set-facts

#shellmod.yaml
---
 - hosts: samplenode1
   gather_facts: false
   tasks:
   - shell: “bash --version”

#shellmod.yaml
---
 - hosts: samplenode1
   gather_facts: false
   tasks:
   - shell: “bash --version”
	register: bash_ver     // the output of this shell task will register into this variable

#register_set_facts
#shellmod.yaml
---
 - hosts: samplenode1
   gather_facts: false
   tasks:
   - shell: “bash --version”
	register: bash_ver 
   - set_fact:			// to format the output and assigned it to new variable, which can be printed or processed. formatting means picking some info from large details 
	  bash_version: “{{bash_ver.stdout.split(‘\n’)[0].split()[3]}}”   
	  my_value: “bash_version”
   - debug: var=bash_version
// use register and set-facts together

[devopsadmin@ip-172-31-45-211 playbooksdir]$ vi shellmod.yaml
[devopsadmin@ip-172-31-45-211 playbooksdir]$ ansible-playbook shellmod.yaml



Become : to give root user access for some task

// to perform complete installation of this httpd (just like tomcat) service
#bcome1.yaml
---
 - hosts: samplenode1
   gather_facts: false
   become: yes       // this gives you root user access level
   tasks:
   - name: Install httpd // task name can be anything. it does nothing except for identification
	yum:				// in ubuntu use apt. it’s a module
	  name: httpd		// package to be installed
	  state: present    // will define the current state of a particular tool. it will determine what task like install, remove, update to be performed. it can have value like present(ensure this tool is present in this machine), absent or latest.

[devopsadmin@ip-172-31-45-211 playbooksdir]$ ansible-playbook bcome1.yaml


Handlers : is a kind of task based upon result of previous task


---
 - hosts: samplenode1
   gather_facts: false
   become: yes      
   tasks:
   - name: Install httpd 
	yum:				
	  name: httpd		
	  state: present          // install service
	register: resultt
   - debug: var=resultt.changed
   - name: start httpd
	service:				// use service module to start or stop or restart
	   name: httpd
	   state: started		// start service. value are started or stopped or restarted
	when: results.changed == True // condition to run second task


// playbook without handler
---
 - hosts: samplenode1
   become: yes      
   tasks:
   - name: Install httpd 
	yum:				
	  name: httpd		
	  state: present 		# CHANGE=0
   - name: start httpd
	service:
	   name: httpd
	   state: started		# CHANGE=0

// playbook with handler
---
 - hosts: samplenode1
   gather_facts: false
   become: yes      
   tasks:
   - name: Install httpd 
	yum:				
	  name: httpd		
	  state: present 		# CHANGE=1
	notify:				// based upon the change run next task. it pickup the handler
	  - start httpd
   handlers:
   - name: start httpd
	service:
	   name: httpd
	   state: started	

Loops :

// playbook without loop. installing 3 different services.
---
 - hosts: samplenode1
   become: yes      
   tasks:
     - yum: 			
	    name: git
	    state: present
     - yum: 			
	    name: httpd
	    state: present
     - yum: 			
	    name: vim
	    state: present


// playbook with loop
---
 - hosts: samplenode1
   gather_facts: false
   become: yes      
   tasks:
     - yum: 			
	    name: “{{item}}”
	    state: absent	
	  loop:
	    - git
	    - httpd
	    - vim


When :

---
 - name: Install NGINX
   hosts: all     
   tasks:
     - name: Install NGINX on Debian
	  apt: 			
	    name: nginx
	    state: present
	  when: ansible_os_family == “Debian”
     - name: Install NGINX on Redhat
	  apt: 			
	    name: nginx
	    state: present
	  when: ansible_os_family == “Redhat”


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#/etc/ansible/variables/varfile_datatype1.yaml
    x: 23
    my_num: 45.67
    my_name: Loksai_ETA
    my_b: YES
	pswd: *&****&^^^^

#test_var-datatype1.yaml
---
 - hosts: samplenode1
   vars_files:
     - /etc/ansible/variables/varfile_datatype1.yaml
   tasks:
   - debug:
      msg:
       - "The value of x is: {{x}} and type: {{x|type_debug}}"
       - "THe value of my_num: {{my_num}} and type : {{my_num|type_debug}}"
       - "The value of my_name : {{my_name}} and type: {{my_name|type_debug}}"
       - "THe value of my_b is: {{my_b}} and type : {{my_b|type_debug}}"

ansible-playbook test_var-datatype1.yaml

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#ansible-playbook var_datatype1.yaml -e "x = 100 my_num = 50.50 my_name = Loksai_ETA my_b = NO"
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~	   
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#test_varfile1.yaml
---
 - hosts: "{{ host_name }}"
   become: yes
   tasks:
   - name: Manage "{{ tool_name }}" service
     yum:
       name: "{{ tool_name }}"
       state: "{{ tool_state }}"

install
remove
upgrade 

ansible-playbook manage_pkg_var.yaml -e "host_name=samplenode1 tool_name=git tool_state=present"

---
 - hosts: samplenode1
   become: yes
   tasks:
   - name: Install git service
     yum:
       name: git
       state: present

yum module ::: 
	name :		git 
	state :		present / absent / latest 








playbook :: to copy your artifacts from jenkins slave to qa-server

artifact here is man-hello-world.war

::::

	Deployment to QA_Server :
	
		Jenkins_Master :
			Slave1 -> build & *.war
			
		Use Ansible to deploy this *.war file to QA_Server 
		
		Ansible Contoller:		*.war or create docker img -- dockerhub/ kubernetes
			Jenkins_Slave			*.war     fetch module
			QA_Server 				*.war	  copy module
			
		Artifactory ::: 
			JFrog ==> Is used to version control the artifacts.

UseCase 1 :
	- Deploy an artifact *.war from Jenkins Slave Machine to QA-Server.



Summary:

Ansible
Install Ansible
Inventory
Modules
Playbook
	handler, loop, var, conditions,
	roles

Ansible roles ::

Intranet: repositories for managing the playbooks
Ansible playbooks =====> repository
Internet : access the playbooks

roles are the packages of ansible components / template

servers
reuse
vs code

- ansible roles are used to augment ansible components, these are reuseable
[devopsadmin@ip-172-31-45-211 ansible]$ cd roles/
[devopsadmin@ip-172-31-45-211 roles]$   //all the roles can be define by default here. 
// role is defined using this command : ansible-galaxy
// this cmd clone templates from GitHub repo when used with install based on galaxy.ansible.com
// $ ansible-galaxy init mysql
// by default are defined in /etc/ansible/ansible.cfg
// roles_path = /etc/ansible/roles

[devopsadmin@ip-172-31-45-211 roles]$ ansible-galaxy init role1
[devopsadmin@ip-172-31-45-211 roles]$ cd role1/
[devopsadmin@ip-172-31-45-211 role1]$ ls
// contains following directories: default, files, handlers, meta, tasks, templates, tests, vars
// these are all ansible components that are augmented in the created role
// check out galaxy.ansible.com






++++++++++++++++++++++++++++++++++++  DOCKER

Yelp
- Yelp is a reviews and recommendations company based out of America
- the company as of 2019 had hosted over 192 million user reviews
- it hosts over 178 million active monthly users across different platforms

Yelp: Monolithic architecture
- yelp originally was developed in monolithic architecture 
- this disabled the application from bringing about major changes for the ever-growing user base
- in order to adapt, yelp has been trying to move its newer services to AWS
- but a huge base of the application’s critical services still runs in-house which cannot be readily moved to cloud 

docker bridges the gap
- docker helped yelp to smooth out the differences between its traditional data centers and AWS instances
- the easy-to-use nature of docker enabled yelp to containerize its critical services and then move them to AWS
- docker has now become a very integral part of yelp’s infrastructure
- it has enabled developers to be more independent, thus making the deployments that much faster


Introduction to Containerization

What is Containerization?
- It is the process of packaging the application and all its dependencies to execute them in an efficient way across different environments. a single unit of this bundled package is known as a container.

Why use containers?
- performance overhead : containers work on top host OS’s kernel, therefore, there is little to no performance overhead
- easily manageable : since all the dependencies run inside an isolated instance, it is easy to manipulate and make changes to the application
- platform independent : containers can be deployed to platforms with different network topologies and security policies without any hassle
- instant boot : containerized application has zero boot time making them available instantaneously
- modularity : depending upon the approach, containers work seamlessly in a monolithic as well as the microservice environment

what is a Container?
- It is an operating system level virtualization technology
- detaches the application and its dependencies from the rest of the system.
- utilizes namespaces(Na) and cgroups(CG) feature of linux kernel to isolate processes

Container : Working
- containers utilize two of the linux kernel features : 
	
	+ cgroups : allows segregating the processes and the required resources
	+ namespaces : process is unaware of everything running outside of its own namespace.
- a container is just another process for the host operating system
- works in contained environment under the OS
- gets restricted view and access to other system processes, resources and environments

Namespace
- provides a system resource, a layer of abstraction
- application presumes it has an isolated instance of the resource
- there are six types of namespaces

Type of namespaces : 
	+ PID namespace
	+ NET namespace
	+ MNT namespace
	+ UTS namespace
	+ IPC namespace
	+ USER namespace

Containers vs virtual machines
i. number of apps on server : can run a limited no of app, depending on VM config. // can run multiple instances of different app together
ii. resource overhead : lightweight with little to no overhead // highly resource-intensive with performance overhead of managing multiple Oss
iii. very fast deployment. container images require seconds to deploy new instances // very slow deployment. requires setting up OS, app dependencies etc
iv. security : users usually require root level permissions to execute container related tasks // provides better security
v. portability : highly portable with emphasis on consistency across environments // very limited portability

Why use containers?
- containers are light compared to a VM, as they directly utilize the host operating system’s kernel
- for a single vm instance, the machine hardware deals with 3 separate kernels

types of containers
	+ OS container : runs & functions as complete OS. e.g LXC, Linux Server
	+ Application containers : runs as an application isolated from the rest of the system e.g Docker, rkt

Introduction to Docker
- Docker is one of the most popular container engines today because of the way it handles containers.

Docker : features
i. fast configuration : the set up process is very quick and easy. it separates the requirements of the infra from the requirements of the application
ii. productivity : rapid deployment, in turn, increases the productivity. reduced resource utilization is another factor increasing the productivity
iii. application isolation : provides applications/ service isolation. appln. inside containers execute independent of the rest of the system
iv. swarm : it helps clustering and scheduling docker containers. it enables controlling cluster of docker hosts from a single point
v. services : services use a set of tasks to define the state of a container inside a cluster. swarm manager takes the defined service as input and schedules it on the nodes
vi. security : allows saving secrets inside the swarm cluster. these secrets can then be accessed by the required service
vii. service discovery : mesh allows service discovery through the same port on all the nodes in swarm. it is possible to reach the node even if the service is not deployed on it




Docker : architectures
- Docker works in a client-server architecture
- the client sends the control commands to the Docker daemon through a REST API
- Docker daemon is responsible for building, running and distributing the containers
- the client and daemon can be on the same system or the client can connect to a remote daemon 

Container lifecycle
- a newly created container can be in one of six states: 
	+ created : a container that has been created but not yet started 
	+ running : a container in its normal working state
	+ paused : container whose processes have been paused at the moment 
	+ stopped : a container that is no longer working. also known as exited
	+ restarted : a previously stopped container which is now being restarted 
	+ dead : a container which is no longer in use and is discarded


Docker CE vs Docker EE

The Docker Engine
Docker engine : is responsible for all the building, scheduling and management of containers.
Docker engine : components
- the docker engine works in a client-server architecture utilizing the following major components :
	
	+ server: a long-running program which is known as a daemon process (docker)
	+ REST API : an API which defines the interfaces the programs can use to communicate with the daemon
	+ CLI : command interface to pass instruction to the daemon
Docker engine : working
- the cli uses the REST API to control and interact with the docker daemon
- cli does this through either using scripts or direct commands
- daemon is responsible for creating and managing docker objects such as container, images, network and volumes

Docker terminology
- container : a software that encapsulate the application and its dependencies in a single packaged unit
- images : dormant package which acts as a blueprint to create containers
- dockerfile : script with instructions and commands to build the application image
- volumes : provide containers a persistent place to store the data
- registry : enables users to share docker images with others
- compose : a utility which enables multiple containers to run as a single service
- swarm - clustering and scheduling tool for orchestrating production environments



Docker Command line interface 
- docker offers a cli to manage and interact with containers.
- docker commands can be prefaced to run with or without sudo permissions.

docker CLI configuration
- docker commands can be prefaced to run with or without “sudo” permissions
- the command line config files are stored in .docker directory inside $HOME directory
- the default docker command behavior can be altered using the config.json file	

Common docker commands
- docker run : creates a container from an image
- docker start : starts an already stopped container(s)
- docker stop : stop an active container
- docker build : builds a docker image from a dockerfile
- docker pull : pulls pre-created images from a specified repository
- docker push : push images to the specified repository
- docker export : exports containers filesystem to a .tar archive file
- docker images : list the docker images currently on the local system
- docker search : searches repository for the specified image
- docker ps : lists all active container without any grace period to shut down its processes
- docker kill : kills an active container without any grace period to shut down it processes
- docker commit : creates a new image out of an already active container
- docker login : command to login to docker hub repository
docker container exec
- docker enables users to execute commands or interact with the container shell directly, using docker exec command.


Port Binding
Why we need port binding?
- by default docker containers can connect to the public internet without requiring any configurations
- but the public internet does not know how to connect with the containerized service

what is port binding?
Port binding
- it’s the process of exposing the required docker container port and binding it to a port on the system.
- also known as port forwarding or port mapping
- by default docker containers can connect to the public internet without requiring any configurations
- but the public internet does not know how to connect with the containerized service
- the container port can be exposed using the -p flag while starting the container
- this exposes the port on the default system IP i.e 0.0.0.0
	docker run -p <system_port>:<container_port> -d <containerName> <imageName>
- in order to expose the port on a specific IP, use the following syntax
	docker run -p <IP>:<system_port>:<container_port> -d <containerName> <imageName>


Docker container running mode
- detached: the container runs in the background. 
- foreground or attached: the container attaches itself to the terminal.

detached vs foreground
- in the detached mode, the container process executes in the background // in the attached mode, the container application attaches itself to the console
- the container exits when the root process starting the container ends // the process can be connected to any of STDIN, STDOUT, and/or STDERR streams
- the ‘-d’ flag is used to start the container in detached.m // all containers execute in foreground mode if the ‘-d’ flag is not specified.
- example : docker run -d -p 80:80 imageName // docker run - -rm -p 80:80 imageName

detached mode
- using the - -rm flag with the detached container exits it when the daemon stops	
- input/output actions on detached containers can  be done by connecting through network
- a detached container can be reattached to the terminal by using the ‘attach’ command

foreground mode
- an attached container is usually started in TTY mode, which is done using the -t flag
- by default, docker attaches the container to STDOUT and STDERR streams
- the -t flag must not be used when providing input through a pipe ‘|’

dockerfile instruction
- the docker image is made of read-only layers
- each of the layers corresponds to an instruction in the dockerfile
- these layers, when stacked together, represent an image
- executing the image and generating a container adds a writable layer on top




++++++++++++++ Image management and registry

Dockerfile reference
- docker builds images by taking instructions from a file called the dockerfile.
- the docker build command executes all the instructions in sequence to create the image.
dockerfile: working
- PATH is used for local files and URL for is used for remote repositories.
dockerfile.

dockerfile instruction
- the docker image is made of read-only layers
- each of the layers corresponds to an instruction in the dockerfile
- executing the image and generating a container adds a writable layer on top.

Dockerfile : the instruction and its layer
	FROM ubuntu: 18.04
	RUN    apt install -y apache2
	COPY  index.html /var/www/html/
	CMD    [“/usr/sbin/httpd”, “-D”, “FOREGROUND”]
	EXPOSE   80
- FROM : create the bottom most layer of ubuntu 18.04 for the image
- RUN : installs apache https server on top of the ubuntu layer
- COPY : copies files from the local directory to the container
- CMD : specifies the command to run when the container is live
- EXPOSE : exposes the container port to the system

Dockerfile: build context
- while issuing the build command for a Dockerfile, the current working directory is considered as the build context. The docker daemon collects all the files from the build context to build the image.
- the build command is handled by the daemon and not the cli
- to exclude files while executing COPY instruction, use .dockerignore file in the context directory
- use the -t flag to specify the repository and tag the image
- the -t flag can also be used to store the image in multiple repositories
Docker Image
- It’s a file, consisting of multiple layers, that is executed to create a container
- the image is built from the instructions in the dockerfile
Docker base image
- it is the basic image on top of which layers are added
- example: to run the LAMP stack, the user will require a base image of Linux OS and then subsequent layers of Apache, MySQL and PHP are added on top
docker image tags
- docker enables users to tag the image IDs with aliases which are easier to remember
- docker images can be tagged in one of two ways:
	+ while building an image
	docker build . -t username/imageName:tagName
	+ tagging an existing image
	docker tag imageID username/imageName:tagName

Managing docker images
- docker images are portable and can be distribute amongst the entire organization making it very accessible
- the easiest way to make these images available to others is by using a docker registry.
- a registry holds named Docker images for content delivery and storage.
- the registry can be configured by creating  a new configuration in YAML format.
- the images can be distributed using either of the following
	i. docker hub : is a cloud-based docker registry.
	+ it’s a public repository for hosting, building and testing docker images
	ii. private registry ( Quay.io, Google container registry, Artifactory, Amazon ECR registry, Sonatype nexus ). 

Pulling and pushing images

i. Pulling images from docker hub
- when the docker run command is executed:	
	+ docker searches for the corresponding image on the local system
	+ if not found, docker automatically pulls the image from the docker hub registry to create the container
	+ pulling an image from a private repository requires authentication
	docker pull registryName/imageName

ii. Pushing images to docker hub
- In order to push images to docker hub:
	+ create a docker hub account
	+ create a new repository
	+ login to the repo from Docker CLI and push the image
	docker login —username=yourUsername —email=yourEmail
	+ to push a custom image to docker hub use
	docker push username/imageName

Deploying a local docker registry
- to start a registry on the local system, it first requires a registry container running locally
	docker run -d -p 5000:5000 —restart=always —name registry registry:2
Multi-stage builds
- helps optimize dockerfiles while maintaining their readability.

Storage and Volumes in docker

Storage in docker
- docker by default stores all its data on a writable layer inside the container itself
- this data is not persistent and is lost if the container no logger exists
- writing directly into containers requires a storage driver which reduces performance

Volumes
- are the most reliable way to store persistent data for docker containers
- they can be easily managed using the Docker CLI commands
- they can be easily shared amongst multiple containers
Creating and managing volumes
- volumes can be created outside the scope of containers
- new volumes can also be pre-populated by existing container
	# to create a new volume
	docker volume create <volumeName>
	# to list volumes
	docker volume ls
	# to inspect the volume specifications
	docker volume inspect <volimeName>
	# to remove a volume
	docker volume rm <volumeName>

Starting container with a volume
- containers can be assigned a volume when they are created 
- in case a volume does not exist, docker creates a new volume from scratch
# to start a container with a volume using —mount flag
docker run -d \ 
	—name <containerName> \
	—mount source=<volumeName>, dst=<containerPath> \
	nginx:latest
# to start a container with a volume using -v flag
docker run -d \
	—name <containerName> \
	-v <volumeName>:<containerPath> \
	nginx:latest

Docker Compose
- compose is a docker tool for running multi-container application
- it uses a YAML format compose file to configure the application’s services
- using a command, the user can then create and start the services from the configurations
- compose works in three steps:
	+ creating a ‘Dockerfile’ for the application
	+ define the services that make the application in ‘docker-compose.yml’
	+ run ‘docker-compose up’ to start the application
- compose common commands
	+ docker-compose build : builds or rebuilds a service from the given dockerfile.
	+ docker-compose run : allows user to run a one-off command in the service
	+ docker-compose up : creates and runs service containers
	+ docker-compose down : removes the containers, networks, images, and volumes related to the service

Docker Swarm
- it’s a container orchestration tool that controls and operates on cluster.
- Swarm is a group of machines running docker and configured to work together in a cluster.
- docker swarm comprises of three different types of nodes.
	i. manager node : is responsible for assigning tasks to the worker nodes.
	ii. leader node : does the management tasks and takes the orchestration related decisions
	iii. worker node : receive and execute the tasks allocated by the manager node.
- features :
	i. integrated orchestrator : new cluster can be spun up with few commands
	ii. decentralized : the entire swarm of nodes can be built from a single image
	iii. declarative service model : the desired state of the services is defined using a declarative approach
	iv. scaling : number of tasks can be defined for each service
	v. state reconciliation : the state of the cluster is constantly monitored by the swarm manager
	vi. network : swarm manager auto-assigns addresses to the containers during updates
	vii. service discovery : each service in the swarm is assigned a unique DNS
	viii. load balancing : an external load balancer can be used to expose the services
	ix. security : swarm enforces nodes to authenticate communication through TLS certificates
	x. rolling updates : to ensure zero downtime swarm gives users control of deployment delays



Edureka practise

ansible playbook vs docker image

- docker is containerization tool use to package application for deployment.

development and testing




Jenkin module : we performed code build and created artifacts!!!
	+ master to schedule jobs
	+ slave1 == java  == jdk, maven
	  slave2 == node.js
	  slave3 == angular
	  . . .
	  slave100

deployment

development perspective and infra perspective.

- infra perspective. for multiple slave above 
	+ 1 master ::

	+ 1 VM :: create docker engine ( run any number of containers)
		   		docker container1(java) container2(nodeJS) . . . container100
	
	+ all the artifact (created in the containers) will be moved to one artifact library e.g JFrog artifactory, for deployment purpose.


- development perspective 

java appln. ===> build ===> created artifacts *.war == testing it in his own dev environment - - jdk1.8, maven, tomcat 8

the same artifact *.war ===> deployed to QA, UAT, PROD

package the artifacts (*.war) 

img/package the applications == artifacts (*.war) along with the dependencies ( jdk1.8, maven, tomcat 8)

the image will be tested and will be moved to other env. (QA, UAT, PROD)




#Docker Installation | Configuration | Commands & Concepts:

[ec2-user@ip-172-31-45-222 ~]$

[root@ip-172-31-45-222 ~]# yum install docker

[root@ip-172-31-45-222 ~]# docker version

// cannot connect to docker daemon without starting the docker daemon

[root@ip-172-31-45-222 ~]# systemctl status docker

[root@ip-172-31-45-222 ~]# systemctl start docker

[root@ip-172-31-45-222 ~]# systemctl status docker

[root@ip-172-31-45-222 ~]# systemctl enable docker // to auto start after machine is restarted


#Basic Docker Commands :

#goto : https://hub.docker.com/ create your acct here

#Test Docker with basic hello-world image from docker hub.

/*
docker run centos
docker pull centos 



docker run -it nginx
docker exec -it <container_id>

docker pull hello-world

docker images

docker run centos
*/



[root@ip-172-31-45-222 ~]# docker ps // to list running containers
[root@ip-172-31-45-222 ~]# docker ps -a 

# we need image to run container

# always use image with official image in docker.hub

Images
- images are read only templates used to create containers
- images are created with the docker build command, either by us or by other docker users
- images are composed of layers of other images
- images are stored in a docker registry

Containers
- if an image is a class, then a container is an instance of a class - a runtime object
- containers are lightweight and portable encapsulations of an environment in which to run applications
- containers are created from images. inside a container, it has all the binaries and dependencies needed to run the application



[root@ip-172-31-45-222 ~]# docker images   // to see available images

[root@ip-172-31-45-222 ~]# docker pull centos // pull latest image from docker hub and store in the local machine

[root@ip-172-31-45-222 ~]# docker pull loksaieta/loksaimvndevops:v1.0   // you can also use the tag instead of the image name



[root@ip-172-31-45-222 ~]# docker run centos
[root@ip-172-31-45-222 ~]# docker ps -a // to see all the running and non-running containers

# vm run operating system while container run task

[root@ip-172-31-45-222 ~]# docker run ubuntu // will both pull and run the images


### Run Container in Interactive Mode:
###docker run -it --name=testvol1 centos bash

[root@ip-172-31-45-222 ~]# docker run -it centos bash   // switch to centos. to login into the container and access its data
[root@0116025e3678 /]#  ls   // explore all the available files and directory
[root@0116025e3678 /]# cat /etc/*release* // to know the release of particular image
[root@0116025e3678 /]# exit // to exit

[root@ip-172-31-45-222 ~]#
 


### Print the Docker Centos Image Release Info.
dockerpath# cat /etc/*release*
dockerpath# exit

###Run Docker Container in DETACHED MODE :
docker run -d centos sleep 20    
docker ps -a

###Run Docker Container in FOREGROUND MODE :
docker run centos sleep 20   // run this container for 20 seconds
docker ps -a

docker run -d centos sleep 200
docker stop <container_id>

[root@ip-172-31-45-222 ~]# docker rm 2c5b 1b4f    // to remove container from the running list. this remove containers with space

[root@ip-172-31-45-222 ~]# docker ps -a // to see the running container

[root@ip-172-31-45-222 ~]# docker images
[root@ip-172-31-45-222 ~]# docker rmi <image_id>

### Before deleting image, ensure that it is not referenced to any container. 
### In it is mapped, delete the container first, then can re container image.

docker rmi -f hello-world

docker pull centos:centos7.9.2009 // pulling image using tag/version

docker run centos sleep 1100

docker stop <container_id>

### Detached Mode
docker run -d centos sleep 2000
// running without displaying anything

docker attach <running_container_id> // display again running container
docker stop <container_id>

###docker pull timer

#docker pull jenkins

#docker run jenkins

docker inspect <container_id>

docker history <imagename>


### Port-Mapping / Port-Binding

goto hub.docker & search for tomcat

docker run -it -p 8585:8080 jenkins
docker run -it -p 8586:8080 tomcat:8.0

docker run -it -p 8080:8080 tomcat:8.0

#### -p host_port:container_port

Once you install tomcat on ur vm :

<public_ip>:8585
<public_ip>:8586


###Docker Logs

docker logs <container_id>

#Access Docker contents using exec - Non-Interactive Mode: 

docker exec e8553a4721d5 cat /etc/hosts

#Login to Docker Container using exec in Interactive Mode: (Using Container Name) 

docker exec -it <container_name-gallant_blackburn> bash

#Run Docker Image in Interactive Mode: (Using Image Name) 

docker run -it centos bash
dockerpath# cat /etc/*release*
dockerpath# exit

### Get the running Container Names
docker ps -q

###Stop all Running Containers:

docker stop $(docker ps -q)

docker kill $(docker ps -aq)




###Build Docker Image :::

Ways to build a docker Image

i. commit changes made in a docker container

ii. write dockerfile.


###Using commit: 


Create an image for

	java appln. deployment ===> (*.war, jdk, tomcat)

	container template ::::: to update changes 

		centos

		login to that container ( to add update

		install any tools ( jdk, tomcat)

		exit 

		docker commit <container_id>


[root@ip-172-31-45-222 ~]# docker run -it debian // to create new image and login into its container for necessary update

[root@0116025e3678 /]# ls   // to check file system

[root@0116025e3678 /]# git // to check if git is installed

[root@0116025e3678 /]# apt-get update && apt-get install -y git 

[root@0116025e3678 /]# exit

[root@ip-172-31-45-222 ~]# git 

[root@ip-172-31-45-222 ~]# docker ps -a // to see the updated container

[root@ip-172-31-45-222 ~]# docker commit <container_id> loksaieta/deb-git:v1.0 
// docker-hub-repository/image-name:tag
// using the container-id to create new image
// by default if no tag is assigned it will use latest instead.
	
[root@ip-172-31-45-222 ~]# docker commit 03ab32a36f71 loksaieta/mar13-git:v1.0
	
[root@ip-172-31-45-222 ~]# docker images // to see the created image

[root@ip-172-31-45-222 ~]# docker run -it loksaieta/mar13-git:v1.0

[root@0116025e3678 /]# ls

[root@0116025e3678 /]# git -version  // work on git
	
	###Login to Container:
[root@0116025e3678 /]# docker exec -it <container_name-wizardly_shannon> bash



	
###Using Docker file:

# flat file to create image

[root@ip-172-31-45-222 ~]# mkdir docker-contents

[root@ip-172-31-45-222 ~]# cd docker-contents

[root@ip-172-31-45-222 docker-contents]# vi Dockerfile // Dockerfile is the default file name.


FROM debian            // start from base image
RUN apt-get update
RUN apt-get install -y git
RUN apt-get install -y vim


###save this Dockerfile

[root@ip-172-31-45-222 docker-contents]# docker build -t loksaieta/deb-git-vim .  // to build your image using dockerfile
// “.” dockerfile from present working directory
// deb-git-vim is the created image name

[root@ip-172-31-45-222 docker-contents]# docker images

[root@ip-172-31-45-222 docker-contents]# docker run -it  loksaieta/deb-git-vim bash // to access the image

[root@0116025e3678 /]# git —version


### to see history of image

# list also steps taken / layers in the dockerfile

[root@ip-172-31-45-222 ~]# docker images

[root@ip-172-31-45-222 ~]# docker history loksaieta/deb-git-vim

[root@ip-172-31-45-222 ~]# cat dockerfile


# to simplify number of layers

FROM debian
RUN apt-get update && apt-get install -y \
	git \
	vim

// instead of 3 RUN below

FROM debian            // start from base image
RUN apt-get update
RUN apt-get install -y git
RUN apt-get install -y vim


#Dockerfile to deploy application

[root@ip-172-31-45-222 docker-contents]# vi Dockerfile

FROM tomcat:8.0
WORKDIR /home/devopsadmin      // artifact from this directory
COPY ./mvn-hello-world.war /usr/local/tomcat/webapps // artifact deploy to the target
RUN cp -r /usr/local/tomcat/webapps.dist/* /usr/local/tomcat/webapps



docker build -t loksaieta/javawebapp1 .
docker build -t loksaieta/javamvnwebapp1 .


### Port-Mapping & Port-Binding

docker run -it -p 8585:8080 loksaieta/mvnjavawebapp1

docker run -it -p 8080:8080 loksaieta/javamvnwebapp1

goto hub.docker & search for tomcat

docker run -it -p 8088:8080 tomcat:8.0

- tomcat runs on port 8080 and it’s accessed outside the container on port 8088

### -p host_port:container_port

once you install tomcat on ur vm :

<public_ip>:8080

## for details on dockers image path

docker log <container-id>

### Access docker contents using exec - non-interactive mode:

docker exec e8553a4721d5 cat /etc/hosts



###
sudo usermod -aG docker devopsadmin




#QA Environment ---> OS, appln, appln server, mysql, Testing tool, Monitoring tools 

#docker-compose.yaml 

#While running multiple containers ----> we shd establish links between the containers... for data trans



 
#### Docker Push images to Docker Registry:

docker images

docker tag <image_id> loksaieta/debian-git1:1.0

docker tag 33e83a2fdb90 loksaieta/debian-git2:v1.0

docker images 


## Push to docker registry

docker login --username=loksaieta

password: dckr_pat_lu71vhQjN0UvzxvxzcvxzcvxzcvxcvYt7wadfsdfsdcsdcsdfpwfpsdf8
// create a token from your acct in hub.docker.com
// account settings => security => new access token
	access token permission: valid name
	access permissions: read, write, delete
// after login, then you can use docker cmd to push image to docker hub
// alternatively to docker hub, you can have environment like azure container, aws container

// from docker hub, you can push image to target environment using kubernetes

// developers responsibility is to create source code and docker file

#docker push loksaieta/deb-git-vim:latest

docker push loksaieta/mvnjavawebapp1

#docker push loksaieta/debian-git:v1.0

##Docker File



###Storage
### Docker Volumes:

- containers were used just to run your appln.

- container was meant for stateless appln. bcos container was not initially design to handle data

- appln. requires volumes to process data. this is called stateful appln.

- stateful application : mount the vol. to handle the data

image ===> application.war == sql-base = credentials / config parameters
file system

logs / reports   = volume mounts!


[root@ip-172-31-45-222 ~]# docker ps

[root@ip-172-31-45-222 ~]# docker run centos sleep 30

- once running container exited, no more access to container data

[root@ip-172-31-45-222 ~]# docker run -it centos bash

[root@262ee14qcbc3 /]# ls

- data file can be created here. but once the container is lost, the data will be lost as well.

- but if you create volume, it will be on the local machine.

 docker volume create v1-name

- this volume will be created in the vm not container. 

[root@ip-172-31-45-222 ~]# docker volume list

- to see list of volumes

[root@ip-172-31-45-222 ~]# docker volume inspect v1-name
// /var/lib/docker/volumes/v1-name/_data

[root@ip-172-31-45-222 ~]# cd /var/lib/docker/volumes/v1-name/_data

[root@ip-172-31-45-222 _data]# pwd

- you need to know the location of volume on the local machine before the volume can be used in the container

[root@ip-172-31-45-222 _data]# docker run -it --mount source=v1-name,destination=/v1-name centos bash

- source : volume on the local machine, destination : volume to be created in the container

[root@262ee14qcbc3 /]# ls

- directory named v1-name, will be created in the root directory.

[root@262ee14qcbc3 /]# cd v1-name

[root@262ee14qcbc3 v1-name]# echo “rec1” >> q1.txt
[root@262ee14qcbc3 v1-name]# echo “rect” >> q2.txt

- you can create data in the volume. as above

[root@262ee14qcbc3 v1-name]# exit

- after exiting container, data are not lost

[root@ip-172-31-45-222 ~]# cd /var/lib/docker/volumes/v1-name/_data
[root@ip-172-31-45-222 _data]# ls

- you will see the created files here. 

- you can also send data to the container from the local machine.

[root@ip-172-31-45-222 _data]# echo “uname” >> secret
[root@ip-172-31-45-222 _data]# echo “password” >> secret

[root@ip-172-31-45-222 _data]# ls  // to see the created file

[root@ip-172-31-45-222 _data]# docker run -it --mount source=v1-name,destination=/v1-name centos bash

- this will send data to the container from the local machine

- to unmount, just delete the volume (v1-name/_data) from the local machine



stateless & Stateful Appln.

100GB 

java n devops aws devops ==>

...........................

Wokflow ::::

create the src 
perform code build 
create artifacts
create Dockerfile
Perform Docker Build
Push the Docker image into docker registry 






++++++++++++++ KUBERNETES

Case Study: Booz Allen Hamilton
- Booz Allen Hamilton is a American management and information technology Consulting firm
- it’s home to about 27,600 employees
- the core business is driven to provide consulting, analysis and engineering services to public and private sector organizations.

The challenge
- In 2017, BoozAllen was contracted by the federal government to relaunch their old recreation.gov website
- the infrastructure for the new website needed to be agile, reliable and scalable.
- they also needed the option to replicate the process in case any other government aided agency required a revamp

The solution: Kubernetes
- the first step to the solution was to create everything in the Microservice architecture using containers
- kubernetes provided the perfect platform for their dynamic and agile containerized platform
- with the help of kubernetes, new changes could be pushed out within 30 minutes compared to hours/days previously
- kubernetes has enabled a robust system enabling the 10+ deployments everyday on production

Kubernetes core concepts

What is Kubernetes? 
- Kubernetes is an open-source, portable platform for automating  deployment, scaling and management of containerized workloads and applications.
	+ it is a container management technology developed in Google lab to manage containerized applications in different kind of environments such as physical, virtual, and cloud infrastructure.
	+ it groups containers that make up an application into logical units for easy management and discovery.
	+ it helps in creating and managing containerization of application.

Kubernetes: Pods
- basic building blocks of kubernetes
- to reduce coupling as much as possible, single container pods are used
- multi-container pods can be deployed for cohesive processes
- multi-container pods are more efficient since they share the same user-space

Kubernetes: Controllers 
- are control loops that monitor the state of the cluster through the API server
- maintains and brings the cluster to the desired state
- all controllers are separate but are compiled into a single binary running as a single process
- e.g Deployment, ReplicaSet, NodeController

Kubernetes: Services 
- add a layer of abstraction over the pods
- the abstraction defines a policy through which the pods are accessed
- services load balance the requests across a set of pods

Kubernetes: Volumes 
- containers are ephemeral, and once the container crashes, all its data is lost
- volumes provide a definite storage structure which can be attached to a pod
- lifecycle of volume can be bound to the pod
- vol help restore stopped/ unavailable containers to their previous state

Kubernetes: ConfigMaps and Secrets  
- allow users to configure applications externally
- allow passing sensitive information without hardcoding it inside the container spec
- configMaps are used for non-confidential data
- secrets are used for confidential data

Kubernetes: Deployments 
- it transitions current environment to the desired state in a controlled manner
- deploys a replication controller or a replica set which further deploy the pods
- used to rollout updates to already deployed applications.
- rollbacks are possible in case of faulty rollout

Kubernetes: StatefulSet 
- statefulSet is ReplicaSet equivalent for stateful applications
- maintains uniqueness of each pod across different deployments and scaling operations
- requires a headless service to provide network identity to the pods


Introduction to Kubectl
- it is the command line tool which manages the Kubernetes cluster
- it takes configurations from $HOME/.kube directory

- Create : creates Kubernetes resources ( such as deployments, services, clusterRoles & jobs ) using a file or standard input.
	+ the file format could be YAML or JSON
	+ resources such as deployments, services, clusterRoles, and jobs can be created using the create cmd
	 $kubectl create -f fileName

- Get : displays the resources and their current state
	+ results are displayed in a tabular format and can be filtered according to the user’s requirement
	 $kubectl get resourceType

- Run : directly deploys a particular image in the format user wants
	+ generally, a deployment is created with a replication controller unless specified otherwise
	 $kubectl run name --image=imageName

- Expose : used to expose Kubernetes resources as a service
	+ the selector for the service is the same as the selector for the resource
	+ to expose deployments or replicaSets, the equality-based selector must be used
	 $kubectl expose resourceType resourceName

- Delete : deletes selected resources on the cluster
	+ resources can be deleted using filename, stdin, resourceName, and labels & selectors
	+ graceful deletion of pods is possible. this allows the current process to end before deletion
	 $kubectl delete -f fileName

- Apply : changes the configuration of an already deployed resource
	+ if the resource does not exist, a new resource is created
	 $kubectl apply -f fileName

- Edit : edits the specification YAML of a deployed resource
	+ allows direct modification to any API resource which can be retrieved using the command line tools
	 $kubectl edit [-f fileName]/[ResourceName]

- other commands : 
	+ label : creates/updates the labels on a resource
	+ scale : scales the current size of the pod-controlling resources to the desired size
	+ describe : gives a detailed description about the resource
	+ exec : used to enter and execute commands inside of the container shell
	+ logs : displays the logs for a pod or a resource
	+ cluster-info : displays the current state of the cluster
	+ cp : copies files/directories to and from containers
	+ explain : describes the API resources
	+ set : used to make changes to deployed application resources



What is a Pod?
- in kubernetes we refers to pod instead of the container
- it’s atomic unit of scheduling

	virtualization == VM, here we can run OS and application

	docker == container, 

	kubernetes == pod, contains entire container (one or more)

- pod is the most basic unit of deployment in Kubernetes hierarchy
- it hosts container(s) inside it and is responsible for ensuring they're healthy and functioning as expected.

advantages of pods
- pods can scale, configure and apply patches to containers effortlessly
- easier to orchestrate containers deployed at a large scale
- pods are self-healing and do not require manual intervention
- resource sharing amongst containers is secure and easy

ways to consume pods :
	 single container pods //  Multi-container pods :
i. consists of one individual container // contain multiple containers inside a single pod
ii. preferred way of deploying pods in the kubernetes environment // used when the application requires two or more processes to work as a cohesive unit
iii. removes the risk of coupling related issues // usually, one of the containers provides service to the other container
iv. pod acts as a wrapper to the container application // containers in the multi-container pod share IP-addresses, volumes, and so on.

- there could be communication between the pods like transfer of data
- controller manager controls your pods, while scheduler schedule your pods to the nodes

Pod networking

- when a new pod is created, it will be assigned new IP address.
- within a pod we can have container(s) which include main container and/or supporting container.
- each container has different port number but share same IP address of the pod 
	pod 1 		= 	Main container
	10.0.30.50 		10.0.30.50:8080

				= 	Supporting container 
					10.0.30.50:3000

	pod 2 		= 	Main container
	10.0.30.60 		10.0.30.60:7777

- we can perform inter pods communication [ 10.0.30.50 == 10.0.30.60 ] using pod network or intra pod communication [ 10.0.30.50:8080 == 10.0.30.50:3000 ] within a pod using local host.



ReplicaSet and ReplicationController

ReplicaSet
- maintains a definite number of pods on the cluster at any given time
- uses a set-based selector to identify and acquire pods
- PodSpec template and the number of pods are defined in the ReplicaSet YAML
- pods have a metadata.ownerReference field through which ReplicaSet takes ownership of the pod 
- replicaSet assumes ownership of pods without owners

ReplicationController
- ensures a specific number of pods are always running
- pods can be scaled up and down depending upon the current requirement
- uses an equality-based selector to acquire pods
- acts as a process supervisor, supervising pods across nodes on the cluster
- maintains the number of pods in the case of a node-failure

Deployments
- are controllers which provide declarative updates to pods and bring the current state of the system to the the desired state
- deploy a ReplicaSet which controls the pods
- current state of the pods can be changed using deployments
- update are rolled out in a gradual manner
- in-case of a faulty update, rollback to a previous version is possible

DaemonSet
- ensures that an instance of a pod is always running on all or selected node(s) in the cluster
- used to run daemon processes for cluster applications/nodes
- does not require scheduler intervention to schedule pods
- a common use-case is to use it as a logging/ monitoring agent
- when deleted, it terminates all the pods it has scheduled


Rolling Updates and Rollbacks

Rolling Updates
- deployments changed how updates for applications are handled in Kubernetes
- the declarative nature of deployments makes it easy to rollout updates
- this reduces the application down-time significantly
- deployments manage rolling updates by creating a new ReplicaSet with the updated content
- the new replicaSet scales up while the old replicaSet scales down the pods
- the number of pods is maintained and there is no down-time
- the old replicaSet remains in the cluster and is not deleted
- the “rollout --history” command is used to track changes/updates
- the cause of the change can be recorded
- the simple way to rollout an update is to edit the deployment using the edit command.

Rollbacks
- after a rollout, the old ReplicaSet remains in the cluster
- if a rollback is required, the old ReplicaSet is scaled up and the current ReplicaSet is scaled down
- the required version can be retrieved from revision history

Kubernetes: Scaling
- pods can be scaled using direct CLI commands or by editing controller specification
- manual scaling : for lighter workload demands, admins tend to scale their application manually
	+ the admin increases/decreases the number of pods according to the requirement session




Labels, Selectors and Annotations

Labels
- key-value pairs attached to kubernetes objects such as nodes, pods, and controllers
- used to organize objects by identifying similar attributes
- added at the time of object creation
- can be added/modified later in the object’s lifecycle

Selectors
- core grouping mechanism in kubernetes
- users/applications can pick a group of objects by their labels using selectors
- multiple labels can be used with selectors using a comma separator, which acts as a logical AND (&&) operator
- different APIs have context-based implementation for empty selectors

- types of selectors :

	i. equality-based selector : allows filtering by key-value matching
		+ all the specified labels must be satisfied by the matching object
		+ two kinds of operators are used
		+ ==(=) : represents equality
		+ != : represents inequality
		+ e.g env = production
		$kubectl get pods —selector owner=edureka

	ii. set-based selector : allows filtering the keys based on a set of values.
		+ support three kinds of operators : in, notin, exists
		+ e.g team in (HR, training)

Annotations
- attaches non-identifying metadata to objects
- the metadata can be utilized by various tools and libraries
- uses key-value pair with valid segments i.e an optional prefix and name separated by /(slash)
- metadata can have characters not permitted by labels
- it can store information such as :
	+ build/release/image information
	+ logging and monitoring pointers
	+ analytics or audit repositories
	+ library/ tool information
	+ information for external ecosystems
	+ custom declarative configuration layer fields

Services
- it load-balance the requests in and out of the application
- it acts as a layer of abstraction between the containers and the network
- it act as a single constant point of entry to a group of pods lending similar service
- it solves pod related issues such as pods being ephemeral and horizontal scaling
- each service has an immutable IP address and port that will never change throughout the service’s life cycle.
- clients directly communicate with the service. this allows pods to be scheduled anywhere
- it gives resurrecting pods more consistently than a deployment

- types of services :
	i. ClusterIP : exposes the application inside the cluster itself. application cannot be accessed outside of the cluster
	ii. NodePort : exposes the application to a static port on the node. can be accessed outside of the cluster using the NodeIP and NodePort
	iii. LoadBalancer : uses external cloud provided load balancers. it automatically creates NodePort and ClusterIP.
	iv. ExternalName : a unique service where selectors, ports and endpoints are not defined. returns an alias to an external service residing outside of the cluster

	+ services are superset of each other
	+ NodePort is superset of ClusterIP and Loadbalancer is superset of NodePort

Persistent Volumes

Volumes: the problem
- normal volumes provide persistent storage but the layer of abstraction is lost
- difficult to manage tightly coupled applications which share volume storage
- volumes are bound to pods lifecycle and are destroyed once the pod is terminated
- unable to provide persistent storage to stateful / distributed application across the cluster

Persistent storage in Kubernetes
- the layer of abstraction can be bought back using persistent volumes and persistent volume claim
- PV can be utilized as a cluster resource
- Kubernetes binds the PV claim to the best possible PV
- the volume claim connects the applications with the persistent storage
- PV provide the stability required for stateful and distributed applications
- kubernetes has the provision of storing persistent data in volumes. this maintain consistency for stateful applications

Persistent volume and persistent volume claims
- PV once created, acts as a resource in the cluster
- PVC are requests for the created resources (PVs)
- PVCs ensure that the application will be able to use the required storage
- PVs and PVCs abstract the storage implementation from the application
- the application needs to specify certain parameters for its storage requirement to the PVC

- Access Modes for persistent volumes
	i. RWO / Read Write Once : the volume can only be mounted by single node in read-write mode
	ii. ROX / Read Only Many : the volume can be mounted   in read-only mode by many nodes
	iii. RWX / Read Write Many : the volume can be mounted in the read-write mode by many nodes

Headless service
- a service with its ClusterIP set to none is known as a headless service
- unlike other services, headless.s does not provide any load-balancing or proxy capabilities.
- helps decouple service from kubernetes giving an alternate form of service discovery
- DNS entries are still recorded on the cluster to discover pods through the service
- helps the client to connect to all the pods
- statefulSet requires a headless.s for its pods to be discovered by the network
- running a DNS query for a headless service returns the list of pods attached to it

StatefulSets
- create pods with sticky identities
- keeps track of the number of pods deployed and ensures that the said number is maintained
- provides a stable identity and state for non-fungible applications
- keeps track of the number of pods deployed and ensures that the said number is maintained
- pod template is specified as a part of statefulSet’s specification
- the pods scheduled are not exact replicas of each other. each pod has its own storage unit to work with
- rolling-updates can be applied to pods associated with statefulSets


ConfigMaps and Secrets

ConfigMaps
- exports configuration data into a container to avoid hardcoding it inside the application
- employs the configMap api resource which contains config. data in the form of key-value pairs
- used to pass non-sensitive information to the containers
- the contents of the configMap are not directly passed to the application
- content is passed either to the container as environment variables or to the volumes as files

Secrets
- allow passing sensitive information to the containers
- holds information in key-value pairs and can be passed as environment variables or files in a volume
- only distributed to the nodes that are running the pod which need access to the secrets content
- stored on memory(RAM) of the node instead of the physical memory (HDD or SSD)
- on the master node, secrets are stored in encrypted form on the etc storage.

Kubernetes Package Manager : Helm
- packages collection of kubernetes logic which can be deployed later
- the packages can be distributed and reused as HELM charts
- consists of yaml configurations and templates
- configurations are passed as manifest files to kubernetes
- directory structure of a Helm chart:
	package-name/
	- charts/
	- templates/
	- chart.yaml
	- LICENCE
	- README.md
	- requirements.yaml
	- Values.yaml









Kubernetes - Cluster Architecture

- Developer : The architecture start with Developer 
	+ who will write the source code
	+ artifact will be created from the source code
	+ based on the artifact, we create docker file to create docker image

- Docker hub : docker image will be pushed into docker hub
	+ from here we pick the image and create container from it, that will be exhibiting all the environment like nginx, mysql

- App configuration : to deploy this application to production environment
	+ kubernetes need configuration file called manifest file (yml)
	+ this contains definitions
	+ this will be defined by devops or kubernetes admin
	+ developers will fill up the templates to include images, strategies like high availability, auto scaling, load balancing etc
	+ how this file will be submitted to kubernetes, we use kubectl command which helps to interact with kubernetes

- Kubernetes follows client-server architecture. Wherein, we have master installed on one machine and the (worker) node on separate 

Linux machines.

- Master Machine Components : this master receives incoming request, process request and schedule the node to the container. 
	+ API server : implements an interface, which means different tools and libraries can readily communicate with it. it acts as interface between user and kubernetes. it validate the manifest file like the syntax and if sent by authorized user, before taking it further. if the request is valid, Api server update the configuration in etc database.
	+ etcd : It stores the configuration information which can be used by each of the nodes in the cluster. it contains information about the incoming requests. so, it act as the single point of source for information about kubernetes cluster like how many nodes / containers are running ? 
	+ Scheduler : Then the api server notify the scheduler, which will access the etc database through the api.s to verify the containers definitions and the defined strategies. It is a service in master responsible for distributing the workload.
	+ Controller Manager : This component is responsible for most of the collectors that regulates the state of cluster and performs a task. It controls the execution of the containers. If container got crashed, it will create a new container based on the same image. It helps ensure high availability of the containers, auto-scaling and load balancing (distributing the work loads among the containers).

- Node Components : nodes are virtual machines created and attached to masters. actual container will be executed on the node only. 
	+ Following are the key components of Node server which are necessary to communicate with Kubernetes master.
	+ Kubelet Service : is responsible for relaying information to and from control plane service. It interacts with etcd store to read configuration details and wright values. This communicates with the master component to receive commands and work. so api.s will send the image to the kubelet which it will verify before forwarding it to Docker component, which act as runtime engine.
	+ CRI: Docker (Container Runtime Interface) : The first requirement of each node is Docker which helps in running the encapsulated application containers in a relatively isolated but lightweight operating environment. It verify the image and get it deployed from docker hub to the server.
	+ Kubernetes Proxy Service : runs on each node and helps in making services available to the external host. It helps in forwarding the request to correct containers and is capable of performing primitive load balancing. it helps to establish communication between the node and other nodes or master.

Kubernetes - Setup
- It is important to set up the Virtual Datacenter (vDC) before setting up Kubernetes. This can be considered as a set of machines where they can communicate with each other via the network.
- Once the IaaS setup on any cloud is complete, you need to configure the Master and the Node.
- Note − The setup is shown for Ubuntu machines. 
- Prerequisites
	+ Installing Docker − Docker is required on all the instances of Kubernetes. Following are the steps to install the Docker.

- Install Docker Engine
	Run the following commands to install the Docker engine.
	Step 1 − Logon to the machine.
	Step 2 − Update the package index.
	$ sudo apt-get update
	Step 3 − Install the Docker Engine using the following command.
	$ sudo apt-get install docker-engine
	Step 4 − Start the Docker daemon.
	$ sudo apt-get install docker-engine
	Step 5 − To very if the Docker is installed, use the following command.
	$ sudo docker run hello-world
- Install etcd 2.0
	+ This needs to be installed on Kubernetes Master Machine. In order to install it, run the following commands.
	$ curl -L https://github.com/coreos/etcd/releases/download/v2.0.0/etcd
-v2.0.0-linux-amd64.tar.gz -o etcd-v2.0.0-linux-amd64.tar.gz ->1
	$ tar xzvf etcd-v2.0.0-linux-amd64.tar.gz ------>2
	$ cd etcd-v2.0.0-linux-amd64 ------------>3
	$ mkdir /opt/bin ------------->4
	$ cp etcd* /opt/bin ----------->5
	
	+ In the above set of command −
	•	First, we download the etcd. Save this with specified name.
	•	Then, we have to un-tar the tar package.
	•	We make a dir. inside the /opt named bin.
	•	Copy the extracted file to the target location.
- Now we are ready to build Kubernetes. We need to install Kubernetes on all the machines on the cluster.
	$ git clone https://github.com/GoogleCloudPlatform/kubernetes.git
	$ cd kubernetes
	$ make release
	+ The above command will create a _output dir in the root of the kubernetes folder. 
	+ Next, we can extract the directory into any of the directory of our choice /opt/bin, etc.
	+ Next, comes the networking part wherein we need to actually start with the setup of Kubernetes master and node. 
	+ In order to do this, we will make an entry in the host file which can be done on the node machine.
	$ echo "<IP address of master machine> kube-master < IP address of Node Machine>" >> /etc/hosts
	
- Kubernetes Master Configuration.
	+ First, we will start copying all the configuration files to their correct location.
	$ cp <Current dir. location>/kube-apiserver /opt/bin/
	$ cp <Current dir. location>/kube-controller-manager /opt/bin/
	$ cp <Current dir. location>/kube-kube-scheduler /opt/bin/
	$ cp <Current dir. location>/kubecfg /opt/bin/
	$ cp <Current dir. location>/kubectl /opt/bin/
	$ cp <Current dir. location>/kubernetes /opt/bin/
	+ The above command will copy all the configuration files to the required location. 
	+ Now we will come back to the same directory where we have built the Kubernetes folder.
	$ cp kubernetes/cluster/ubuntu/init_conf/kube-apiserver.conf /etc/init/
	$ cp kubernetes/cluster/ubuntu/init_conf/kube-controller-manager.conf /etc/init/
	$ cp kubernetes/cluster/ubuntu/init_conf/kube-kube-scheduler.conf /etc/init/
	$ cp kubernetes/cluster/ubuntu/initd_scripts/kube-apiserver /etc/init.d/
	$ cp kubernetes/cluster/ubuntu/initd_scripts/kube-controller-manager /etc/init.d/
	$ cp kubernetes/cluster/ubuntu/initd_scripts/kube-kube-scheduler /etc/init.d/
	$ cp kubernetes/cluster/ubuntu/default_scripts/kubelet /etc/default/
	$ cp kubernetes/cluster/ubuntu/default_scripts/kube-proxy /etc/default/
	$ cp kubernetes/cluster/ubuntu/default_scripts/kubelet /etc/default/
	+ The next step is to update the copied configuration file under /etc. dir.
	
	+ Configure etcd on master using the following command.
	$ ETCD_OPTS = "-listen-client-urls = http://kube-master:4001"
- Configure kube-apiserver
	+ For this on the master, we need to edit the /etc/default/kube-apiserver file which we copied earlier.
	$ KUBE_APISERVER_OPTS = "--address = 0.0.0.0 \
	--port = 8080 \
	--etcd_servers = <The path that is configured in ETCD_OPTS> \
	--portal_net = 11.1.1.0/24 \
	--allow_privileged = false \
	--kubelet_port = < Port you want to configure> \
	--v = 0"
- Configure the kube Controller Manager
	+ We need to add the following content in /etc/default/kube-controller-manager.
	$ KUBE_CONTROLLER_MANAGER_OPTS = "--address = 0.0.0.0 \
	--master = 127.0.0.1:8080 \
	--machines = kube-minion \ -----> #this is the kubernatics node
	--v = 0
	+ Next, configure the kube scheduler in the corresponding file.
	$ KUBE_SCHEDULER_OPTS = "--address = 0.0.0.0 \
	--master = 127.0.0.1:8080 \
	--v = 0"
	+ Once all the above tasks are complete, we are good to go ahead by bring up the Kubernetes Master. In order to do this, we will restart the Docker.
	$ service docker restart
- Kubernetes Node Configuration
	+ Kubernetes node will run two services the kubelet and the kube-proxy. 
	+ Before moving ahead, we need to copy the binaries we downloaded to their required folders where we want to configure the kubernetes node.
	+ Use the same method of copying the files that we did for kubernetes master. As it will only run the kubelet and the kube-proxy, we will configure them.
	$ cp <Path of the extracted file>/kubelet /opt/bin/
	$ cp <Path of the extracted file>/kube-proxy /opt/bin/
	$ cp <Path of the extracted file>/kubecfg /opt/bin/
	$ cp <Path of the extracted file>/kubectl /opt/bin/
	$ cp <Path of the extracted file>/kubernetes /opt/bin/
	+ Now, we will copy the content to the appropriate dir.
	$ cp kubernetes/cluster/ubuntu/init_conf/kubelet.conf /etc/init/
	$ cp kubernetes/cluster/ubuntu/init_conf/kube-proxy.conf /etc/init/
	$ cp kubernetes/cluster/ubuntu/initd_scripts/kubelet /etc/init.d/
	$ cp kubernetes/cluster/ubuntu/initd_scripts/kube-proxy /etc/init.d/
	$ cp kubernetes/cluster/ubuntu/default_scripts/kubelet /etc/default/
	$ cp kubernetes/cluster/ubuntu/default_scripts/kube-proxy /etc/default/
	
- We will configure the kubelet and kube-proxy conf files.
	+ We will configure the /etc/init/kubelet.conf.
	$ KUBELET_OPTS = "--address = 0.0.0.0 \
	--port = 10250 \
	--hostname_override = kube-minion \
	--etcd_servers = http://kube-master:4001 \
	--enable_server = true
	--v = 0"
	/
	+ For kube-proxy, we will configure using the following command.
	$ KUBE_PROXY_OPTS = "--etcd_servers = http://kube-master:4001 \
	--v = 0"
	/etc/init/kube-proxy.conf
- Finally, we will restart the Docker service.
	$ service docker restart
- Now we are done with the configuration. You can check by running the following commands.
	$ /opt/bin/kubectl get minions












Edureka practical

- Kubernetes is container orchestration i.e orchestrating containers.

- like you have 1000 micro services, each having personal source code repository and have docker file for each service. once you run all these MS it will be assume as one application.

- what if one container got lost or crash or not visible?? In order to ensure high availability of the container, we need container orchestration. It will continue monitoring container’s status.

- if container is unavailable, it create new container based on the same image i.e auto scaling. if there are more users on the app at same time, to avoid causing lagging of a page we go for scaling. 

- container replicas will be created. like 10 replicas

- you can scale up or scale down based on demand. you can’t do it manually but by container orchestration.

tools - 

docker swarm for only dockers         kubernetes for any
- docker-compose file			   manifest file

e.g for this Web appln. we need mysql, nginx in the container. we write docker-compose file to define how many replica of the container you want to maintain to ensure high availability.


Kubernetes Installation

- Pre-reqs 
	+ 3GB or more RAM
	+ 2 CPU or more
	+ full network connectivity among all machines in the cluster
	+ disable SWAP on all nodes
	+ disable SELinus on all nodes



#	Kube-Master(Controller)  - VM 
#	Kube-Worker1
#	Kube-Worker2

#https://kubernetes.io/docs/setup/  //kubernetes download details
	
#Add Port: 0 - 65535  // open port (edit inbound rule). at least 9 ports will be required. if you’re not sure which to be used open the range of ports.


###On Both Master and Worker Nodes:
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

[ec2-user@ip-172-31-13-1 ~]# sudo -i

[root@ip-172-31-13-1 ~]# yum update -y

[root@ip-172-31-13-1 ~]# swapoff -a

#The Kubernetes scheduler determines the best available node on which to deploy newly created pods. If memory swapping is allowed to occur on a host system, this can lead to performance and stability issues within Kubernetes.

[root@ip-172-31-13-1 ~]# setenforce 0   // to disable firewall

#Disabling the SElinux makes all containers can easily access host filesystem.

[root@ip-172-31-13-1 ~]# yum install docker -y     // to install docker 

[root@ip-172-31-13-1 ~]# systemctl enable docker 
[root@ip-172-31-13-1 ~]# systemctl start docker      // docker needs to be running before installing kubernetes

// to download kubernetes repository
// copy from the beginning to the end of the command in the prompt
[root@ip-172-31-13-1 ~]# cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7x86_64  
enabled=1
gpgcheck=1
repo_gpgcheck=0
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kube*
EOF

// to update ip6table in this config file that comes with the downloaded package
// to establish communication between the vm
[root@ip-172-31-13-1 ~]# cat <<EOF >  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF

[root@ip-172-31-13-1 ~]# sysctl --system // verify the updated entry in the config file

// to install kubernetes
[root@ip-172-31-13-1 ~]# yum install -y kubeadm-1.21.3 kubelet-1.21.3 kubectl-1.21.3 --disableexcludes=kubernetes 

[root@ip-172-31-13-1 ~]# systemctl enable kubelet 
[root@ip-172-31-13-1 ~]# systemctl start kubelet


#Only on Master Node:
#~~~~~~~~~~~~~~~~~~~~~

i. kubeadm

ii. minikube : light weight version of kubernetes. use for training purpose. this can be installed on vm.


#Update Master_Private IP in --apiserver-advertise-address=172.31.15.103

// initialize kubernetes server
// private IP address (like 172.31.15.103) of the server is required
// cidr range can be same - 192.168.0.0/16
[root@ip-172-31-13-1 ~]# sudo kubeadm init --apiserver-advertise-address=172.31.15.103 --pod-network-cidr=192.168.0.0/16 --ignore-preflight-errors=NumCPU --ignore-preflight-errors=Mem


[root@ip-172-31-13-1 ~]# mkdir -p $HOME/.kube // get into the directory
[root@ip-172-31-13-1 ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config // copy the configuration
[root@ip-172-31-13-1 ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config // users who want to access should be able to run this configuration
// then the user can work on kubernetes cluster

#export KUBECONFIG=/etc/kubernetes/kubelet.conf
 
#We need to install a flannel network plugin to run coredns to start pod network communication.
//flannel network plugin is required for network communication
//the plugin installation requires the following 2 commands

[root@ip-172-31-13-1 ~]# sudo kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml 
[root@ip-172-31-13-1 ~]# sudo kubectl apply -f https://docs.projectcalico.org/v3.8/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml 

#Test the configuration ::

// this displays each of the components running as a pod
[root@ip-172-31-13-1 ~]# kubectl get pods --all-namespaces

// to see the available node
// at this point, you will see only master node
[root@ip-172-31-13-1 ~]# kubectl get nodes


#Execute the below commmand in Worker Nodes, to join all the worker nodes with Master :
// after initializing kubernetes server above, a join command with token like below will be displayed in the log. 
// run this command on your first worker node
// note the IP-address:port-number for the master
[root@ip-172-31-13-2 ~]# kubeadm join 172.31.15.103:6443 --token 48tjrr.ukfr4zqnasd56h05 --discovery-token-ca-cert-hash sha256:3e44d70f9e7a27694a815f3bc13f5ed3828e543aa60ec963f47e36e4f1d7c931

// run same command on your second worker node
[root@ip-172-31-13-3 ~]# kubeadm join 172.31.15.103:6443 --token 697sl0.gwfng5vgrqlmccsi --discovery-token-ca-cert-hash sha256:3e44d70f9e7a27694a815f3bc13f5ed3828e543aa60ec963f47e36e4f1d7c931

// you will see 3 nodes
[root@ip-172-31-13-1 ~]# kubectl get nodes

[root@ip-172-31-13-1 ~]# kubectl get pods // no pod created yet
[root@ip-172-31-13-1 ~]# kubectl get pods --all-namespaces // display pods running on the master

#Generate NEW Token :
kubeadm token create --print-join-command


kubectl describe nodes

- cluster is a collection of worker nodes which are attached to master.

Kubectl

- command line utility for running commands against kubernetes clusters like getting info about the pod, info about the node etc.

 syntax : kubectl command type name flags

 command - create, get, describe, delete, logs, exec, edit, run, apply, scale etc
 type - pod(s)/po, deployment(s)/deploy, replicaset(s)/rs, replicationcontroller(s)/rc, service(s)/svc, daemonset(s), namespace(s)/ns, persistentvolume(s)/pv, persistentvolumeclaim(s)/pvc, job(s), cronjob(s) etc


kubectl create -f example.yaml   // create a resource from a file

kubectl get [type(s)] [name(s)] [flags]   // list one or more resources


POD creation - process flow : once you submit the manifest file, the apiserver  will verify the pod configuration with etcd component and authenticate the user. at this stage, it’s in pending
- once the pod is verified and deployed into the worker node, then in running mode otherwise it will be in failed mode if the image is not present or user not verified or mismatch in the configuration.
- while in the running state it might fail due to some issues, then failed mode
- if it completes the task, then in succeeded mode

	kubectl create -f test-pod.yaml  	// to submit the manifest file. use create cmd if that’s first time of creating the file. but if you want to update the the manifest file, then use apply cmd
	kubectl describe -f test-pod.yaml  // give details of every stage of each container e.g nginx-container and redis-container
	kubectl get pods test-pod          // give status of the pod e.g pending, running, failed or succeeded

	cat test-pod.yaml
apiVersion: v1					// or GROUP_NAME/VERSION
kind: Pod
metadata:
	name: test-pod   			// name of the object to be created
	labels:					// to categorize teams involved for multiple environments
		app: nginx
		tier: dev
spec:
	containers:
	- 	name: nginx-container
		image: nginx         	// image to pickup from docker hub
		env:
		-	name: DEMO_GREETING
			value: “Hello from the environment”
		ports:
		-	containerPort: 8080		// to expose the container
	-	name: redis-container
		image: redis			// image to pickup from docker hub

- manifest file like test-pod.yaml contains 4 important key parameters

	+ apiVersion : depend on version of object you want to create e.g v1

	+ kind : what type of object you want to create e.g Pod, deployment

	+ metadata : name of the object to be created. it can also contain name tag

	+ spec : give container’s definition


POD - Tasks

create
	kubectl create -f [file-name].yaml
	kubectl apply -f [file-name].yaml

display 
	kubectl get pods <pod-name>
	kubectl get pods <pod-name> -o wide
	kubectl get pods <pod-name> -o yaml
	kubectl get pods <pod-name> -o json

describe
	kubectl describe pods <pod-name>

edit : to update running pod
	kubectl edit pods <pod-name>

logs
	kubectl logs pods <pod-name>

delete
	kubectl delete pods <pod-name>



*******************************************************************
# 1. https://labs.play-with-k8s.com/

# nginx-pod.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  labels:
    app: nginx
    tier: dev
spec:
  containers:
  - name: nginx-container
    image: nginx

    ports:
    - containerPort: 80    // default port for nginx, while 8080 for tomcat

[root@ip-172-31-13-1 ~]# mkdir kubernetes
[root@ip-172-31-13-1 ~]# cd kubernetes/
[root@ip-172-31-13-1 kubernetes]# vi nginx-pod.yaml

*******************************************************************

2. Create and display Pods

# Create and display PODs
[root@ip-172-31-13-1 kubernetes]# kubectl create -f nginx-pod.yaml //if file is in this directory or define the path
// pod/nginx-pod created
[root@ip-172-31-13-1 kubernetes]# kubectl get pod // see pod status in running mode
[root@ip-172-31-13-1 kubernetes]# kubectl get pod -o wide // give more info about all the pod like node where the pods are running, IP of the master etc
[root@ip-172-31-13-1 kubernetes]# kubectl get pod nginx-pod -o yaml // giving specific pod name. give more info about the named pod
[root@ip-172-31-13-1 kubernetes]# kubectl describe pod nginx-pod // describe only named pod


*******************************************************************

3. Test & Delete

# To get inside the pod
[root@ip-172-31-13-1 kubernetes]# kubectl exec -it nginx-pod -- /bin/sh   // this allow you to get into the pod and write the following html code
// it will change the prompt

# Create test HTML page
cat <<EOF > /usr/share/nginx/html/test.html
<!DOCTYPE html>
<html>
<head>
<title>Testing..</title>
</head>
<body>
<h1 style="color:rgb(90,70,250);">Hello, Everyone...!</h1>
<h2>Welcome to Kubernetes Demo :-) </h2>
</body>
</html>
EOF
exit

# Want to access the created html file through browser
# Expose PODS using NodePort service
[root@ip-172-31-13-1 kubernetes]# kubectl expose pod nginx-pod --type=NodePort --port=80   // this expose pod to internet creating nodePort service. this service help to expose the pod to internet

// is the port required to access application through the internet
# Display Service and find NodePort
[root@ip-172-31-13-1 kubernetes]# kubectl describe svc nginx-pod

# Open Web-browser and access webapge using 
http://<external-nodeip>:<nodeport>            // will nginx home page
http://<external-nodeip>:<nodeport>/test.html  // to access app

#eg.: http://13.233.174.93:31424/test.html  // 31424 is nodeport value, 13.233.174.93:31424 is public IP of that particular node

// if you are using it in real time environment you will be using your domain name from DNS

// how to get above details
[root@ip-172-31-13-1 kubernetes]# kubectl get pods -o wide

# Delete pod & svc
kubectl delete svc nginx-pod
kubectl delete pod nginx-pod


*******************************************************************
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




Scheduling : if the current node is down, appln. should be able to run on available node

Configmap : when you want to run appln. that appln. might need data. decouples configuration from pods and components. stores configuration data as key-value pairs. you can have more than one config. file like dev, QA and prod. configuration to one image. this config. can be secretes or configmap

	+ stateful appln. : to run appln. in the container there is need for data. if you’re using docker, you can use volume but in kubernetes you use services.
then you will not map your configuration along with your images.

	+ stateless appln. : 


Overview:
---------
1. Creating Configmap from "multiple files" & Consuming it inside Pod from "volumes" 

   1a. Create Configmap "nginx-configmap-vol" from "multiple files"
   1b. Consume "nginx-configmap-vol" configmap inside Pod from "volumes" 
   1c. Create | Display | Validate

2. Cleanup

   2a. Delete configmaps
   2b. Delete pods
   2c. Validate

*************************************************************************************************************************************************

1. Creating Configmap from "multiple files" & Consuming it inside Pod from "volumes" 

// create directory for the config. files
[root@ip-172-31-13-1 kubernetes]# mkdir kconfigs
[root@ip-172-31-13-1 kubernetes]# cd kconfigs
[root@ip-172-31-13-1 kconfigs]# 


1a.  Create Configmap "nginx-configmap-vol" from "multiple files":
------------------------------------------------------------------

// creating the config. files / data in the created directory
[root@ip-172-31-13-1 kconfigs]# echo -n 'Non-sensitive data inside file-1' > file-1.txt
[root@ip-172-31-13-1 kconfigs]#  echo -n 'Non-sensitive data inside file-2' > file-2.txt

// creating the configmap from the config. files
[root@ip-172-31-13-1 kconfigs]#  kubectl create configmap nginx-configmap-vol --from-file=file-1.txt --from-file=file-2.txt

# rm -f file-1 file-2

// display created configmaps
[root@ip-172-31-13-1 kconfigs]# kubectl get configmaps

// to get more info about the configmap
[root@ip-172-31-13-1 kconfigs]# kubectl describe configmaps nginx-configmap-vol

==========================================================

1b.  Consume above "nginx-configmap-vol" configmap inside Pod from "volumes" 
---------------------------------------------------------------------------
// create yaml file 
[root@ip-172-31-13-1 kconfigs]#  vi nginx-pod-configmap-vol.yaml

#nginx-pod-configmap-vol.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-configmap-vol
spec:
  containers:
  - name: nginx-container
    image: nginx
    volumeMounts:
    - name: test-vol
      mountPath: "/etc/non-sensitive-data"
      readOnly: true
  volumes:
    - name: test-vol
      configMap:
        name: nginx-configmap-vol
        items:
        - key: file-1.txt
          path: file-a.txt
        - key: file-2.txt
          path: file-b.txt

// configmap - nginx-configmap-vol to be passed to the container volume - "/etc/non-sensitive-data"

==========================================================

1c. Create | Display | Validate:
--------------------------------

# Create pod 
[root@ip-172-31-13-1 kconfigs]# kubectl create -f nginx-pod-configmap-vol.yaml

# Display created pods
[root@ip-172-31-13-1 kconfigs]# kubectl get po

kubectl get configmaps
kubectl describe pod nginx-pod-configmap-vol

# Validate from "inside" the pod
// validate using the secrets from the next section. refer to 1a
[root@ip-172-31-13-1 kconfigs]# kubectl exec nginx-pod-configmap-vol -it /bin/sh
[root@ip-172-31-13-1 ~]# cd /etc/non-sensitive-data
ls 
cat Non-sensitive data inside file-1.txt
cat password.txt
exit

(OR)

# Validate from "outside" the pod
kubectl exec nginx-pod-configmap-vol ls /etc/non-sensitive-data
kubectl exec nginx-pod-configmap-vol cat /etc/non-sensitive-data/file-a.txt
kubectl exec nginx-pod-configmap-vol cat /etc/non-sensitive-data/file-b.txt


2. Cleanup

# Delete configmaps
kubectl delete configmaps nginx-configmap-vol

# Delete pods
kubectl delete pods nginx-pod-configmap-vol

# Validate
kubectl get pods
kubectl get configmaps



Overview:
---------

Secret : is use to pass login details or tokens

1. Create Secret using "kubectl" & Consuming it from "volumes" inside Pod

   1a. Create secret "nginx-secret-vol" using "Kubectl"
   1b. Consume "nginx-secret-vol" from "volumes" inside Pod
   1c. Create | Display | Validate

2. Cleanup

   2a. Delete secrets
   2b. Delete pods
   2c. Validate

*************************************************************************************************************************************************

# 1. Creating Secret using Kubectl & Consuming it from "volumes" inside Pod


1a. Creating secret using "Kubectl":
------------------------------------
[root@ip-172-31-13-1 kconfigs]# echo -n 'admin' > username.txt
[root@ip-172-31-13-1 kconfigs]# echo -n 'pa$$w00rd' > password.txt

[root@ip-172-31-13-1 kconfigs]#  ls

[root@ip-172-31-13-1 kconfigs]# kubectl create secret generic nginx-secret-vol --from-file=username.txt --from-file=password.txt

# rm -f username.txt password.txt

[root@ip-172-31-13-1 kconfigs]# kubectl get secrets
[root@ip-172-31-13-1 kconfigs]# kubectl describe secrets nginx-secret-vol

==========================================================

1b. Consuming "nginx-secret-vol" from "volumes" inside Pod
--------------------------------------------------------

[root@ip-172-31-13-1 kconfigs]# vi nginx-pod-secret-vol.yaml

#nginx-pod-secret-vol.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod-secret-vol
spec:
  containers:
  - name: nginx-container
    image: nginx
    volumeMounts:
    - name: test-vol
      mountPath: "/etc/confidential"
      readOnly: true
  volumes:
  - name: test-vol
    secret:
      secretName: nginx-secret-vol

// the secret - nginx-secret-vol to be passed to container volume - "/etc/confidential"

==========================================================

1c. Create | Display | Validate:
--------------------------------

# Create
[root@ip-172-31-13-1 kconfigs]# kubectl create -f nginx-pod-secret-vol.yaml

# Display
kubectl get po
kubectl get secrets

[root@ip-172-31-13-1 kconfigs]# kubectl describe pod nginx-pod-secret-vol

# Validate from "inside" the pod
[root@ip-172-31-13-1 kconfigs]# kubectl exec nginx-pod-secret-vol -it /bin/sh
# cd /etc/confidential
# ls 
# cat username.txt     // display the content
# cat password.txt     // display the content
# exit

(OR)

# Validate from "outside" the pod
kubectl exec nginx-pod-secret-vol ls /etc/confidential
kubectl exec nginx-pod-secret-vol cat /etc/confidential/username.txt
kubectl exec nginx-pod-secret-vol cat /etc/confidential/password.txt


2. Cleanup

# Delete secrets
kubectl delete secrets nginx-secret-vol

# Delete pods
kubectl delete pods nginx-pod-secret-vol

# Validate
kubectl get pods
kubectl get secrets






Controllers: is use to control the execution of your pod. there are various controllers that includes - 

	+ Replication Controller (old) : ensures that a specified number of pods are running at any time . replication controllers and pods are associated with “labels”. allocation of replicas to nodes are done by the schedulers by default. this ensures high availability of the pods even if some of the nodes crash. it uses equality-based selectors.

	+ replicaSet (new) : ensures that a specified number of pods are running at any time. replicaSet and pods are associated with “labels”. it uses set-based selectors.

	+ deployment controller : if you application running on different nodes and you need to upgrade the appln. (for instance, from v1.0 to v1.1) then you will need deployment controller. using this controller we can quickly upgrade our versions and take care of the replicas. its features include multiple replicas, upgrade, rollback, scale up or down, pause and resume. you can only have one service per port.

	s1.war - - v1.0 - - docker hub
	s1.war - - v1.1 - - docker hub

	+ daemonset :

Services :

	+ Cluster IP : is a default service that. for instance you have the replicas of frontend-pod communicating with replicas of backend-pod, this frontend replicas communicate with the service(of clusterIP) of the backend. so the ip address of the backend is not exposed.

	+ NodePort : use to expose port of pod to internet. nodeport value in range of 30000 to 32767. we can have 3 different type of port - targetport ( for container level), port ( service level) and nodeport ( application / node level). target.p and port usually have the same port value but different ip addresses. and each replicas of pod have same port and different ports. while creating the deployment use labels and in creating service use valid selector.

	+ LoadBalancer : In a situation where we have more than one replicas (pods)[10.210.0.1/80, 10.210.0.2/80, 10.210.0.3/80] run on different nodes [192.168.1.1/31000, 192.168.1.2/31000, 192.168.1.3/31000] and only one service[10.180.0.15/80] running on one of the node that controls all the replicas. to access the url of the nodes you need the load balancer. here only one ip will map all the worker nodes to allow the user to access the application. LB is based on service provider, it can’t be created directly because it’s an external ip address. here multiple ip addresses is not shared with the user.


Namespaces : logical partition of the cluster.


*******************************************************************


1. Replication Controller YAML file

[root@ip-172-31-13-1 ~]# cd kubernetes

[root@ip-172-31-13-1 kubernetes]# mkdir ctrollers

[root@ip-172-31-13-1 ctrollers]# vi nginx-rc.yaml


# nginx-rc.yaml  

apiVersion: v1
kind: ReplicationController
metadata:
  name: nginx-rc    // name of the rc object
spec:
  replicas: 3
  template:
    metadata:
      name: nginx-pod
      labels:
        app: nginx-app
    spec:
      containers:
      - name: nginx-container
        image: nginx
        ports:
        - containerPort: 80
  selector:
    app: nginx-app


*******************************************************************
# 2. Create and display

[root@ip-172-31-13-1 ctrollers]# kubectl create -f nginx-rc.yaml

[root@ip-172-31-13-1 ctrollers]# kubectl get po 
// see 3 created pods among others
kubectl get po -o wide
kubectl get po -l app=nginx-app
kubectl get rc nginx-rc
kubectl describe rc nginx-rc

*******************************************************************
# 3. Reschedule

#stop any running worker node 

kubectl get po -o wide --watch
kubectl get po -o wide 
kubectl get nodes 

*******************************************************************
# 4. Scaling up cluster

kubectl scale rc nginx-rc --replicas=5   // to scale up replica controller
// use to scale up/down application using controller

kubectl get rc nginx-rc
kubectl get po -o wide

*******************************************************************
# 5. Scalling down

kubectl scale rc nginx-rc --replicas=3
kubectl get rc nginx-rc
kubectl get po -o wide

*******************************************************************
# 6. Cleanup

kubectl delete -f nginx-rc.yaml
kubectl get rc
kubectl get po -l app=nginx-app

*******************************************************************






*******************************************************************
*******************************************************************
1. ReplicaSet YAML file

# nginx-rs.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-rs // name of the rs object
spec:
  replicas: 3
  template:
    metadata:
      name: nginx-pod
      labels:
        app: nginx-app
        tier: frontend
    spec:
      containers:
      - name: nginx-container
        image: nginx
        ports:
        - containerPort: 80
  selector:
    matchLabels:
      app: nginx-app
    matchExpressions:
      - {key: tier, operator: In, values: [frontend]}


*******************************************************************
# 2. Create and display replicaset

kubectl create -f nginx-rs.yaml
kubectl get po -o wide
kubectl get po -l app=nginx-app
kubectl get rs nginx-rs -o wide
kubectl describe rs nginx-rs

*******************************************************************
# 3. Automatic Pod Reschedule 

kubectl get po -o wide --watch
kubectl get po -o wide
kubectl get nodes

*******************************************************************
# 4. Scale up pods

kubectl scale rs nginx-rs --replicas=5
kubectl get rs nginx-rs -o wide
kubectl get po -o wide

*******************************************************************
# 5. Scale down pods

kubectl scale rs nginx-rs --replicas=3
kubectl get rs nginx-rs -o wide
kubectl get po -o wide

*******************************************************************
# 6. Cleanup

kubectl delete -f nginx-rs.yaml
kubectl get rs
kubectl get po -l app=nginx-app

*******************************************************************





*******************************************************************

# 1. Deployment YAML file

[root@ip-172-31-13-1 ctrollers]# vi nginx-deploy.yaml


# nginx-deploy.yaml

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deploy    // name of the deployment object
  labels:
    app: nginx-app    // what to use often
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx-app   // ds label is very important to create replicas
    spec:
      containers:			// container to be created
      - name: nginx-container
        image: nginx:1.7.9    // use tag to represent the version
        ports:
        - containerPort: 80    // port no for the container
  selector:
    matchLabels:
      app: nginx-app       // replicas will be created based on this label



*******************************************************************
# 2. Create and Display Deployment


[root@ip-172-31-13-1 ctrollers]# kubectl create -f nginx-deploy.yaml  // to create controller object

[root@ip-172-31-13-1 ctrollers]# kubectl get deploy -l app=nginx-app  // to see the deployment object created with the label to filter the deployed object

[root@ip-172-31-13-1 ctrollers]# kubectl get rs -l app=nginx-app

[root@ip-172-31-13-1 ctrollers]# kubectl get po -l app=nginx-app // check the list of pods that are running

[root@ip-172-31-13-1 ctrollers]# kubectl get pods -o wide // to get the ip address of the node where the pods are running

[root@ip-172-31-13-1 ctrollers]# kubectl describe deploy nginx-deploy // to get detail info about the object

*******************************************************************
# 3. Testing: Rollback update 

// if the image is upgraded from nginx:1.7.9 => nginx:1.9.1

[root@ip-172-31-13-1 ctrollers]# kubectl set image deploy nginx-deploy nginx-container=nginx:1.91 --record    // to upgrade image to new version. that object will be down for some time before it starts running. but if you don’t want the appln. to be down, then use rollingUpdate to start updating from instance 1 to n. rollingUpdate is a default strategy type.
// use - -record parameter to see the history of changes

[root@ip-172-31-13-1 ctrollers]# kubectl rollout status deployment/nginx-deploy
// to see status of the deployment/ rollout

[root@ip-172-31-13-1 ctrollers]# kubectl rollout history deployment/nginx-deploy // to see the changes

[root@ip-172-31-13-1 ctrollers]# kubectl rollout undo deployment/nginx-deploy // to undo the changes

kubectl rollout status deployment/nginx-deploy

kubectl describe deploy nginx-deploy | grep -i image

*******************************************************************
# 4. Testing: Update Version of "nginx:1.7.9"  to "nginx:1.9.1"

kubectl set image deploy nginx-deploy nginx-container=nginx:1.9.1
# or
kubectl edit deploy nginx-deploy  // both commands are possible for upgrading running container, 

kubectl rollout status deployment/nginx-deploy
kubectl get deploy

*******************************************************************
# 5. Testing: Scale UP

[root@ip-172-31-13-1 ctrollers]# kubectl scale deployment nginx-deploy --replicas=5   // to scale up / down of replicas, to ensure high availability of appln.
kubectl get deploy
kubectl get po -o wide

*******************************************************************
# 6. Testing: Scale DOWN

kubectl scale deployment nginx-deploy --replicas=3
kubectl get deploy
kubectl get po -o wide
   
*******************************************************************

# 7. Cleanup

[root@ip-172-31-13-1 ctrollers]# kubectl delete -f nginx-deploy.yaml // to delete object if no longer needed. this will delete all the instances of the object by default

kubectl get deploy
kubectl get rs
kubectl get po 

*******************************************************************



Worker node 1, 2, 3, 4, 5, 6, 7, 8, 9

run a pod in all the worker nodes !!! or in a subset of nodes

use Daemonsets

	e.g create pod to monitor the memory utilization / cpu utilization / malware / 

replicasets vs daemonsets : rs helps to ensure specific no of pods are running at certain time by the scheduler but don’t know which instances, it will depends on which ones are available. while for ds all the node must have the pod.




Overview of DaemonSet:
~~~~~~~~~~

1. Deploy Pod on "all" worker nodes inside k8s cluster using DaemonSet

   1a. fluentd-DaemonSet manifest file
   1b. Create | Display | Validate

3. Cleanup

*************************************************************************************************************************************************


1. Deploy Pod on "all" worker nodes inside k8s cluster using DaemonSet

1a. YAML File:

[root@ip-172-31-13-1 ctrollers]# vi fluentd-ds-allnodes.yaml

# fluentd-ds-allnodes.yaml

apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-ds
spec:
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      containers:
      - name: fluentd
        image: gcr.io/google-containers/fluentd-elasticsearch:1.20
  selector:
    matchLabels:
      name: fluentd

---------------------------------------------------------------------

1b. Create | Display | Validate

[root@ip-172-31-13-1 ctrollers]# kubectl create -f fluentd-ds-allnodes.yaml

[root@ip-172-31-13-1 ctrollers]# kubectl get po -o wide

[root@ip-172-31-13-1 ctrollers]# kubectl get ds

[root@ip-172-31-13-1 ctrollers]# kubectl describe ds fluentd-ds

*************************************************************************************************************************************************

3. Ceanup

[root@ip-172-31-13-1 ctrollers]# kubectl delete ds fluentd-ds

[root@ip-172-31-13-1 ctrollers]# kubectl delete ds nginx-ds

[root@ip-172-31-13-1 ctrollers]# kubectl get po

*************************************************************************************************************************************************



Volumes

	+ pods are ephemeral and stateless

	+ volumes bring persistence to pods. vol existed even when the pod(s) is removed.

	+ adv of kubernetes vol vs docker vol : container(s) has access to vol in docker while in kub. volumes are associated with lifecycle of pod; like docker, kub. support many types of volumes including ephemeral ( same lifetime as pods) and durable (beyond pods lifetime).

	+ volume type include configMap, emptyDir, gcePersistentDisk, hostPath, persistentVolumeClaim, secret, awsElasticBlockStore etc


hostPath 

- mounts a file or directory from the host node’s filesystem into your pod

- remains even after the pod is terminated

- similar to docker volume


Persistent Volumes

- abstracts details of how storage is provided how it is consumed

i.  Persistent Volume (PV): piece of storage in cluster

ii. Persistent Volume Claim (PVC): request for storage

- lifecycle of a persistent volume:

	provisioning (static / dynamic) => binding => using => reclaiming



*******************************************************************
*******************************************************************

# 1. HostPath YAML file

[root@ip-172-31-13-1 ctrollers]# vi nginx-hostpath.yaml

apiVersion: v1
kind: Pod
metadata:
  name: nginx-hostpath
spec:
  containers:
    - name: nginx-container
      image: nginx
      volumeMounts:
      - mountPath: /test-mnt     // path created in the host
        name: test-vol
  volumes:
  - name: test-vol     // name should be the same as above. d created vol is meant for entire cluster not only for a particular worker node but it’s allocated at the pod level.
    hostPath:
      path: /test-vol      // create simple directory in the cluster


*******************************************************************
# 2. Create and Display HostPath

kubectl create -f nginx-hostpath.yaml
kubectl get po
kubectl exec nginx-hostpath df /test-mnt

*******************************************************************
3. Test: Creating "test" file underlying host dir & accessing from from pod

From HOST:
~~~~~~~~~~
cd /test-vol
echo "From Host" > from-host.txt
cat from-host.txt

From POD:
~~~~~~~~
kubectl exec nginx-hostpath cat /test-mnt/from-host.txt


*******************************************************************
4. Test: Creating "test" file inside the POD & accessing from underlying host dir

From POD:
~~~~~~~~~
kubectl exec nginx-hostpath -it -- /bin/sh
cd /test-mnt
echo "From Pod" > from-pod.txt
cat from-pod.txt

From Host:
~~~~~~~~~~
cd /test-vol
ls
cat from-pod.txt


*******************************************************************
5. Clean up

kubectl delete po nginx-hostpath
kubectl get po
ls /test-vol


******************************************************************* 


Namespaces : logical partition of K8s Cluster to run pods in your own environment i.e isolation of process of execution of pods.

- you’ll have default namespace

- you might have diff environment like dev, prod, QA etc 

- each of this environment can be created using diff namespaces

- pods running in one NS is isolated to that environment

- this make it easy to deploy to particular environment

- using command to create ns

 [root@ip-172-31-13-1 ~]# kubectl create namespace dev  // namespace/dev created
// use pod definition to point to the ns

 kubectl create namespace prod  // namespace/prod created

 kubectl get namespace

- using manifest file to create ns

apiVersion: v1
kind: Namespace
metadata: 
	name: dev

- how to use created ns to create pod??

[root@ip-172-31-13-1 ~]# vi nginx-pod.yaml

apiVersion: v1
kind: Pod
metadata
	labels:
		env: dev
	name: nginx-pod      	// name of the po
	namespace: dev			// namespace to be used. by default pod will be running on the default ns
spec:
	. . . 
	. . .

[root@ip-172-31-13-1 ~]# kubectl get pods - -namespace=dev

[root@ip-172-31-13-1 ~]# kubectl get pods - -all-namespaces

[root@ip-172-31-13-1 ~]# kubectl get ns



Helm : packages collection of kubernetes logic which can be deployed deployed later. the packages can be distributed and reused as HELM charts

- d’s same as apt or yum in linux


HELM Chart:::
Helm is a pkg manager for kubernetes
which help to automatically create manifest files for any kubernetes objects in the form of HELM Charts

HELM Chart for prometheus

Eg.: Kubernetes Dashboard installation using HELM



Install HELM 
on kubernetes master node install Helm

#Downlaod helm tar ball for linux

[root@ip-172-31-13-1 ~]# wget https://get.helm.sh/helm-v3.6.3-linux-amd64.tar.gz

#https://github.com/helm/helm/releases/download/v3.9.4/helm-v3.9.4-linux-amd64.tar.gz.asc

#Extract tar ball

[root@ip-172-31-13-1 ~]# tar -zxvf helm-v3.6.3-linux-amd64.tar.gz

[root@ip-172-31-13-1 ~]# ls  // 4 directories created

#Move helm (1 of the directories created above) to usr/local/bin/helm
[root@ip-172-31-13-1 ~]# sudo mv linux-amd64/helm /usr/local/bin/helm


[root@ip-172-31-13-1 ~]# vi .bash_profile  

#Append the helm path to PATH Variable and save.

PATH=$PATH:$HOME/bin:/usr/local/bin


[root@ip-172-31-13-1 ~]# source ~/.bash_profile

#run helm
[root@ip-172-31-13-1 ~]# helm

#Install Kubernetes Dashboard using Helm
//download kubernetes dashboard  and create repositories
//this will automatically provide kubernetes-dashboard service for the package
[root@ip-172-31-13-1 ~]# helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard


[root@ip-172-31-13-1 ~]# helm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard

[root@ip-172-31-13-1 ~]# kubectl get svc 

#choose kubernetes-dashboard service , which by default it’s clusterIP. so change to nodePort IP

[root@ip-172-31-13-1 ~]# kubectl edit svc kubernetes-dashboard

#goto type: [key && Change ClusterIP to NodePort & save the file with :wq]

#Now check the service, it should show you, NodePort with Nodeport IP

[root@ip-172-31-13-1 ~]# kubectl get svc 

# Now use the external IP & Nodeport to access the dashboard. When it asked for token, execute following commands to create service account & token

[root@ip-172-31-13-1 ~]# kubectl create serviceaccount dashboard -n default // name is dashboard

[root@ip-172-31-13-1 ~]# kubectl create clusterrolebinding dashboard-admin -n default --clusterrole=cluster-admin --serviceaccount=default:dashboard  // give priviledges to created serviceaccount - dashboard

[root@ip-172-31-13-1 ~]# kubectl get secret $(kubectl get serviceaccount dashboard -o jsonpath="{.secrets[0].name}") -o jsonpath="{.data.token}" | base64 --decode     // to generate token needed to sign into kubernetes dashboard

https://15.207.114.100:31290/

https://65.1.130.134:30107/
*******************************************************














Managed services 

	+ AKS - Azure kubernetes service

	+ EKS - Elastic kubernetes service

Search EKS in aws => add cluster => 

	+ GKS - Google kubernetes service





+++++++++++++++++++++++++++++Artifact Repository - Nexus
Artifact Repositories
- Nexus Repository Manager 3 (free/paid)
- JFrog Artifactory (paid)
- Apache Archive (free)
- Many specific ones for Docker containers, npm, .NET packages etc






++++++++++++++++++++++++++ Monitoring using Prometheus and Grafana 

Continuous monitoring 
- it is an automated process by which one can observe and detect compliance issues and security threats during each phase of the DevOps pipeline.
Continuous monitoring tools in DevOps
i. monitoring tools : Sensu, Nagios
ii. alerting tools : PagerDuty, Slack, Servicenow
iii. metric storage : Splunk, Influxdb, Aws
iv. Visualization tools : Grafana

Trade Desk’s Monitoring Issues
- The Trade Desk is a global tech company that markets a software platform for digital advertising 
- The company was founded in 2009 and currently hosts over 1200 employers with a market cap of 8.89 billion USD
- Such a massive operations is handled using a combination of physical data centers and the cloud

The Trade Desk: Monitoring Troubles
- Their old infrastructure required an extremely labour intensive, difficult to scale storage system.
- This system was vey unreliable in getting queries and providing metrics on time
- This meant that the system required a lot of handholding on a regular basis

Solution: Prometheus and Grafana
- Prometheus provided the company an easy way to collect all the required metrics for their day-to-day operations
- Prometheus also gave them the ability to monitor their modern system as well as their legacy system
- The alerting mechanism in the new monitoring system was very easy to embed with the dashboards
- Grafana made the dashboard visualizing more accessible for the various teams in the organization


Prometheus
- it is a community-driven open-source monitoring framework
- has a robust alert mechanism
- a Cloud Native Computing Foundation (CNCF) graduated project

Prometheus Architecture
						Prometheus Server

Push Gateaway	<==	Retrieval	  =>   Storage	=>	HTTP Server	==>	Prometheus Web GUI
													==>	Alert Manager

- Retrieval : Pulls metric data. data retrieval worker
- Storage : stores metric data. time series database
- HTTP Server : accepts PromQL queries

Prometheus: Features

- Dimensional data : uses a dimensional data model where series are identified by a metric name and a set of key-value pair.
- Simple operations : servers running prometheus are independent of each other for reliability. since it is written in GO language, the binaries can easily be updated.
- Alert system : all the alerts are managed by an alert manager. The alerts are defined using the prometheus’s own PromQL.
- Visualization : offers various options to visualize the monitoring data. the built-in expression browser can be integrated with tools such as Grafana.
- Library support : with support of over ten different languages, creating and implementing custom libraries is very easy.
- Efficient storage : prometheus stores all the monitoring data on the local disk itself in an efficient custom format.


Prometheus on Kubernetes: working
- constantly tracks the Kubernetes API for changes and updates the configuration accordingly
- uses a Custom Resource Definition called ServiceMonitor in order to monitor a target
- ServiceMonitor attaches itself to the target(s) using the matchLabel selector
- works in a pull-based mechanism enabling abstraction from the service being monitored

Prometheus metrics and its types
i. Counter : record a value that only goes up. query how fast the value is increasing
ii. Gauges : record a value that only goes up and down. you do not have to query its rate
iii. Histogram : take many measurements of a value to later calculate averages or percentiles. you know what the range of values will be up front, so you can define your own.
iv. Summary : take many measurements of a value to later calculate averages or percentiles. you don’t know what the range of values will be up front, so cannot use histograms.

Grafana
- multi-platform open source analytics and interactive visualization web application
- provides charts, graphs, alerts

Grafana features :
i. Visualize : grafana has a plethora of visualization options to help you understand your data, beautifully
ii. alert : seamlessly define alerts where it makes sense - while you’re in the data.
iii. unify : grafana supports dozens of databases, natively. mix them together in the same dashboard
iv. alert : grafana’s completely open source, and backed by a vibrant community
v. extend : discover hundreds of dashboards and plugins in the official library
vi. collaborate : bring everyone together and share data and dashboards across teams






Edureka Practice!!!

Prometheus architecture : 

- 2 things are important in monitoring tool as it uses clients-server architecture

	+ source : where you install your monitoring tool, i.e the controller machine

	+ target : targets machines are connected to the monitoring tool e.g node, applications, containers, kubernetes

- 3 components include the following

i. retrieval : use to retrieve metric data from target machines (based on node exporter, which is installed in all the target.m)

ii. storage : time series db to store metric data

iii. HTTP server : accept queries, then sending data to prometheus web GUI and Alert manager.

Grafana : it’s a visualization tool

- data source needed to be added


Create 2 instances

- Build_Server (target machine) : ip-172-31-0-205
	
- Monitoring_Server : ip-172-31-15-74

	+ install prometheus
	
	

#Install Prometheus & Grafana :

### Goto https://prometheus.io/download/

// to download prometheus
[root@ip-172-31-15-74 ~]# wget https://github.com/prometheus/prometheus/releases/download/v2.38.0/prometheus-2.38.0.linux-amd64.tar.gz

// to extract 
[root@ip-172-31-15-74 ~]# tar -zxvf prometheus-2.38.0.linux-amd64.tar.gz

[root@ip-172-31-15-74 ~]# ls  // you will get directory called prometheus-2.38**


[root@ip-172-31-15-74 ~]# cd prometheus-2.38**

[root@ip-172-31-15-74 prometheus-2.38**]# ls // it contains prometheus (as a service), prometheus.yml (config. file), 




###===> Create following file:

[root@ip-172-31-15-74 ~]# sudo vi /etc/systemd/system/prometheus.service  // for prometheus (as a service) in the directory above
// all services are created and run in this path

	#-------------------------------------------------------------------------

	[Unit]
	Description=Prometheus Server
	Documentation=https://prometheus.io/docs/introduction/overview/
	After=network-online.target

	[Service]
	User=root
	Restart=on-failure

	ExecStart=/root/prometheus-2.38.0.linux-amd64/prometheus --config.file=/root/prometheus-2.38.0.linux-amd64/prometheus.yml

	[Install]
	WantedBy=multi-user.target

	#-------------------------------------------------------------------------


// to make the created service effective run the following

[root@ip-172-31-15-74 ~]# sudo systemctl daemon-reload
[root@ip-172-31-15-74 ~]# sudo systemctl status prometheus
[root@ip-172-31-15-74 ~]# sudo systemctl start prometheus
[root@ip-172-31-15-74 ~]# sudo systemctl enable prometheus   // auto-start

<prometheus-external-ip>:9090
http://172.31.15.74:9090/    // external-ip of the machine and by default prometheus runs on port 9090
// this will display prometheus simple dashboard

up - to get target machines connected to the monitoring server






###*********************************************************************

In the target machine


###Install Node Exporters:

### Goto https://prometheus.io/download/

## Search for Node Exporter:
##Copy Link Linux address
### https://github.com/prometheus/node_exporter/releases/download/v1.4.0-rc.0/node_exporter-1.4.0-rc.0.linux-amd64.tar.gz


###Goto the server you wish to monitor and install Node Exporter :

// to downlaod
[root@ip-172-31-0-205 ~]# wget https://github.com/prometheus/node_exporter/releases/download/v1.4.0-rc.0/node_exporter-1.4.0-rc.0.linux-amd64.tar.gz

// to extract
[root@ip-172-31-0-205 ~]# tar -zxvf node_exporter-1.4.0-rc.0.linux-amd64.tar.gz




###===> Create following file:

[root@ip-172-31-0-205 ~]# sudo vi /etc/systemd/system/node_exporter.service   // this service continously monitor your target machine

	#-------------------------------------------------------------------------

	[Unit]
	Description=Prometheus Server
	Documentation=https://prometheus.io/docs/introduction/overview/
	After=network-online.target

	[Service]
	User=root
	Restart=on-failure

	ExecStart=/root/node_exporter-1.4.0-rc.0.linux-amd64/node_exporter

	[Install]
	WantedBy=multi-user.target

	#-------------------------------------------------------------------------

[root@ip-172-31-0-205 ~]# sudo systemctl daemon-reload
[root@ip-172-31-0-205 ~]# sudo systemctl status node_exporter
[root@ip-172-31-0-205 ~]# sudo systemctl start node_exporter
[root@ip-172-31-0-205 ~]# sudo systemctl enable node_exporter
[root@ip-172-31-0-205 ~]# sudo systemctl status node_exporter // note the port where it's running (9100) required in the prometheus server.

[root@ip-172-31-0-205 ~]# hostname -i  // to get the IP of the host machine 172.31.0.205


// go to config file in prometheus server

### copy the IP Address of the server you wish to monitor

## Goto to Prometheus Server ::

### installation path 
cd /root/prometheus-2.38.0.linux-amd64

vi prometheus.yml
## Add the target with valid node_exporter port as mentioned below:

[root@ip-172-31-15-74 prometheus-2.38**]# vi prometheus.yml

	static_configs:
		- targets: ["localhost:9090"]
		- targets: ["172.31.0.205:9100"]


## Restart Prometheus:

[root@ip-172-31-15-74 ~]# sudo systemctl restart prometheus
[root@ip-172-31-15-74 ~]# sudo systemctl status prometheus

###Goto Prometheus server :

<prometheus-external-ip>:9090
http://13.232.146.141:9090/

## in the query field type up and click on execute to see the list of servers up for monitoring





####*******

##on prometheus monitoring server or on different machine

#Install Grafana :

### Goto https://grafana.com/grafana/download
#choose OSS for practise

###select OSS Edition.
#Choose Linux

## Linux Distribution :

## Red Hat, CentOS, RHEL, and Fedora(64 Bit)SHA256: 

// to download
[root@ip-172-31-15-74 ~]# wget https://dl.grafana.com/oss/release/grafana-9.1.2-1.x86_64.rpm

// to install
[root@ip-172-31-15-74 ~]# sudo yum install grafana-9.1.2-1.x86_64.rpm

// start and enable service
[root@ip-172-31-15-74 ~]# sudo /bin/systemctl enable grafana-server.service
[root@ip-172-31-15-74 ~]# sudo /bin/systemctl start grafana-server.service

//in the browser
<grafana-external-ip>:3000    //default port for gravana is 3000
http://172.31.15.74:3000/

default user name & password : admin & admin





##**************

###Create Prometheus Data Source in Grafana

// prometheus is considered as data source for grafana

###Goto Grafana 

##<grafana-external-ip>:3000
##http://13.232.146.141:3000/

Click on : settings button --> Data Source --> Add Data Source --> Select Prometheus

Enter in the Name field as: Prometheus
Enter in the url field as the prometheus-server url with port eg.: <prometheus-external-ip>:9090 | http://13.232.146.141:9090/
			//if grafana and prometheus are running on same machine put: http://localhost:9090

Click on Save & Test --- for Data Source is working 
Click on Back Button
See the Prometheus Data Source Created 
	Click on : settings button --> Data Sources
	
To create dashboard operation

dashboard ==> new dashboard ==> add a new panel 
										==> data source : Prometheus
										==> code : to add promQl


Test in Prometheus-server query 

https://prometheus.io/docs/prometheus/latest/querying/examples/

	sum by(mode)(irate(node_cpu_seconds_total{mode!="idle"}[5m])) > 0

	CPU utilization: sum by(mode)(irate(node_cpu_seconds_total{mode!="idle"}[5m])) > 0

	Goto grafana and cl

	boot_time: time() - node_boot_time_seconds{}

	memory_utilization: 100 * (1 - ((avg_over_time(node_memory_MemFree_bytes[10m]) + avg_over_time(node_memory_Cached_bytes[10m]) + avg_over_time(node_memory_Buffers_bytes[10m])) / avg_over_time(node_memory_MemTotal_bytes[10m])))tar -zxvf prometheus-2.38.0.linux-amd64.tar.gz







++++++++++++++++++++++++++ Provisioning Using Terraform

Rapyder’s approach to IaC using Terraform

- Rapyder is a Cloud Consultancy firm providing cloud-based infrastructure solutions
- the company started as a consultancy helping run school competitions back in 1991
- their clientele includes the likes of Reliance, sify, lynk, Neogrowth, mapmyindia etc

Yelp: Monolithic architecture
- some of the clients required their infrastructure be spread across multiple cloud providers
- the infrastructure needs should be met in minutes
- setting up database backups along with the infrastructure was a part of the setup

Terraform to the rescue
- Terraform provided the solution to their problem by being cloud agnostic
- Infrastructure could be setup from different vendors like aws and azure within minutes using TF.
- Managing database backups also became quite painless with TF
- TF enabled clients to test applications in production without any downtime

Provisioning 
- involves providing the IT infrastructure to host services for your organization’s business, users, employees and customers

Introduction to TF
- TF is an infrastructure as code software used for provisioning infrastructure safely and efficiently.

Terraform: Features
i. IaC : infrastructure as code allows for users to describe their desired infrastructure as a high-level configuration syntax
ii. Execution plans : allows you to preview the changes your configuration will make
iii. Resource graph : TF creates graphs to ensure that all the non-dependent resources are created parallel
iv. Change automation : with the help of execution plans and resource graphs users know exactly what will change avoiding many human errors


Terraform 							Ansible
- Mainly a infrastructure provisioning tool	- mainly a infrastructure provisioning tool
- advanced orchestration options			- relatively poor orchestration performance
- less mature because relatively new but 	- much more mature with huge community support
  has good community support
- chances of configuration drift less		- higher chances of configuration drift

Terraform architecture
- TF architecture constitutes of two main components

i. core : TF core is a statically compiled binary written in golang
- core takes input in the form of TF configuration from the user and info from state
- using the configuration file and the state info, core creates a plan to bring the system to the desired state.
- TF config. is written by the user to define what needs to be done
- state is a file that up-to-date information about the current setup of the infrastructure
- before any new operation, TF updates the state with the real infrastructure

ii. providers: the second main component of TF is the provider plugin
- the config. defines which provider(s) to use for a particular setup
- providers add resources which can be managed using TF
- most of the major providers can be found on the TF registry

Terraform Configuration
- TF configuration is the primary way of provisioning infrastructure using TF
- the config. is written in the TF language
- TF lang. is declarative in nature
- the configuration tells TF how to manage and provision the infrastructure

- syntax elements
	i. resource
	+ resource blocks are used to define infrastructure objects
	+ resource behavior defines how TF handles resource declarations
	+ meta-arguments provides special arguments that can be used with each resource type
	ii. blocks
	+ blocks acts as containers for configuration of objects such as resources. it can have a block type , 0 or more labels and any number of arguments
	iii. arguments
	+ arguments are inputs given to a name. 
	iv. expressions
	+ these represent values. they can be direct values, referenced, or a set of values


TF basic commands
i. Init 
- used to prepare the working directory for follow up commands
- this is the first command that is run after writing a new configuration
- the working directory must contain a configuration to run this command
ii. plan
- it displays the changes that will be made by the configuration
- it creates an execution plan by refreshing the state and comparing it with the required configuration
- it is equivalent to the dry run feature available in some of build software
iii. apply
- apply as the name suggests applies the newly created configuration
- it can also be used to deploy the predetermined set of actions generated in the execution plan

Managing resources in TF
i. Modifying resources
- modifying an existing resource can be done by simply making the changes to the existing TF configuration and executing the apply command
- the changes can also be seen marked with a yellow tilde ‘~’ sign in execution plan
ii. Deleting resources
- deleting resources can be done by using the destroy command
- simply running the destroy cmd terminates all the resources created by TF
- in-order to delete a specific resource, parameters can be passed


Referencing resources
- TF gives you the option to replicate infrastructure by referencing existing resources


Terraform state commands
- ds cmd provides you with advanced state management capabilities. d state cmd can be used to manipulate TF state instead of changing it directly.

- subcommands
i. list : lists all the resources in the current state
ii. show : displays details about a deployed resource
iii. rm : removes a deployed resource
iv. mv : it is used to move objects to a given destination
v. pull : displays the current state on stdout
vi. replace-provider : changes the provider of the current state.

Terraform project
- We will deploy a project with the following infrastructure using TF
- Part A: Network setup
	+ Create a VPC
	+ Create an internet gateway
	+ Create a custom route table
	+ Create a Subnet
	+ Associate the subnet with the route table
- Part B: Security group setup
	+ Create a new security group
	+ Enable ports 22, 80, 443
- Part C: Network Interface setup
	+ Create a new network interface with IP in the previously created subnet
	+ Create an elastic IP associated with the network interface
- Part D: EC2 Instance setup
	+ Create a new ubuntu ec2 instance and attach the network interface to it
	+ Install httpd server on it





Edureka practical !!!


What is the purpose of IaC tools?

Infrastructure Team

	+ Provisioning Infrastructure == process of creating the Infra

	+ Configure the servers 

	+ formerly provisioning Infrastructure using shell scripts instead of IaC tools


CI/CD

	+ C-Development

	+ C-Integration
		Testing - QA, UAT
			500 Microservices —
			Use IAC tools like Terraform - write the scripts to provision all these 500+ servers dynamically
			Configure the servers using Ansible
			Build
			Create artifacts
			Testing the artifacts
			Destroy all the test servers

	+ C-Delivery/Deployment


Declarative Lang.

Terraform — - HCL  == provision infrastructure on any cloud platform
Ansible - - yaml == configure infrastructure 
AWS  - - CFT [ Cloudformation template] - - json
AZURE - - ARM - - yaml


Terraform architecture constitutes of two main components

- core : which is the main installed TF software on controller machine.

	+ it will interact with the service providers on the target env. where you want to create the instances including those listed below.

- Providers : include AWS/ Azure/ GCP, Kubernetes, Fastly

- work flow : Infrastructure admin will create TF Config. file using HCF lang. this will be executed on the core. After execution, it will identify the target env. where it will provision. that means you will mention the provider in the config. file.
	+ state is a file that will maintain the current state of your infrastructure to avoid duplication.



Action Items

	+ Install Terraform

	+ Create the config. file
		Identify the scope / provider (aws)
		create *.tf file to define desired state of the resources
		terraform plan - verify config. file
		terraform apply - execute cmd

	+ Create a folder - to keep tf files
		Create *.tf file 
		terraform init - should be done once
		terraform plan - verify config. file
		terraform apply - execute cmd

	+ How to proceed ::
		where to install terraform?
			local windows, 

	+ How to access ::
		command prompt to use TF
		use visual studio code IDE : to easily develop config files 
			install VS code
			add terraform plugins - HashiCorp Terraform

	+ On VS code
		create **.tf file in a folder - Terraform-demo

Config. file
- start with provider, which is your target environment where you want to provision the server.
	+ this will include the region, access_key and secret_key. this is first step
	+ locate region on aws
	+ access_key & secret_key : settings => security credentials => IAM dashboard => access keys => create new access key => display access key and security key
	+ this replaces the password or token
- create aws instance
- save the file
- create plan command in the terminal (of the directory path where you have config. file)
	> terraform - -version
	> terraform init   	// to initialize the config. file and install the metadata required to connect to your provider (AWS).
	> terraform plan  	// to review the config. file and verify the configuration. display the configuration with the following annotations:
		+ => add new resource
		~ => update any existing resource
		- => delete any existing resource
	> terraform apply	// implement the changes in the target environment

guide to write config. file
registry.terraform.io => providers => documentation =>


// this create dummy instance that can’t be accessed because no security group or anything else
+++++++++++++++++++++++++  file start here
provider "aws" {           
  region     = "ap-south-1"
  access_key = "AK74M"
  secret_key = "kfu79FP8EqHBap3"
}

# Create AWS Instance

resource "aws_instance" “dev_server1” {	
// type of resource - aws_instance, name of resource - tf_instance1
  ami           = "ami-09f7fbc41963e146f"  
// to get ami image : launch instance => amazon linux => copy AMI ID
  instance_type = "t2.micro"
  key_name      = "loksaieta123"
// instances => key_name of instance

  tags = {
    Name = "Terraform-server1"  // server name 
  }
}

# Create AWS Instance

resource "aws_instanceeee" "app_server2" {
  ami           = "ami-09f7fbc41963e146f"
  instance_type = "t2.micro"
  key_name      = "loksaieta123"

  tags = {
    Name = "TerraformDemoServer2"
  }
}
+++++++++++++++++++++++++  file end here


+ => Add new resource
~ => update any existing resource
- => Delete any existing resource







terraform init

terraform plan

terraform apply

terraform destroy

terraform state list





*****************************************************************************************




Install Terraform --> Local Windows Machine!!!

Use Command prompt to run Terraform commands like : terraform init, terraform plan, terraform apply, etc..

Create the Config file ?

Visual Studio Code  to create or develop Terraform Config files! i.e., *.tf files 





Creating config. file for vpc
- to create ec2 instance, Infra team should define what resources will be required
- here you create vpc, subnet and then instances.


provider => vpc => subnet => Internet gateway => route table => route table association => security group => Instance  [8]
 
// in order to make your instance accessible, up and running you need to include all this
+++++++++++++++++++++++++  file start here
provider "aws" {    	// provider that will point to aws instance i.e aws console
  region     = "ap-south-1"
  access_key = "ANKXU"
  secret_key = "GrU2F96"
}

# Create VPC   		// as copied from registry.terraform.io documentation
// vpc is required to be able to create subnet
resource "aws_vpc" "myvpc8" {      
  cidr_block       = "10.0.0.0/16"
  instance_tenancy = "default"

  tags = {
    Name = "myvpc8"
  }
}

# Create Subnet 		// as copied from registry.terraform.io documentation
// subnet is created within vpc

resource "aws_subnet" "mysubnet8" {
  vpc_id     = aws_vpc.myvpc8.id
  cidr_block = "10.0.1.0/24"

  tags = {
    Name = "mysubnet8"
  }
}

# Internet Gateway		// as copied from registry.terraform.io documentation
// to create internet gateway you need vpc

resource "aws_internet_gateway" "mygw8" {
  vpc_id = aws_vpc.myvpc8.id

  tags = {
    Name = "mygw8"
  }
}

# Route Table			// as copied from registry.terraform.io documentation
// to create route table you need vpc

resource "aws_route_table" "myrt8" {
  vpc_id = aws_vpc.myvpc8.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.mygw8.id
  }

  tags = {
    Name = "myrt8"
  }
}

# Route Table Association	// as copied from registry.terraform.io documentation
// to create route table association you need subnet and route table

resource "aws_route_table_association" "myrta8" {
  subnet_id      = aws_subnet.mysubnet8.id
  route_table_id = aws_route_table.myrt8.id
}

# Security Groups 			// as copied from registry.terraform.io documentation
// to establish connection to your instances
// to create security group you need vpc

resource "aws_security_group" "mysg8" {
  name        = "mysg8"
  description = "Allow inbound traffic"
  vpc_id      = aws_vpc.myvpc8.id

// ingress are the inbound rules
  ingress {
    description      = "HTTP"
    from_port        = 80
    to_port          = 80
    protocol         = "tcp"
    cidr_blocks      = ["0.0.0.0/0"]
  }

  ingress {
    description      = "SSH"
    from_port        = 22
    to_port          = 22
    protocol         = "tcp"
    cidr_blocks      = ["0.0.0.0/0"]
  }

// egress are the outbound rules
  egress {
    from_port        = 0
    to_port          = 0
    protocol         = "-1"
    cidr_blocks      = ["0.0.0.0/0"]
    ipv6_cidr_blocks = ["::/0"]
  }

  tags = {
    Name = "mysg8"
  }
}

# Create Instance  // after inbound & outbound rules are defined, you can have proper definition of resource
// to create resource / instance you need subnet and security group

resource "aws_instance" "App_server2" {  // change name - App_server2, to create multiple instances at the same time
  ami           = "ami-09f7fbc41963e146f"
  instance_type = "t2.micro"
  associate_public_ip_address = true
  subnet_id = aws_subnet.mysubnet8.id		// subnet created
  vpc_security_group_ids = [aws_security_group.mysg8.id] // security group created 
  key_name = "loksaieta123"    // key name of the instance

  tags = {
    Name = "Terraform-App_Server8"   // name of the server 
  }
}

+++++++++++++++++++++++++  file end here


after creating the config. file 

	+ open terminal in the path of directory that has the file

	> terraform - -version   // to make sure terraform is accessible

	> terraform init	// it will install provider .exe cmd in your directory

	> terraform  plan  		// display resources to be added, here 7 resources

	> terraform apply  		// this will create all the resources starting from vpc 

	> terraform  plan  		// if you make changes in the config. file. it will reflect the changes 

	> terraform apply 		// to update the changes

	> terraform destroy -target aws_instance.App_server1	// to delete resource from target environment

	> terraform state list 	// display all the resources that has bn defined

	> terraform state show aws_instance.App_server1 	// if you want to know more about a particular resource 

	> terraform state rm aws_instance.App_server1 // to remove resource from the state file, so that it will not be tracked. the state file is present within your directory as terraform.tfstate


CI/CD Demo!

	Resources:
	CI/CD
		Jenkins pipelines 	
			build, create artifacts, build docker img, deploy using K8s
// to perform build of artifact and publish to QA server. you integrate your docker build into ds pipeline, to create appln. image.
// ansible will not be required since creating artifacts and build docker img on the same server. but if on different env. you might use ansible.
// you can use ansible to copy artifact from jenkins slave machine to QA server
// jenkins master running in my local machine
// pipeline : aws_cicd_pipeline_build_java_mvn_app
// source code repository : GitHub.com/LoksaiETA/Java-mvn-app2   
// the repository also contains Dockerfile, Jenkinsfile, k8smvndeployment.yaml, nodePort.yaml, pom.xml








++++++++++++++++++++++++++ Continuous Testing with Selenium
What is Testing?
- identifying the mistakes or bugs in the software or an application
- verifying whether the application works as expected or not
Types of Testing
i. Manual testing : testing manually for defects or bugs or errors
ii. Automated testing : testing automatically for defects or bugs or errors
- it’s performed by using automation tools to run test cases 
Types of Testing
i. Unit testing : performed at build time on a single unit of code and / or without use of external dependencies or deployment.
	+ e.g JUnit, XUnit and Rspec
ii. Integration testing : performed as you bring together pieces of your application and as it needs to use external dependencies like databases.
	+ Abao/RAML and Serverspec
iii. UI / End-to-end (E2E) testing : performed to exercise the full flow of your application in the same way an end user would.
	+ e.g Selenium and Protractor
iv. Security testing : performed to look for flaws in your code and runtime to prevent compromises and leaking of data in production.
	+ FindBugs, Fortify and Gauntlt
What is Selenium?
- a tool which is used to automate web applications across different platforms using different programming languages.
- it’s open source and mainly used for functional and regression testing
Components of Selenium suite
i. Selenium IDE
ii. Selenium RC
iii. Selenium WebDriver : most widely used
- it’s a tool used to automate testing for web application
- it allows us to execute tests against different browsers like Firefox, Chrome, IE & Safari
iv. Selenium Grid : to test on different servers at the same time
Selenium supports two types of Testing
- Functional testing : testing each and every function inside the application to make sure that it is working as per the requirement
- Regression testing : re-testing all the functionality when a new feature is added in the application to make sure all functions work good with the new feature
Tools that can be integrated with selenium
- git : integrate git with selenium to share codes on the git repository
- maven : integrate maven with selenium to manage the project dependencies and ensure easy build process
- testNG : integrate TestNG with selenium to manage the test cases and produce HTML reports
- jenkins : integrate jenkins with selenium to build and test the code whenever a change is committed
integration of selenium with other tools
- developer develops the code and pushes it in the git repository
- code is pulled out for continuous integration by jenkins
- maven will create the build for the codes
- this automated build is then tested by selenium
- TestNG will create HTML reports after the testing is done by selenium
Selenium setup
- create new java project in eclipse ide
- download selenium (selenium client and web driver language bindings) from seleniumhq.org/download
- add downloaded selenium api to  newly created java project, as referenced libraries
- import WebDriver (ChromeDriver) + chromedriver.exe + gecko driver.exe, which is an interface in selenium api
======1
import org.openqa.selenium.chrome.ChromeDriver;
import org.openqa.selenium.firefox.FirefoxDriver;
public class VerifyUrl {
	public static void main(String[] args){
		System.setProperty(“web driver.gecko.driver”, “geckodriver.exe”);
		FirefoxDriver driver = new FirefoxDriver();
		driver.get(“http://www.edureka.co”);
		driver.manage().window().maximize();
		String x = driver.getCurrentUrl();
		System.out.println(x);
	}
}
======2
import org.openqa.selenium.By;
import org.openqa.selenium.chrome.ChromeDriver;
public class Wbd1 {
	public static void main(String[] args){
		System.setProperty(“web driver.chrome.driver”, “chromedriver.exe”);
		ChromeDriver driver = new ChromeDriver();
		driver.manage().window().maximize();
		driver.get(“http://www.facebook.com”);
		driver.findElement(By.id(“u_0_j”)).sendKeys(“Selenium”);
		driver.findElement(By.name(“lastname”)).sendKeys(“Testing”);
		driver.findElement(By.name(“websubmit”)).click();
}
- using user locator to identify the element and user corresponding answer to hardcode the value.
======3
import org.openqa.selenium.By;
import org.openqa.selenium.chrome.ChromeDriver;
public class Wbd2 {
	public static void main(String[] args){
		System.setProperty(“webdriver.chrome.driver”, “chromedriver.exe”);
		ChromeDriver driver = new ChromeDriver();
		driver.manage().window().maximize();
		driver.get(“http://www.google.com”);
		driver.findElement(By.linkText(“Images”)).click();
		driver.navigate().back();
		driver.navigate().forward();
	}
}
		
- by highlighting images on google.com and inspect it.
	
	

